<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.3">Jekyll</generator><link href="/feed.xml" rel="self" type="application/atom+xml" /><link href="/" rel="alternate" type="text/html" /><updated>2025-09-08T11:25:24+08:00</updated><id>/feed.xml</id><title type="html">Wilson Fok</title><subtitle>Hello, My name is Wilson Fok. I love to extract useful insights and knowledge from big data. I also like to make new friends and connections. Let&apos;s connect! </subtitle><author><name>Wilson Fok</name></author><entry><title type="html">Gaussian Splatting - Failures and Successes</title><link href="/deep_learning/2025/08/19/GaussianSplatting_failuresAndLessons/" rel="alternate" type="text/html" title="Gaussian Splatting - Failures and Successes" /><published>2025-08-19T00:00:00+08:00</published><updated>2025-08-19T00:00:00+08:00</updated><id>/deep_learning/2025/08/19/GaussianSplatting_failuresAndLessons</id><content type="html" xml:base="/deep_learning/2025/08/19/GaussianSplatting_failuresAndLessons/">&lt;h1 id=&quot;failures&quot;&gt;Failures&lt;/h1&gt;

&lt;h2 id=&quot;image-quantity-and-quality&quot;&gt;Image Quantity and Quality&lt;/h2&gt;

&lt;p&gt;While most parameters—such as model settings, data pipeline configurations, and optimization settings—work reasonably well out of the box, I found two factors especially critical for the quality of Gaussian splatting and sparse scene reconstruction.&lt;/p&gt;

&lt;p&gt;The first is the number of input images. In my initial experiments, I used only one frame out of every 15–20 frames, which meant discarding many valuable images from the video. This led to reconstruction errors caused by the low frame rate. When I increased the number of frames retained from 20 down to 15 between images, the reconstruction improved significantly while keeping the computational cost manageable.&lt;/p&gt;

&lt;p&gt;The second factor is the number of optimization iterations. The default of 30,000 iterations produces good results, but longer runs can further improve quality; in one experiment, I extended training to 93,000 iterations.&lt;/p&gt;

&lt;p&gt;A third limiting factor is image resolution. The resulting mesh quality was relatively low, largely due to a combination of limited frames and low-resolution input images. Image resolution is constrained by available GPU memory—higher resolution images require GPUs with greater memory capacity.&lt;/p&gt;

&lt;p&gt;Recap:&lt;/p&gt;
&lt;div class=&quot;juxtapose&quot;&gt;
    &lt;img src=&quot;/assets/images/gaussianSplatting/resolutions/240x134_2.png&quot; /&gt;
    &lt;img src=&quot;/assets/images/gaussianSplatting/resolutions/1920x1080_2.png&quot; /&gt;
&lt;/div&gt;
&lt;script src=&quot;https://cdn.knightlab.com/libs/juxtapose/latest/js/juxtapose.min.js&quot;&gt;&lt;/script&gt;

&lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.knightlab.com/libs/juxtapose/latest/css/juxtapose.css&quot; /&gt;

&lt;h1 id=&quot;successes&quot;&gt;Successes&lt;/h1&gt;

&lt;p&gt;There are many steps in this pipeline where errors can occur, so I highly recommend a process of experimentation and trial-and-error.&lt;/p&gt;

&lt;p&gt;A word of caution: early-stage errors tend to propagate and can severely degrade final results. For example, if COLMAP’s sparse reconstruction is inaccurate or fails, subsequent Gaussian splatting will produce flawed outputs. This was evident in the medial-lateral C-arm Gaussian splatting result, where an extra phantom arm appeared—an artifact not present in the real object. This mistake did not creep in during Gaussian splatting step, but rather by misalignment during COLMAP’s sparse reconstruction, which assigned incorrect camera poses. Consequently, the Gaussian splats were misplaced based on these faulty poses.&lt;/p&gt;

&lt;h2 id=&quot;videos-and-images&quot;&gt;Videos and Images&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Capture images with good texture. Avoid completely texture-less images (e.g., a white wall or empty desk). If the scene does not contain enough texture itself, you could place additional background objects, such as posters, etc.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Capture images at similar illumination conditions. Avoid high dynamic range scenes (e.g., pictures against the sun with shadows or pictures through doors/windows). Avoid specularities on shiny surfaces.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Capture images with high visual overlap. Make sure that each object is seen in at least 3 images – the more images the better.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Capture images from different viewpoints. Do not take images from the same location by only rotating the camera, e.g., make a few steps after each shot. At the same time, try to have enough images from a relatively similar viewpoint. Note that more images is not necessarily better and might lead to a slow reconstruction process. If you use a video as input, consider down-sampling the frame rate.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;future-work-and-promising-algorithms&quot;&gt;Future Work and Promising Algorithms&lt;/h1&gt;

&lt;p&gt;The field of computer vision is advancing rapidly, with new papers and techniques emerging almost daily that overcome technical challenges. As a result, future algorithms are likely to provide faster, higher-quality, and more practical 3D reconstructions.&lt;/p&gt;

&lt;p&gt;I am particularly excited about exploring Gaussian Splatting for dynamic scenes—that is, handling temporal changes to capture 4D information, rather than just static 3D scenes like the C-arm machine example above.&lt;/p&gt;

&lt;h1 id=&quot;blog-posts-on-this-topics&quot;&gt;Blog posts on this topics&lt;/h1&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;/deep_learning/2025/08/10/GaussianSplatting_introduction/&quot;&gt;Gaussian Splatting - Introduction&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/deep_learning/2025/08/11/GaussianSplatting_toyExample/&quot;&gt;Gaussian Splatting - Toy Example&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/deep_learning/2025/08/12/GaussianSplatting_camera_poses/&quot;&gt;Gaussian Splatting - Camera Poses&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/deep_learning/2025/08/13/GaussianSplatting_gaussianSplatting/&quot;&gt;Gaussian Splatting - Gaussian Splatting&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/deep_learning/2025/08/14/GaussianSplatting_meshesAndBeyond/&quot;&gt;Gaussian Splatting - Meshes and Beyond&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/deep_learning/2025/08/19/GaussianSplatting_failuresAndLessons/&quot;&gt;Gaussian Splatting - Failure, Success, and Lesson Learned&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Wilson Fok</name></author><category term="Deep_Learning" /><summary type="html">Failures Image Quantity and Quality While most parameters—such as model settings, data pipeline configurations, and optimization settings—work reasonably well out of the box, I found two factors especially critical for the quality of Gaussian splatting and sparse scene reconstruction. The first is the number of input images. In my initial experiments, I used only one frame out of every 15–20 frames, which meant discarding many valuable images from the video. This led to reconstruction errors caused by the low frame rate. When I increased the number of frames retained from 20 down to 15 between images, the reconstruction improved significantly while keeping the computational cost manageable. The second factor is the number of optimization iterations. The default of 30,000 iterations produces good results, but longer runs can further improve quality; in one experiment, I extended training to 93,000 iterations. A third limiting factor is image resolution. The resulting mesh quality was relatively low, largely due to a combination of limited frames and low-resolution input images. Image resolution is constrained by available GPU memory—higher resolution images require GPUs with greater memory capacity. Recap: Successes There are many steps in this pipeline where errors can occur, so I highly recommend a process of experimentation and trial-and-error. A word of caution: early-stage errors tend to propagate and can severely degrade final results. For example, if COLMAP’s sparse reconstruction is inaccurate or fails, subsequent Gaussian splatting will produce flawed outputs. This was evident in the medial-lateral C-arm Gaussian splatting result, where an extra phantom arm appeared—an artifact not present in the real object. This mistake did not creep in during Gaussian splatting step, but rather by misalignment during COLMAP’s sparse reconstruction, which assigned incorrect camera poses. Consequently, the Gaussian splats were misplaced based on these faulty poses. Videos and Images Capture images with good texture. Avoid completely texture-less images (e.g., a white wall or empty desk). If the scene does not contain enough texture itself, you could place additional background objects, such as posters, etc. Capture images at similar illumination conditions. Avoid high dynamic range scenes (e.g., pictures against the sun with shadows or pictures through doors/windows). Avoid specularities on shiny surfaces. Capture images with high visual overlap. Make sure that each object is seen in at least 3 images – the more images the better. Capture images from different viewpoints. Do not take images from the same location by only rotating the camera, e.g., make a few steps after each shot. At the same time, try to have enough images from a relatively similar viewpoint. Note that more images is not necessarily better and might lead to a slow reconstruction process. If you use a video as input, consider down-sampling the frame rate. Future Work and Promising Algorithms The field of computer vision is advancing rapidly, with new papers and techniques emerging almost daily that overcome technical challenges. As a result, future algorithms are likely to provide faster, higher-quality, and more practical 3D reconstructions. I am particularly excited about exploring Gaussian Splatting for dynamic scenes—that is, handling temporal changes to capture 4D information, rather than just static 3D scenes like the C-arm machine example above. Blog posts on this topics Gaussian Splatting - Introduction Gaussian Splatting - Toy Example Gaussian Splatting - Camera Poses Gaussian Splatting - Gaussian Splatting Gaussian Splatting - Meshes and Beyond Gaussian Splatting - Failure, Success, and Lesson Learned</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="/assets/images/gaussianSplatting/cover.avif" /><media:content medium="image" url="/assets/images/gaussianSplatting/cover.avif" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Gaussian Splatting - Meshes and Beyond</title><link href="/deep_learning/2025/08/14/GaussianSplatting_meshesAndBeyond/" rel="alternate" type="text/html" title="Gaussian Splatting - Meshes and Beyond" /><published>2025-08-14T00:00:00+08:00</published><updated>2025-08-14T00:00:00+08:00</updated><id>/deep_learning/2025/08/14/GaussianSplatting_meshesAndBeyond</id><content type="html" xml:base="/deep_learning/2025/08/14/GaussianSplatting_meshesAndBeyond/">&lt;h1 id=&quot;application-to-other-domain-in-computer-vision&quot;&gt;Application to other domain in Computer Vision&lt;/h1&gt;

&lt;p&gt;The mesh representation is highly versatile and can be manipulated across various applications such as Blender and Unity. However, what determines the mesh quality and the fine, intricate details of the C-arm machine remains unclear to me. To investigate, I varied the raw image resolution during the training of Gaussian Splatting to see if changes in image plane resolution affect the level of detail in the mesh.&lt;/p&gt;

&lt;p&gt;Since lower-resolution images provide fewer details—averaging more color and light information over a larger grid area—the Gaussian splats also reflect this loss of information. The figures below illustrate this behavior clearly.&lt;/p&gt;

&lt;div class=&quot;juxtapose&quot;&gt;
    &lt;img src=&quot;/assets/images/gaussianSplatting/resolutions/240x134.png&quot; /&gt;
    &lt;img src=&quot;/assets/images/gaussianSplatting/resolutions/1920x1080.png&quot; /&gt;
&lt;/div&gt;
&lt;script src=&quot;https://cdn.knightlab.com/libs/juxtapose/latest/js/juxtapose.min.js&quot;&gt;&lt;/script&gt;

&lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.knightlab.com/libs/juxtapose/latest/css/juxtapose.css&quot; /&gt;

&lt;p&gt;The image on the left shows a mesh trained with low resolution images (240x134); the right shows a mesh trained with high resolution images (1920x1080)&lt;/p&gt;

&lt;p&gt;The image on the left shows a mesh trained with low-resolution images (240x134), while the one on the right shows a mesh trained with high-resolution images (1920x1080). By comparing and contrasting these two images (move the slider in the center), we can see that features such as the keyboard, the calibration sheet, the foot pedal of the C-arm, and the sharp edges of various components are richer and more distinctive in the high-resolution mesh.&lt;/p&gt;

&lt;p&gt;Tips: The Python Open3D (O3D) package offers several useful functions for mesh post-processing. Its visualization features are particularly powerful because they leverage GPU acceleration. Since NumPy functions are highly optimized and the mesh contains many vertices and faces, I avoid Python for-loops and instead rely on NumPy arrays, advanced indexing, and boolean operations to process the mesh as efficiently as possible.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;open3d&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;o3d&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;os&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;tqdm&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tqdm&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;crop_mesh_with_radius&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mesh_path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output_path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;radius&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;mesh&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;o3d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;io&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read_triangle_mesh&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mesh_path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;vertices&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;asarray&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mesh&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vertices&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;reading mesh from&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mesh_path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Original mesh has &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vertices&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; vertices and &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mesh&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;triangles&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; triangles.&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# Calculate center of mass
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;center&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vertices&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Center of mass: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;center&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# Plot original mesh
&lt;/span&gt;    &lt;span class=&quot;c1&quot;&gt;# print(&quot;Plotting original mesh...&quot;)
&lt;/span&gt;    &lt;span class=&quot;c1&quot;&gt;# o3d.visualization.draw_geometries([mesh], window_name=&quot;Original Mesh&quot;)
&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# Distance from center
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;dists&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linalg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;norm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vertices&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;center&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;kept_indices&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;where&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dists&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;radius&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;kept_indices&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;kept_indices&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kept_indices&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# index_map = {int(old_idx): new_idx for new_idx, old_idx in enumerate(kept_indices)}
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;new_vertices&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vertices&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;kept_indices&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;vertex_mask&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vertices&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;bool&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;  
    &lt;span class=&quot;n&quot;&gt;vertex_mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;kept_indices&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# triangles: (n_triangles, 3)
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;triangles&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;asarray&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mesh&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;triangles&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;assert&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;triangles&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;triangles&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# Create a mask for triangles where all vertices are kept
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;triangle_mask&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vertex_mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;triangles&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;all&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;filtered_triangles&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;triangles&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;triangle_mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# Mapping from old to new vertex indices
&lt;/span&gt;    &lt;span class=&quot;c1&quot;&gt;# index_map = {old_idx: new_idx for new_idx, old_idx in enumerate(kept_indices)}
&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# kept_indices: (n_kept,), triangles: (n_triangle, 3)
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;remap_array&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;full&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vertices&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;remap_array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;kept_indices&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;kept_indices&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;filtered_triangles_mapped&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;remap_array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;filtered_triangles&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# (n_kept_triangle, 3)
&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# filtered_triangles_mapped = np.vectorize(index_map.get)(filtered_triangles)
&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;filtered_triangles_mapped&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;filtered_triangles_mapped&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;filtered_triangles_mapped&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    
    &lt;span class=&quot;c1&quot;&gt;# filtered_triangles = []
&lt;/span&gt;    &lt;span class=&quot;c1&quot;&gt;# for tri in tqdm(triangles):
&lt;/span&gt;    &lt;span class=&quot;c1&quot;&gt;#     if set(tri).issubset(index_map.keys()):
&lt;/span&gt;    &lt;span class=&quot;c1&quot;&gt;#         filtered_triangles.append([index_map[v] for v in tri])
&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;cropped_mesh&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;o3d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;geometry&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;TriangleMesh&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;o3d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;utility&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Vector3dVector&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;new_vertices&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;o3d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;utility&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Vector3iVector&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;filtered_triangles_mapped&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    

    &lt;span class=&quot;c1&quot;&gt;# Transfer vertex colors if they exist
&lt;/span&gt;    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mesh&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vertex_colors&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;old_vertex_colors&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;asarray&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mesh&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vertex_colors&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;new_vertex_colors&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;old_vertex_colors&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;kept_indices&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;cropped_mesh&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vertex_colors&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;o3d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;utility&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Vector3dVector&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;new_vertex_colors&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;


    &lt;span class=&quot;c1&quot;&gt;# Plot mesh after cropping
&lt;/span&gt;    &lt;span class=&quot;c1&quot;&gt;# print(&quot;Plotting cropped mesh...&quot;)
&lt;/span&gt;    &lt;span class=&quot;c1&quot;&gt;# o3d.visualization.draw_geometries([cropped_mesh], window_name=&quot;Cropped Mesh&quot;)
&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;o3d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;io&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;write_triangle_mesh&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;output_path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cropped_mesh&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Cropped mesh saved to &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;output_path&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;


&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;keep_largest_cluster&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mesh_path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output_path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;mesh&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;o3d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;io&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read_triangle_mesh&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mesh_path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;reading mesh from&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mesh_path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;vertices&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;asarray&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mesh&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vertices&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Original mesh has &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vertices&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; vertices and &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mesh&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;triangles&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; triangles.&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;o3d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;utility&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;VerbosityContextManager&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;o3d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;utility&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;VerbosityLevel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Debug&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;triangle_clusters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cluster_n_triangles&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cluster_area&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;mesh&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cluster_connected_triangles&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

        &lt;span class=&quot;n&quot;&gt;cluster_indices&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;argsort&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cluster_n_triangles&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[::&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# Largest first
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;largest_cluster_idx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cluster_indices&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Largest cluster index: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;largest_cluster_idx&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;, Size: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cluster_n_triangles&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;largest_cluster_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

        &lt;span class=&quot;c1&quot;&gt;# Create a mask for triangles belonging to the largest cluster
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;triangle_mask&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;triangle_clusters&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;largest_cluster_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;filtered_triangles&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;asarray&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mesh&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;triangles&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;triangle_mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

        &lt;span class=&quot;c1&quot;&gt;# Find unique vertices used in the filtered triangles
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;unique_vertex_indices&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;unique&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;filtered_triangles&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;new_vertices&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vertices&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;unique_vertex_indices&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

        &lt;span class=&quot;c1&quot;&gt;# Mapping from old to new vertex indices
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;index_map&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;old_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;new_idx&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;new_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;old_idx&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;enumerate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;unique_vertex_indices&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)}&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;filtered_triangles_mapped&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vectorize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;index_map&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;filtered_triangles&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

        &lt;span class=&quot;n&quot;&gt;cropped_mesh&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;o3d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;geometry&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;TriangleMesh&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;o3d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;utility&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Vector3dVector&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;new_vertices&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;o3d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;utility&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Vector3iVector&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;filtered_triangles_mapped&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

        &lt;span class=&quot;c1&quot;&gt;# Transfer vertex colors if they exist
&lt;/span&gt;        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mesh&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vertex_colors&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;old_vertex_colors&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;asarray&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mesh&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vertex_colors&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;new_vertex_colors&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;old_vertex_colors&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;unique_vertex_indices&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;cropped_mesh&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vertex_colors&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;o3d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;utility&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Vector3dVector&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;new_vertex_colors&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;


        &lt;span class=&quot;n&quot;&gt;cropped_mesh&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;remove_unreferenced_vertices&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;cropped_mesh&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;remove_degenerate_triangles&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

        &lt;span class=&quot;c1&quot;&gt;# Plot mesh after cropping
&lt;/span&gt;        &lt;span class=&quot;c1&quot;&gt;# print(&quot;Plotting cropped mesh...&quot;)
&lt;/span&gt;        &lt;span class=&quot;c1&quot;&gt;# o3d.visualization.draw_geometries([cropped_mesh], window_name=&quot;Cropped Mesh&quot;)
&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;o3d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;io&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;write_triangle_mesh&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;output_path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cropped_mesh&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Cropped mesh saved to &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;output_path&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;


&lt;span class=&quot;n&quot;&gt;root_dir&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;C:\Users\hp\tableTop\gs\2d-gaussian-splatting\outputs\lifesync\VID_20250725_173235_20_93&quot;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mesh_dir&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;mesh_dir&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;join&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;root_dir&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;15_&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mesh_dir&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;train&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;ours_93000&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;crop_mesh_with_radius&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;join&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mesh_dir&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;fuse_unbounded.ply&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;join&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mesh_dir&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;fuse_cropped.ply&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;radius&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;4.5&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;keep_largest_cluster&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;join&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mesh_dir&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;fuse_cropped.ply&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;join&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mesh_dir&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;fuse_cropped_largest.ply&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# break
&lt;/span&gt;



&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;blender&quot;&gt;Blender&lt;/h3&gt;
&lt;p&gt;After importing the mesh as a PLY file, Blender allows for extensive editing capabilities. To showcase a compelling lighting effect, I positioned sun light and point light sources near the C-arm model and rendered a video in Blender. The camera was set to follow a circular trajectory, smoothly tracking the target object.&lt;/p&gt;

&lt;p&gt;Note: Be sure to enable the Geometry Editor and Shader features.&lt;/p&gt;

&lt;p&gt;Switch on Geometry editor to convert the point cloud to point representation.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/gaussianSplatting/geometry.png&quot; style=&quot;display: block; margin-left: auto; margin-right: auto; width: 90%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Assign the color stored as Col (blender is case sensitive) as the color. this color can be saved as a material that is assigned to the geometry&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/gaussianSplatting/shader.png&quot; style=&quot;display: block; margin-left: auto; margin-right: auto; width: 90%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Convert point representation to meshes so that the meshes can later be exported to fbx or other preferred file formats.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/gaussianSplatting/geometryMesh.png&quot; style=&quot;display: block; margin-left: auto; margin-right: auto; width: 90%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;For convenience, Cycles rendering was disabled, and EEVEE was used instead. While the quality is comparable, EEVEE offers significantly faster rendering times.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Mobile C-arm Machine Model created by 2D Gaussian Splatting in Blender with lights rendered by EEVEE&lt;/em&gt;&lt;/p&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/bnDNn8VHPwg?si=GdEjrQ04J6_l6Tyk&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&quot; referrerpolicy=&quot;strict-origin-when-cross-origin&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;After further tuning and adjustments, the resulting video better showcases the high-resolution quality of the mesh. Newton famously said he was “standing on the shoulders of giants,” and inspired by that spirit, I integrated output meshes generated by the original 2D Gaussian Splatting research team onto the 2D C-arm model. This highlights how digital assets can be recombined and repurposed in new and creative ways.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/gaussianSplatting/mixScene.png&quot; style=&quot;display: block; margin-left: auto; margin-right: auto; width: 90%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;However, a useful tip is to convert the point cloud to a mesh as the mesh looks more concrete. Below is the procedure to accomplish this with this (DTU) dataset.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;High Quality and Resolution 2D Gaussian Splatting 2D C-arm Machine in ML Orientation&lt;/em&gt;&lt;/p&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/RlMIy31elyc?si=XTtB-nG5deGMJR6Z&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&quot; referrerpolicy=&quot;strict-origin-when-cross-origin&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;&lt;em&gt;High Quality and Resolution 2D Gaussian Splatting 2D C-arm Machine and DTU Meshes in Blender&lt;/em&gt;&lt;/p&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/l43mT6j3ZFc?si=C4wEkI1RaGO8V5HM&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&quot; referrerpolicy=&quot;strict-origin-when-cross-origin&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;h3 id=&quot;unity&quot;&gt;Unity&lt;/h3&gt;
&lt;p&gt;Unity is a powerful and widely-used game engine that enables designers to create interactive 2D and 3D experiences. A natural question arises: can our mesh be used as an asset in Unity?&lt;/p&gt;

&lt;p&gt;Unfortunately Unity does not natively support PLY meshes. Nonetheless, this limitation can be overcome by installing a free unity package to handle ply files. &lt;a href=&quot;https://assetstore.unity.com/packages/p/point-cloud-free-viewer-19811&quot;&gt;point-cloud-free-viewer&lt;/a&gt;. This point could free viewer package successfully imports PLY files as point cloud game object in Unity 6. This point cloud of C-arm machine looks impressive sitting at the corner in this Unity VR example scene.&lt;/p&gt;

&lt;p&gt;Yet, interactive features such as rigid bodies, mesh colliders, and grab interactions are currently incompatible with pure point cloud objects, as the C-arm lacks a triangular mesh structure.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/gaussianSplatting/playMode.png&quot; style=&quot;display: block; margin-left: auto; margin-right: auto; width: 90%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/gaussianSplatting/closeUp_playMode.png&quot; style=&quot;display: block; margin-left: auto; margin-right: auto; width: 90%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;By converting the point cloud to a mesh in Blender, we can export it as an FBX file, which Unity readily imports as a game object. Once imported, adding XR grab interactable components allows the model to respond to physics — for example, falling and breaking under gravity (with gravity set to 9.8 m/s² in Unity). A handy Unity package is called Recorder, which helps me to record play mode / simulation mode into a video file. After further tuning and adjustments, better, prettier, sharper are obtained.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Virtual Reality Tutorial Example showing a Gaussian Splatting Pointcloud for a 2D C-arm Machine in Unity&lt;/em&gt;&lt;/p&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/sJ_40LogvdM?si=hdTy604rM06lhM-x&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&quot; referrerpolicy=&quot;strict-origin-when-cross-origin&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;&lt;em&gt;Virtual Reality Tutorial Example showing a Gaussian Splatting Meshes become Interactables in Unity&lt;/em&gt;&lt;/p&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/_hOFuMI3sc4?si=CRK0IyikGJgP2l8L&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&quot; referrerpolicy=&quot;strict-origin-when-cross-origin&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;Following up next is &lt;a href=&quot;/deep_learning/2025/08/19/GaussianSplatting_failuresAndLessons/&quot;&gt;Gaussian Splatting - Failure, Success, and Lesson Learned&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;blog-posts-on-this-topics&quot;&gt;Blog posts on this topics&lt;/h1&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;/deep_learning/2025/08/10/GaussianSplatting_introduction/&quot;&gt;Gaussian Splatting - Introduction&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/deep_learning/2025/08/11/GaussianSplatting_toyExample/&quot;&gt;Gaussian Splatting - Toy Example&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/deep_learning/2025/08/12/GaussianSplatting_camera_poses/&quot;&gt;Gaussian Splatting - Camera Poses&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/deep_learning/2025/08/13/GaussianSplatting_gaussianSplatting/&quot;&gt;Gaussian Splatting - Gaussian Splatting&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/deep_learning/2025/08/14/GaussianSplatting_meshesAndBeyond/&quot;&gt;Gaussian Splatting - Meshes and Beyond&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/deep_learning/2025/08/19/GaussianSplatting_failuresAndLessons/&quot;&gt;Gaussian Splatting - Failure, Success, and Lesson Learned&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Wilson Fok</name></author><category term="Deep_Learning" /><summary type="html">Application to other domain in Computer Vision The mesh representation is highly versatile and can be manipulated across various applications such as Blender and Unity. However, what determines the mesh quality and the fine, intricate details of the C-arm machine remains unclear to me. To investigate, I varied the raw image resolution during the training of Gaussian Splatting to see if changes in image plane resolution affect the level of detail in the mesh. Since lower-resolution images provide fewer details—averaging more color and light information over a larger grid area—the Gaussian splats also reflect this loss of information. The figures below illustrate this behavior clearly. The image on the left shows a mesh trained with low resolution images (240x134); the right shows a mesh trained with high resolution images (1920x1080) The image on the left shows a mesh trained with low-resolution images (240x134), while the one on the right shows a mesh trained with high-resolution images (1920x1080). By comparing and contrasting these two images (move the slider in the center), we can see that features such as the keyboard, the calibration sheet, the foot pedal of the C-arm, and the sharp edges of various components are richer and more distinctive in the high-resolution mesh. Tips: The Python Open3D (O3D) package offers several useful functions for mesh post-processing. Its visualization features are particularly powerful because they leverage GPU acceleration. Since NumPy functions are highly optimized and the mesh contains many vertices and faces, I avoid Python for-loops and instead rely on NumPy arrays, advanced indexing, and boolean operations to process the mesh as efficiently as possible. import open3d as o3d import numpy as np import os from tqdm import tqdm def crop_mesh_with_radius(mesh_path, output_path, radius): mesh = o3d.io.read_triangle_mesh(mesh_path) vertices = np.asarray(mesh.vertices) print(&quot;reading mesh from&quot;, mesh_path) print(f&quot;Original mesh has {len(vertices)} vertices and {len(mesh.triangles)} triangles.&quot;) # Calculate center of mass center = vertices.mean(axis=0) print(f&quot;Center of mass: {center}&quot;) # Plot original mesh # print(&quot;Plotting original mesh...&quot;) # o3d.visualization.draw_geometries([mesh], window_name=&quot;Original Mesh&quot;) # Distance from center dists = np.linalg.norm(vertices - center, axis=1) kept_indices = np.where(dists &amp;lt;= radius)[0] print(kept_indices, type(kept_indices), kept_indices.shape) # index_map = {int(old_idx): new_idx for new_idx, old_idx in enumerate(kept_indices)} new_vertices = vertices[kept_indices] vertex_mask = np.zeros(vertices.shape[0], dtype=bool) vertex_mask[kept_indices] = True # triangles: (n_triangles, 3) triangles = np.asarray(mesh.triangles) assert len(triangles.shape) ==2 and triangles.shape[1] == 3 # Create a mask for triangles where all vertices are kept triangle_mask = vertex_mask[triangles].all(axis=1) filtered_triangles = triangles[triangle_mask] # Mapping from old to new vertex indices # index_map = {old_idx: new_idx for new_idx, old_idx in enumerate(kept_indices)} # kept_indices: (n_kept,), triangles: (n_triangle, 3) remap_array = np.full(vertices.shape[0], -1, dtype=int) remap_array[kept_indices] = np.arange(len(kept_indices)) filtered_triangles_mapped = remap_array[filtered_triangles] # (n_kept_triangle, 3) # filtered_triangles_mapped = np.vectorize(index_map.get)(filtered_triangles) print(filtered_triangles_mapped, type(filtered_triangles_mapped), filtered_triangles_mapped.shape) # filtered_triangles = [] # for tri in tqdm(triangles): # if set(tri).issubset(index_map.keys()): # filtered_triangles.append([index_map[v] for v in tri]) cropped_mesh = o3d.geometry.TriangleMesh( o3d.utility.Vector3dVector(new_vertices), o3d.utility.Vector3iVector(filtered_triangles_mapped) ) # Transfer vertex colors if they exist if len(mesh.vertex_colors) &amp;gt; 0: old_vertex_colors = np.asarray(mesh.vertex_colors) new_vertex_colors = old_vertex_colors[kept_indices] cropped_mesh.vertex_colors = o3d.utility.Vector3dVector(new_vertex_colors) # Plot mesh after cropping # print(&quot;Plotting cropped mesh...&quot;) # o3d.visualization.draw_geometries([cropped_mesh], window_name=&quot;Cropped Mesh&quot;) o3d.io.write_triangle_mesh(output_path, cropped_mesh) print(f&quot;Cropped mesh saved to {output_path}&quot;) def keep_largest_cluster(mesh_path, output_path): mesh = o3d.io.read_triangle_mesh(mesh_path) print(&quot;reading mesh from&quot;, mesh_path) vertices = np.asarray(mesh.vertices) print(f&quot;Original mesh has {len(vertices)} vertices and {len(mesh.triangles)} triangles.&quot;) with o3d.utility.VerbosityContextManager(o3d.utility.VerbosityLevel.Debug) as cm: triangle_clusters, cluster_n_triangles, cluster_area = ( mesh.cluster_connected_triangles() ) cluster_indices = np.argsort(cluster_n_triangles)[::-1] # Largest first largest_cluster_idx = cluster_indices[0] print(f&quot;Largest cluster index: {largest_cluster_idx}, Size: {cluster_n_triangles[largest_cluster_idx]}&quot;) # Create a mask for triangles belonging to the largest cluster triangle_mask = (triangle_clusters == largest_cluster_idx) filtered_triangles = np.asarray(mesh.triangles)[triangle_mask] # Find unique vertices used in the filtered triangles unique_vertex_indices = np.unique(filtered_triangles) new_vertices = vertices[unique_vertex_indices] # Mapping from old to new vertex indices index_map = {old_idx: new_idx for new_idx, old_idx in enumerate(unique_vertex_indices)} filtered_triangles_mapped = np.vectorize(index_map.get)(filtered_triangles) cropped_mesh = o3d.geometry.TriangleMesh( o3d.utility.Vector3dVector(new_vertices), o3d.utility.Vector3iVector(filtered_triangles_mapped) ) # Transfer vertex colors if they exist if len(mesh.vertex_colors) &amp;gt; 0: old_vertex_colors = np.asarray(mesh.vertex_colors) new_vertex_colors = old_vertex_colors[unique_vertex_indices] cropped_mesh.vertex_colors = o3d.utility.Vector3dVector(new_vertex_colors) cropped_mesh.remove_unreferenced_vertices() cropped_mesh.remove_degenerate_triangles() # Plot mesh after cropping # print(&quot;Plotting cropped mesh...&quot;) # o3d.visualization.draw_geometries([cropped_mesh], window_name=&quot;Cropped Mesh&quot;) o3d.io.write_triangle_mesh(output_path, cropped_mesh) print(f&quot;Cropped mesh saved to {output_path}&quot;) root_dir =r&quot;C:\Users\hp\tableTop\gs\2d-gaussian-splatting\outputs\lifesync\VID_20250725_173235_20_93&quot; for mesh_dir in [1,2,4,8]: mesh_dir = os.path.join(root_dir, &quot;15_&quot; + str(mesh_dir), &quot;train&quot;, &quot;ours_93000&quot;) crop_mesh_with_radius( os.path.join(mesh_dir, &quot;fuse_unbounded.ply&quot;), os.path.join(mesh_dir, &quot;fuse_cropped.ply&quot;), radius=4.5 ) keep_largest_cluster( os.path.join(mesh_dir, &quot;fuse_cropped.ply&quot;), os.path.join(mesh_dir, &quot;fuse_cropped_largest.ply&quot;) ) # break Blender After importing the mesh as a PLY file, Blender allows for extensive editing capabilities. To showcase a compelling lighting effect, I positioned sun light and point light sources near the C-arm model and rendered a video in Blender. The camera was set to follow a circular trajectory, smoothly tracking the target object. Note: Be sure to enable the Geometry Editor and Shader features. Switch on Geometry editor to convert the point cloud to point representation. Assign the color stored as Col (blender is case sensitive) as the color. this color can be saved as a material that is assigned to the geometry Convert point representation to meshes so that the meshes can later be exported to fbx or other preferred file formats. For convenience, Cycles rendering was disabled, and EEVEE was used instead. While the quality is comparable, EEVEE offers significantly faster rendering times. Mobile C-arm Machine Model created by 2D Gaussian Splatting in Blender with lights rendered by EEVEE After further tuning and adjustments, the resulting video better showcases the high-resolution quality of the mesh. Newton famously said he was “standing on the shoulders of giants,” and inspired by that spirit, I integrated output meshes generated by the original 2D Gaussian Splatting research team onto the 2D C-arm model. This highlights how digital assets can be recombined and repurposed in new and creative ways. However, a useful tip is to convert the point cloud to a mesh as the mesh looks more concrete. Below is the procedure to accomplish this with this (DTU) dataset. High Quality and Resolution 2D Gaussian Splatting 2D C-arm Machine in ML Orientation High Quality and Resolution 2D Gaussian Splatting 2D C-arm Machine and DTU Meshes in Blender Unity Unity is a powerful and widely-used game engine that enables designers to create interactive 2D and 3D experiences. A natural question arises: can our mesh be used as an asset in Unity? Unfortunately Unity does not natively support PLY meshes. Nonetheless, this limitation can be overcome by installing a free unity package to handle ply files. point-cloud-free-viewer. This point could free viewer package successfully imports PLY files as point cloud game object in Unity 6. This point cloud of C-arm machine looks impressive sitting at the corner in this Unity VR example scene. Yet, interactive features such as rigid bodies, mesh colliders, and grab interactions are currently incompatible with pure point cloud objects, as the C-arm lacks a triangular mesh structure. By converting the point cloud to a mesh in Blender, we can export it as an FBX file, which Unity readily imports as a game object. Once imported, adding XR grab interactable components allows the model to respond to physics — for example, falling and breaking under gravity (with gravity set to 9.8 m/s² in Unity). A handy Unity package is called Recorder, which helps me to record play mode / simulation mode into a video file. After further tuning and adjustments, better, prettier, sharper are obtained. Virtual Reality Tutorial Example showing a Gaussian Splatting Pointcloud for a 2D C-arm Machine in Unity Virtual Reality Tutorial Example showing a Gaussian Splatting Meshes become Interactables in Unity Following up next is Gaussian Splatting - Failure, Success, and Lesson Learned Blog posts on this topics Gaussian Splatting - Introduction Gaussian Splatting - Toy Example Gaussian Splatting - Camera Poses Gaussian Splatting - Gaussian Splatting Gaussian Splatting - Meshes and Beyond Gaussian Splatting - Failure, Success, and Lesson Learned</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="/assets/images/gaussianSplatting/cover.avif" /><media:content medium="image" url="/assets/images/gaussianSplatting/cover.avif" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Gaussian Splatting - Gaussian Splatting</title><link href="/deep_learning/2025/08/13/GaussianSplatting_gaussianSplatting/" rel="alternate" type="text/html" title="Gaussian Splatting - Gaussian Splatting" /><published>2025-08-13T00:00:00+08:00</published><updated>2025-08-13T00:00:00+08:00</updated><id>/deep_learning/2025/08/13/GaussianSplatting_gaussianSplatting</id><content type="html" xml:base="/deep_learning/2025/08/13/GaussianSplatting_gaussianSplatting/">&lt;h1 id=&quot;how-to-perform-gaussian-splatting&quot;&gt;How to perform Gaussian Splatting?&lt;/h1&gt;

&lt;p&gt;The Gaussian Splatting Python scripts are provided in the original publication - &lt;a href=&quot;https://github.com/graphdeco-inria/gaussian-splatting&quot;&gt;3D Gaussian Splatting for Real-Time Radiance Field Rendering&lt;/a&gt;.
To use these scripts, you must first set up the required Python environment. While the setup may seem straightforward, I encountered several challenges during the process that I document here to help others avoid them.&lt;/p&gt;

&lt;p&gt;The code was first published in 2023, and many dependencies are no longer readily available or compatible with the latest software versions. A common mistake I made was attempting to run it with the most up-to-date packages, which led to numerous incompatibility issues.&lt;/p&gt;

&lt;h3 id=&quot;hardware-specification&quot;&gt;Hardware Specification&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;GPU:&lt;/strong&gt; Nvidia GeForce RTX 4070, 8 GB VRAM&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Driver version:&lt;/strong&gt; 580.88&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;CUDA:&lt;/strong&gt; 11.8&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;CPU:&lt;/strong&gt; Intel i7-13620H&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;OS:&lt;/strong&gt; Windows 11&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;RAM:&lt;/strong&gt; 40 GB (Windows page file 60 GB)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;python-and-package-installation&quot;&gt;Python and Package installation&lt;/h3&gt;
&lt;p&gt;I installed both Python 3.8 and 3.10, finding Python 3.10 generally more compatible with most packages since 3.8 is becoming obsolete. Most packages install smoothly as pre-compiled wheels, but some, especially CUDA-related custom modules in Gaussian Splatting, require local compilation.&lt;/p&gt;

&lt;p&gt;Making sure the compilation tools are of the compatible version with these packages can be tricky. Since most of the time, the authors would not have tested many cases and their success configuration may not be the same as what I have got. After many experiments by trial and error, I have worked out a solution as follows:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Microsoft Visual Studio Build Tools 2019 (v143 toolset):&lt;/strong&gt;&lt;br /&gt;
Avoid the latest Visual Studio versions like 17.14, which are not backward compatible.&lt;/li&gt;
  &lt;li&gt;Always compile from the &lt;strong&gt;VS x64 Native Tools Command Prompt&lt;/strong&gt; to ensure the correct compiler is called.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;With this setup, I could follow the official installation guides without issues.&lt;/p&gt;

&lt;h3 id=&quot;tips-and-common-issues&quot;&gt;Tips and Common Issues&lt;/h3&gt;

&lt;p&gt;Tips: If we use install torch and torch related packages on pip, it is likely that pip install the CPU-only version. So please visit &lt;a href=&quot;https://pytorch.org/get-started/previous-versions&quot;&gt;Pytorch official website&lt;/a&gt; to get the CUDA-enabled version that is compatible for your device.&lt;/p&gt;

&lt;p&gt;Watch out for: OMP: Error #15: Initializing libiomp5md.dll, but found libiomp5md.dll already initialized.
OMP: Hint This means that multiple copies of the OpenMP runtime have been linked into the program. That is dangerous, since it can degrade performance or cause incorrect results. The best thing to do is to ensure that only a single OpenMP runtime is linked into the process, e.g. by avoiding static linking of the OpenMP runtime in any library. As an unsafe, unsupported, undocumented workaround you can set the environment variable KMP_DUPLICATE_LIB_OK=TRUE to allow the program to continue to execute, but that may cause crashes or silently produce incorrect results. For more information, please see http://www.intel.com/software/products/support/.&lt;/p&gt;

&lt;h3 id=&quot;crash-and-blue-screen&quot;&gt;Crash and blue-screen&lt;/h3&gt;

&lt;p&gt;Initially, frequent crashes were blamed on Python or the packages. In hindsight, I have figured out the importance and usefulness of windows operating system event report feature. On many occasions, the system crashed due to mis-configured libraries and system settings. Of course, I had no idea at the beginning and mis-attributed the crash due to Python and its packages. After reinstalling Python and its packages, the situation showed no sign of any improvement. I then studied the event report features on windows to get a better understanding on what did occur right before a crash (Windows Event Viewer). The WINDGB tool was very useful because a dump file was generated to record the exact event and all the related programs or dll at that time right after the crash. WINDGB revealed the core issue - multiple CUDA versions installed on the system. I had installed and uninstalled different CUDA versions which corrupted Nvidia dll and caused several errors.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Final solution:&lt;/strong&gt;
Thus, the final solution is to update all the graphical drivers and libraries because all evidence pointed to an urgent driver updates (Nvidia and Intel graphic cards). After a CLEAN installation of the latest drivers, the crash had ceased eventually . As for CUDA, keep one and only one version (in this case CUDA 11.8) on the system. Otherwise, python setup may compile the source code against the wrong version.&lt;/p&gt;

&lt;p&gt;In addition to software upgrades, shutdown all memory intensive programs prior to running Python and increase Windows’ page memory as much as possible.&lt;/p&gt;

&lt;p&gt;After this step, system stability was restored and crashes stopped.&lt;/p&gt;

&lt;h3 id=&quot;training-3d-gaussian-splatting&quot;&gt;Training 3D Gaussian Splatting&lt;/h3&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# Run 3D AND 2D gaussian splatting train script
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;python&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;py&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;C&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;Users&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;hp&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;tableTop&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;gs&lt;/span&gt;\&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dmodeling&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;\&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dcarm&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;extracted_frames_VID_20250725_172445_20&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;outputs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;VID_20250725_172445_20_93&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;resolution&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iterations&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;93000&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;densification_interval&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;opacity_reset_interval&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3000&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;densify_from_iter&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;500&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;python&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;py&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;C&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;Users&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;hp&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;tableTop&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;gs&lt;/span&gt;\&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dmodeling&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;\&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dcarm&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;extracted_frames_VID_20250725_175410_20&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;outputs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;VID_20250725_175410_20_93&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;resolution&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iterations&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;93000&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;densification_interval&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;opacity_reset_interval&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3000&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;densify_from_iter&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;500&lt;/span&gt;


&lt;span class=&quot;n&quot;&gt;python&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;py&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;C&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;Users&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;hp&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;tableTop&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;gs&lt;/span&gt;\&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dmodeling&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;\&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dcarm&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;extracted_frames_VID_20250725_173235_20&lt;/span&gt;\&lt;span class=&quot;mi&quot;&gt;15&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;outputs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;VID_20250725_173235_20_93&lt;/span&gt;\&lt;span class=&quot;mi&quot;&gt;15&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;resolution&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iterations&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;93000&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;densification_interval&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;opacity_reset_interval&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3000&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;densify_from_iter&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;500&lt;/span&gt;




&lt;span class=&quot;n&quot;&gt;Note&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;this&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;laptop&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;has&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;limited&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;GPU&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;memory&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;See&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;computer&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;specification&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;so&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;the&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;images&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;are&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;downscaled&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;by&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;multiple&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;of&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;8.&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;e&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1080&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x1920&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;135&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x240&lt;/span&gt;




&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt;

&lt;p&gt;The Gaussian Splatting model was evaluated on a C-arm machine configured in both Anterior-Posterior (AP) and Medial-Lateral (ML) poses.&lt;/p&gt;

&lt;p&gt;There are two ways to assess the quality:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Quantitative metrics&lt;/strong&gt;: SSIM, PSNR, and LPIPS scores provide numerical evaluation of image quality.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Qualitative visualization&lt;/strong&gt;: Using &lt;a href=&quot;https://github.com/Florian-Barthel/splatviz&quot;&gt;Splatviz&lt;/a&gt;, which allows intuitive inspection of the reconstructions.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;ssim-psnr-lpips&quot;&gt;SSIM, PSNR, LPIPS&lt;/h2&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# evaluation script (evaluation on test images , a total of 53)
&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;python&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;render&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;py&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iteration&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;93000&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;C&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;Users&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;hp&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;tableTop&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;gs&lt;/span&gt;\&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dmodeling&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;\&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dcarm&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;extracted_frames_VID_20250725_172445_20&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;outputs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;VID_20250725_172445_20_93&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;eval&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;skip_train&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;python&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;render&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;py&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iteration&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;7000&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;C&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;Users&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;hp&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;tableTop&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;gs&lt;/span&gt;\&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dmodeling&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;\&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dcarm&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;extracted_frames_VID_20250725_172445_20&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;outputs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;VID_20250725_172445_20_93&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;eval&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;skip_train&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;python&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;render&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;py&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iteration&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;30000&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;C&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;Users&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;hp&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;tableTop&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;gs&lt;/span&gt;\&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dmodeling&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;\&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dcarm&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;extracted_frames_VID_20250725_172445_20&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;outputs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;VID_20250725_172445_20_93&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;eval&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;skip_train&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;python&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;metrics&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;py&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model_paths&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;outputs&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;VID_20250725_172445_20_93&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;python&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;render&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;py&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iteration&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;7000&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;C&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;Users&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;hp&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;tableTop&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;gs&lt;/span&gt;\&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dmodeling&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;\&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dcarm&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;extracted_frames_VID_20250725_175410_20&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;outputs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;VID_20250725_175410_20_93&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;eval&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;skip_train&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;python&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;render&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;py&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iteration&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;30000&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;C&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;Users&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;hp&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;tableTop&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;gs&lt;/span&gt;\&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dmodeling&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;\&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dcarm&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;extracted_frames_VID_20250725_175410_20&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;outputs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;VID_20250725_175410_20_93&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;eval&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;skip_train&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;python&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;render&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;py&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iteration&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;93000&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;C&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;Users&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;hp&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;tableTop&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;gs&lt;/span&gt;\&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dmodeling&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;\&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dcarm&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;extracted_frames_VID_20250725_175410_20&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;outputs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;VID_20250725_175410_20_93&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;eval&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;skip_train&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;python&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;metrics&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;py&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model_paths&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;outputs&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;VID_20250725_175410_20_93&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;python&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;render&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;py&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iteration&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;93000&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;C&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;Users&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;hp&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;tableTop&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;gs&lt;/span&gt;\&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dmodeling&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;\&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dcarm&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;extracted_frames_VID_20250725_173235_20&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;outputs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;VID_20250725_173235_20_93&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;eval&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;skip_train&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;python&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;render&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;py&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iteration&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;7000&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;C&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;Users&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;hp&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;tableTop&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;gs&lt;/span&gt;\&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dmodeling&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;\&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dcarm&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;extracted_frames_VID_20250725_173235_20&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;outputs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;VID_20250725_173235_20_93&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;eval&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;skip_train&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;python&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;render&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;py&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iteration&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;30000&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;C&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;Users&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;hp&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;tableTop&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;gs&lt;/span&gt;\&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dmodeling&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;\&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dcarm&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;extracted_frames_VID_20250725_173235_20&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;outputs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;VID_20250725_173235_20_93&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;eval&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;skip_train&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;python&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;metrics&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;py&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model_paths&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;outputs&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;VID_20250725_173235_20_93&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;anterior-posterior&quot;&gt;Anterior Posterior&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;AP Orientation&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;At iteration 7000&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;SSIM :    0.9049926
PSNR :   25.9037876
LPIPS:    0.1312765
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;At iteration 30000&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;SSIM :    0.9583084
PSNR :   30.5848618
LPIPS:    0.0602733
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;At iteration 93000&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;SSIM :    0.9692391
PSNR :   32.1568565
LPIPS:    0.0452350
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;medial-lateral&quot;&gt;Medial Lateral&lt;/h3&gt;

&lt;p&gt;&lt;em&gt;Vertical camera frames&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;At iteration 7000&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;SSIM :    0.9277247
PSNR :   26.7998466
LPIPS:    0.0987048
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;At iteration 30000&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;SSIM :    0.9654269
PSNR :   30.7180729
LPIPS:    0.0463373
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;At iteration 93000&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;SSIM :    0.9736962
PSNR :   32.5040054
LPIPS:    0.0358106
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;em&gt;Horizontal camera frames&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;At iteration 7000&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;SSIM :    0.8346600
PSNR :   22.8985519
LPIPS:    0.2103405
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;At iteration 30000&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;SSIM :    0.9369362
PSNR :   28.4063663
LPIPS:    0.0972803
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;At iteration 93000&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;SSIM :    0.9571357
PSNR :   30.3842888
LPIPS:    0.0668879
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;All in all, the Gaussian Splatting results are very good. Indeed, the more iterations, the better the quality. (albeit the improvement slows drastically)&lt;/p&gt;

&lt;p&gt;The videos were shot in two different camera orientations. The vertical and the horizontal show very similar end results. I think so long as the video quality is good and the reconstruction is accurate, Gaussian Splatting is quite robust that camera orientation does not matter.&lt;/p&gt;

&lt;h1 id=&quot;splatviz&quot;&gt;Splatviz&lt;/h1&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
&lt;span class=&quot;c1&quot;&gt;# Run splat-vis for visualization
&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;python&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;run_main&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;py&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data_path&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;C&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;Users&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;hp&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;tableTop&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;gs&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;gaussian&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;splatting&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;outputs&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;VID_20250725_172445_20_93_previous&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;point_cloud&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;iteration_93000&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;python&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;run_main&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;py&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data_path&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;C&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;Users&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;hp&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;tableTop&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;gs&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;gaussian&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;splatting&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;outputs&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;VID_20250725_172445_20_93&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;point_cloud&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;iteration_30000&lt;/span&gt;


&lt;span class=&quot;n&quot;&gt;python&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;run_main&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;py&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data_path&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;C&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;Users&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;hp&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;tableTop&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;gs&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;gaussian&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;splatting&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;outputs&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;VID_20250725_172445_20_93&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;point_cloud&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;iteration_93000&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;python&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;run_main&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;py&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data_path&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;C&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;Users&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;hp&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;tableTop&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;gs&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;gaussian&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;splatting&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;outputs&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;VID_20250725_175410_20_93&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;point_cloud&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;iteration_93000&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;anterior-posterior-1&quot;&gt;Anterior Posterior&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/gaussianSplatting/ap_splatvis.png&quot; style=&quot;display: block; margin-left: auto; margin-right: auto; width: 90%;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;medial-lateral-1&quot;&gt;Medial Lateral&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/gaussianSplatting/ml_splatvis.png&quot; style=&quot;display: block; margin-left: auto; margin-right: auto; width: 90%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Zoom in block to indicate filtering script for Gaussian ellipsoids display&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;filter&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Gaussian&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ellipsoids&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;mask&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linalg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;norm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_scaling&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;slider&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;gs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_xyz&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_xyz&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;gs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_rotation&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_rotation&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;gs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_scaling&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_scaling&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;gs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_opacity&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_opacity&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;gs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_features_dc&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_features_dc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;gs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_features_rest&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_features_rest&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;


&lt;span class=&quot;n&quot;&gt;mask&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linalg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;norm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_opacity&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;slider&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;gs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_xyz&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_xyz&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;gs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_rotation&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_rotation&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;gs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_scaling&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_scaling&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;gs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_opacity&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_opacity&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;gs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_features_dc&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_features_dc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;gs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_features_rest&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_features_rest&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h1 id=&quot;2dgs-2d-gaussian-splatting-for-geometrically-accurate-radiance-fields&quot;&gt;2DGS: 2D Gaussian Splatting for Geometrically Accurate Radiance Fields&lt;/h1&gt;

&lt;p&gt;&lt;a href=&quot;https://surfsplatting.github.io/&quot;&gt;2D Gaussian Splatting&lt;/a&gt; extends 3D Gaussian Splatting by addressing its limitation in representing mainly volumetric structures but not surfaces. Surfaces are better represented by &lt;strong&gt;2D surfels&lt;/strong&gt;, which are surface elements defined by:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;A 2D tangential disk with radius &lt;em&gt;r&lt;/em&gt; and a center point.&lt;/li&gt;
  &lt;li&gt;A normal vector pointing outward.&lt;/li&gt;
  &lt;li&gt;Graphics data such as color, texture, and depth.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Unlike meshes, surfels do not require connectivity between elements, allowing flexible level-of-detail adaptation: smaller and denser surfels for close-ups, larger and fewer for distant views, balancing quality and computation.&lt;/p&gt;

&lt;h3 id=&quot;using-2d-gaussian-splatting-for-3d-mesh-extraction-of-the-c-arm-machine&quot;&gt;Using 2D Gaussian Splatting for 3D Mesh Extraction of the C-arm Machine&lt;/h3&gt;

&lt;p&gt;I chose 2D Gaussian Splatting over the 3D version to extract surface meshes of the C-arm machine. The surfel surfaces were exported as PLY meshes or point clouds.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
&lt;span class=&quot;c1&quot;&gt;# mesh extraction and video generation
&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;python&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;render&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;py&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;C&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;Users&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;hp&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;tableTop&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;gs&lt;/span&gt;\&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gaussian&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;splatting&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;outputs&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;VID_20250725_172445_20_93&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;C&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;Users&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;hp&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;tableTop&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;gs&lt;/span&gt;\&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dmodeling&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;\&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dcarm&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;extracted_frames_VID_20250725_172445_20&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;unbounded&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mesh_res&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2048&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;python&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;render&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;py&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;C&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;Users&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;hp&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;tableTop&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;gs&lt;/span&gt;\&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gaussian&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;splatting&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;outputs&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;VID_20250725_175410_20_93&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;C&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;Users&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;hp&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;tableTop&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;gs&lt;/span&gt;\&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dmodeling&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;\&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dcarm&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;extracted_frames_VID_20250725_175410_20&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;unbounded&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mesh_res&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2048&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;python&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;render&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;py&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;C&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;Users&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;hp&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;tableTop&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;gs&lt;/span&gt;\&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gaussian&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;splatting&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;outputs&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;VID_20250725_173235_20_93&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;C&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;Users&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;hp&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;tableTop&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;gs&lt;/span&gt;\&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dmodeling&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;\&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dcarm&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;extracted_frames_VID_20250725_173235_20&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;unbounded&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mesh_res&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2048&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;render_path&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;python&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;render&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;py&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;C&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;Users&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;hp&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;tableTop&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;gs&lt;/span&gt;\&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gaussian&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;splatting&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;outputs&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;VID_20250725_173235_20_93&lt;/span&gt;\&lt;span class=&quot;mi&quot;&gt;15&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;C&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;Users&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;hp&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;tableTop&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;gs&lt;/span&gt;\&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dmodeling&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;\&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dcarm&lt;/span&gt;\&lt;span class=&quot;n&quot;&gt;extracted_frames_VID_20250725_173235_20&lt;/span&gt;\&lt;span class=&quot;mi&quot;&gt;15&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;unbounded&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mesh_res&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2048&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;render_path&lt;/span&gt;



&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;anterior-posterior-2&quot;&gt;Anterior Posterior&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/gaussianSplatting/snapshot02.png&quot; style=&quot;display: block; margin-left: auto; margin-right: auto; width: 90%;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;medial-lateral-2&quot;&gt;Medial Lateral&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/gaussianSplatting/snapshot03.png&quot; style=&quot;display: block; margin-left: auto; margin-right: auto; width: 90%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The results clearly show the C-arm but with some &lt;strong&gt;holes or missing parts&lt;/strong&gt; in the mesh. Additionally, the scene contains background elements like signposts, since no mask was available to separate the C-arm (foreground) from the background.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Manually cleaning&lt;/em&gt; is to remove non-C-arm faces and vertices of the mesh. These non-C-arm structure arises because when I perform the mesh extraction, I did not have an image mask to separate the C-arm from the background. It is hard to create such image mask, even though powerful segmentation model such as SAM2 is publicly available.&lt;/p&gt;

&lt;p&gt;I reckon the segmentation challenges are three-fold:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Partial views:&lt;/strong&gt; SAM2 segmentation model struggles to identify the whole C-arm in close-up images since only portions are visible.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Low image resolution:&lt;/strong&gt; The rendered images used for segmentation are much lower resolution than those SAM2 expects.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Non-photorealistic renderings:&lt;/strong&gt; Generated images sometimes lack photo-realism, limiting segmentation accuracy.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;estimating-surface-mesh-via-tsdf&quot;&gt;Estimating Surface Mesh via TSDF&lt;/h3&gt;

&lt;p&gt;To convert Gaussian splats into a dense volumetric scene, I used the &lt;strong&gt;Truncated Signed Distance Function (TSDF)&lt;/strong&gt;. Since, after training, we can render novel views, we use these noisy depth images from RGB rendered images (the camera poses are set according to how we would like to generate the scene). This integration operates on the voxel blocks.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Important note:&lt;/strong&gt; The size of the voxel blocks is critical. If too small relative to the scene, memory usage explodes causing crashes. For my scene, voxel size 0.002 struck a good balance.
After volume reconstruction, the mesh is extracted using algorithms like Marching Cubes.&lt;/p&gt;

&lt;h3 id=&quot;camera-pose-selection&quot;&gt;Camera Pose Selection&lt;/h3&gt;

&lt;p&gt;A good coverage of camera poses is essential to avoid holes or missing parts in the mesh (such as the base platform of the C-arm or wheels of the cart). Insufficient coverage leads to incomplete geometry recovery.&lt;/p&gt;

&lt;p&gt;A simpler approach is to use an &lt;strong&gt;unbounded camera pose normalization&lt;/strong&gt;:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Normalize the scene into a unit sphere.&lt;/li&gt;
  &lt;li&gt;Uniformly sample cameras on the sphere surface for full coverage.&lt;/li&gt;
  &lt;li&gt;Perform TSDF fusion per voxel block and assemble volumes.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;anterior-posterior-3&quot;&gt;Anterior Posterior&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/gaussianSplatting/snapshot10.png&quot; style=&quot;display: block; margin-left: auto; margin-right: auto; width: 90%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/gaussianSplatting/snapshot11.png&quot; style=&quot;display: block; margin-left: auto; margin-right: auto; width: 90%;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;medial-lateral-3&quot;&gt;Medial Lateral&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/gaussianSplatting/snapshot23.png&quot; style=&quot;display: block; margin-left: auto; margin-right: auto; width: 90%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/gaussianSplatting/snapshot24.png&quot; style=&quot;display: block; margin-left: auto; margin-right: auto; width: 90%;&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;sparse-reconstruction-errors-and-consequences&quot;&gt;Sparse Reconstruction Errors and Consequences&lt;/h4&gt;

&lt;p&gt;Once an erroneous sparse reconstruction is used to train a Gaussian Splatting model, the error manifests itself in an interesting way.&lt;/p&gt;

&lt;p&gt;Take the above erroneous sparse reconstructed scene as an example, I trained a 2D Gaussian Splatting model and then extracted a mesh from it.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/gaussianSplatting/snapshot101.png&quot; style=&quot;display: block; margin-left: auto; margin-right: auto; width: 90%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/gaussianSplatting/snapshot102.png&quot; style=&quot;display: block; margin-left: auto; margin-right: auto; width: 90%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The model created an extra phantom C-arm on the back, very interesting results. The misaligned images suggest to the model that there is another phantom arm.&lt;/p&gt;

&lt;p&gt;What is most intriguing about this result is that in terms of SSIM, PSNR, LPIPS, and reprojection errors, none suggests a huge error exists.
That is why Splatvis or visualization is very important - a sure way to verify whether the result makes physical / common sense.&lt;/p&gt;

&lt;h3 id=&quot;rendered-videos-novel-views-of-c-arm-by-2d-gaussian-splatting&quot;&gt;Rendered Videos (Novel Views) of C-arm by 2D Gaussian Splatting&lt;/h3&gt;

&lt;p&gt;Finally, this is the part we have been waiting for - rendering the exact same object / C-arm machine but in 100% novel views. To sample this novel camera poses, I defined a new trajectory (circular orbit) and placed cameras evenly.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;2D Gaussian Splatting (Horizontal Camera filming) of a C-arm Machine in ML Orientation&lt;/em&gt;&lt;/p&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/RhODF_o-eU4?si=kk3ww7NSOUI3f2X-&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&quot; referrerpolicy=&quot;strict-origin-when-cross-origin&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;&lt;em&gt;Depth Map: 2D Gaussian Splatting (Horizontal Camera filming) of a C-arm Machine in ML Orientation&lt;/em&gt;&lt;/p&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/-IZ77YHK8QE?si=XP3jDqgyOtv5IP1j&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&quot; referrerpolicy=&quot;strict-origin-when-cross-origin&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;&lt;em&gt;2D Gaussian Splatting (Horizontal Camera filming) of a C-arm Machine in AP Orientation&lt;/em&gt;&lt;/p&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/xoZIMdK2QMQ?si=-TUUG5N4xy2dfaKF&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&quot; referrerpolicy=&quot;strict-origin-when-cross-origin&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;&lt;em&gt;Depth map: 2D Gaussian Splatting (Horizontal Camera filming) of a C-arm Machine in AP Orientation&lt;/em&gt;&lt;/p&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/2DP5MfRsnsc?si=WjOjQwrvJNHRLKxC&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&quot; referrerpolicy=&quot;strict-origin-when-cross-origin&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;&lt;em&gt;2D Gaussian Splatting (Vertical Camera filming) of a C-arm Machine in ML Orientation&lt;/em&gt;&lt;/p&gt;

&lt;iframe width=&quot;315&quot; height=&quot;560&quot; src=&quot;https://youtube.com/embed/SL8tMtM7qmE?si=LaBN9O6JcfqG8SOo&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;&lt;em&gt;Depth Map: 2D Gaussian Splatting (Vertical Camera filming) of a C-arm Machine in ML Orientation&lt;/em&gt;&lt;/p&gt;

&lt;iframe width=&quot;315&quot; height=&quot;560&quot; src=&quot;https://youtube.com/embed/Yp4pH535woE?si=H9zSRYjU_LHRTBI4&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;&lt;em&gt;2D Gaussian Splatting (Vertical Camera filming) of a C-arm Machine in AP Orientation&lt;/em&gt;&lt;/p&gt;

&lt;iframe width=&quot;315&quot; height=&quot;560&quot; src=&quot;https://youtube.com/embed/b-Aqys3IJ5g?si=VxYkoxdsmi4lYGZE&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;&lt;em&gt;Depth Map: 2D Gaussian Splatting (Vertical Camera filming) of a C-arm Machine in AP Orientation&lt;/em&gt;&lt;/p&gt;

&lt;iframe width=&quot;315&quot; height=&quot;560&quot; src=&quot;https://youtube.com/embed/Z7Nb8JbVnww?si=gytWXq0Hsu-4mNd5&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;h1 id=&quot;enhanced-quality-and-fidelity&quot;&gt;Enhanced Quality and Fidelity&lt;/h1&gt;

&lt;p&gt;My valuable lessons, failures and successes are written in &lt;a href=&quot;/deep_learning/2025/08/19/GaussianSplatting_failuresAndLessons/&quot;&gt;Gaussian Splatting - Failure, Success, and Lesson Learned&lt;/a&gt;. Equipped with such hindsight, I improved the quality of the Gaussian Splatting outputs.&lt;/p&gt;

&lt;p&gt;2D Gaussian Splatting using High Resolution Video of a C-arm Machine to enhance novel view generation&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Gaussian Splatting: High Resolution C-arm Video to Enhance Novel View Generation from Smartphone&lt;/em&gt;&lt;/p&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/96a9EQ8jIII?si=K7SJLJhk0fOmvo0G&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&quot; referrerpolicy=&quot;strict-origin-when-cross-origin&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;&lt;em&gt;Normal Map - Gaussian Splatting: High Resolution C-arm Video to Enhance Novel View Generation from Smartphone&lt;/em&gt;&lt;/p&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/cojZnJmjBWM?si=L-NuXUTzEMokS8uP&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&quot; referrerpolicy=&quot;strict-origin-when-cross-origin&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;&lt;em&gt;Depth Map - Gaussian Splatting: High Resolution C-arm Video to Enhance Novel View Generation from Smartphone&lt;/em&gt;&lt;/p&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/Vl4DalN8W3M?si=_-taM6jKx1WXmoyt&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&quot; referrerpolicy=&quot;strict-origin-when-cross-origin&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;Following up next is &lt;a href=&quot;/deep_learning/2025/08/14/GaussianSplatting_meshesAndBeyond/&quot;&gt;Gaussian Splatting - Meshes and Beyond&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;blog-posts-on-this-topics&quot;&gt;Blog posts on this topics&lt;/h1&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;/deep_learning/2025/08/10/GaussianSplatting_introduction/&quot;&gt;Gaussian Splatting - Introduction&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/deep_learning/2025/08/11/GaussianSplatting_toyExample/&quot;&gt;Gaussian Splatting - Toy Example&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/deep_learning/2025/08/12/GaussianSplatting_camera_poses/&quot;&gt;Gaussian Splatting - Camera Poses&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/deep_learning/2025/08/13/GaussianSplatting_gaussianSplatting/&quot;&gt;Gaussian Splatting - Gaussian Splatting&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/deep_learning/2025/08/14/GaussianSplatting_meshesAndBeyond/&quot;&gt;Gaussian Splatting - Meshes and Beyond&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/deep_learning/2025/08/19/GaussianSplatting_failuresAndLessons/&quot;&gt;Gaussian Splatting - Failure, Success, and Lesson Learned&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Wilson Fok</name></author><category term="Deep_Learning" /><summary type="html">How to perform Gaussian Splatting? The Gaussian Splatting Python scripts are provided in the original publication - 3D Gaussian Splatting for Real-Time Radiance Field Rendering. To use these scripts, you must first set up the required Python environment. While the setup may seem straightforward, I encountered several challenges during the process that I document here to help others avoid them. The code was first published in 2023, and many dependencies are no longer readily available or compatible with the latest software versions. A common mistake I made was attempting to run it with the most up-to-date packages, which led to numerous incompatibility issues. Hardware Specification GPU: Nvidia GeForce RTX 4070, 8 GB VRAM Driver version: 580.88 CUDA: 11.8 CPU: Intel i7-13620H OS: Windows 11 RAM: 40 GB (Windows page file 60 GB) Python and Package installation I installed both Python 3.8 and 3.10, finding Python 3.10 generally more compatible with most packages since 3.8 is becoming obsolete. Most packages install smoothly as pre-compiled wheels, but some, especially CUDA-related custom modules in Gaussian Splatting, require local compilation. Making sure the compilation tools are of the compatible version with these packages can be tricky. Since most of the time, the authors would not have tested many cases and their success configuration may not be the same as what I have got. After many experiments by trial and error, I have worked out a solution as follows: Microsoft Visual Studio Build Tools 2019 (v143 toolset): Avoid the latest Visual Studio versions like 17.14, which are not backward compatible. Always compile from the VS x64 Native Tools Command Prompt to ensure the correct compiler is called. With this setup, I could follow the official installation guides without issues. Tips and Common Issues Tips: If we use install torch and torch related packages on pip, it is likely that pip install the CPU-only version. So please visit Pytorch official website to get the CUDA-enabled version that is compatible for your device. Watch out for: OMP: Error #15: Initializing libiomp5md.dll, but found libiomp5md.dll already initialized. OMP: Hint This means that multiple copies of the OpenMP runtime have been linked into the program. That is dangerous, since it can degrade performance or cause incorrect results. The best thing to do is to ensure that only a single OpenMP runtime is linked into the process, e.g. by avoiding static linking of the OpenMP runtime in any library. As an unsafe, unsupported, undocumented workaround you can set the environment variable KMP_DUPLICATE_LIB_OK=TRUE to allow the program to continue to execute, but that may cause crashes or silently produce incorrect results. For more information, please see http://www.intel.com/software/products/support/. Crash and blue-screen Initially, frequent crashes were blamed on Python or the packages. In hindsight, I have figured out the importance and usefulness of windows operating system event report feature. On many occasions, the system crashed due to mis-configured libraries and system settings. Of course, I had no idea at the beginning and mis-attributed the crash due to Python and its packages. After reinstalling Python and its packages, the situation showed no sign of any improvement. I then studied the event report features on windows to get a better understanding on what did occur right before a crash (Windows Event Viewer). The WINDGB tool was very useful because a dump file was generated to record the exact event and all the related programs or dll at that time right after the crash. WINDGB revealed the core issue - multiple CUDA versions installed on the system. I had installed and uninstalled different CUDA versions which corrupted Nvidia dll and caused several errors. Final solution: Thus, the final solution is to update all the graphical drivers and libraries because all evidence pointed to an urgent driver updates (Nvidia and Intel graphic cards). After a CLEAN installation of the latest drivers, the crash had ceased eventually . As for CUDA, keep one and only one version (in this case CUDA 11.8) on the system. Otherwise, python setup may compile the source code against the wrong version. In addition to software upgrades, shutdown all memory intensive programs prior to running Python and increase Windows’ page memory as much as possible. After this step, system stability was restored and crashes stopped. Training 3D Gaussian Splatting # Run 3D AND 2D gaussian splatting train script python train.py -s C:\Users\hp\tableTop\gs\3dmodeling\data\3dcarm\extracted_frames_VID_20250725_172445_20 -m ./outputs/VID_20250725_172445_20_93 --resolution 8 --iterations 93000 --densification_interval 100 --opacity_reset_interval 3000 --densify_from_iter 500 &amp;amp;&amp;amp; python train.py -s C:\Users\hp\tableTop\gs\3dmodeling\data\3dcarm\extracted_frames_VID_20250725_175410_20 -m ./outputs/VID_20250725_175410_20_93 --resolution 8 --iterations 93000 --densification_interval 100 --opacity_reset_interval 3000 --densify_from_iter 500 python train.py -s C:\Users\hp\tableTop\gs\3dmodeling\data\3dcarm\extracted_frames_VID_20250725_173235_20\15 -m ./outputs/VID_20250725_173235_20_93\15 --resolution 8 --iterations 93000 --densification_interval 100 --opacity_reset_interval 3000 --densify_from_iter 500 Note: this laptop has limited GPU memory. See computer specification, so the images are downscaled by a multiple of 8. i.e. from 1080x1920 -&amp;gt; 135x240 Results The Gaussian Splatting model was evaluated on a C-arm machine configured in both Anterior-Posterior (AP) and Medial-Lateral (ML) poses. There are two ways to assess the quality: Quantitative metrics: SSIM, PSNR, and LPIPS scores provide numerical evaluation of image quality. Qualitative visualization: Using Splatviz, which allows intuitive inspection of the reconstructions. SSIM, PSNR, LPIPS # evaluation script (evaluation on test images , a total of 53) python render.py --iteration 93000 -s C:\Users\hp\tableTop\gs\3dmodeling\data\3dcarm\extracted_frames_VID_20250725_172445_20 -m ./outputs/VID_20250725_172445_20_93 --eval --skip_train &amp;amp;&amp;amp; python render.py --iteration 7000 -s C:\Users\hp\tableTop\gs\3dmodeling\data\3dcarm\extracted_frames_VID_20250725_172445_20 -m ./outputs/VID_20250725_172445_20_93 --eval --skip_train &amp;amp;&amp;amp; python render.py --iteration 30000 -s C:\Users\hp\tableTop\gs\3dmodeling\data\3dcarm\extracted_frames_VID_20250725_172445_20 -m ./outputs/VID_20250725_172445_20_93 --eval --skip_train python metrics.py --model_paths .\outputs\VID_20250725_172445_20_93 python render.py --iteration 7000 -s C:\Users\hp\tableTop\gs\3dmodeling\data\3dcarm\extracted_frames_VID_20250725_175410_20 -m ./outputs/VID_20250725_175410_20_93 --eval --skip_train python render.py --iteration 30000 -s C:\Users\hp\tableTop\gs\3dmodeling\data\3dcarm\extracted_frames_VID_20250725_175410_20 -m ./outputs/VID_20250725_175410_20_93 --eval --skip_train python render.py --iteration 93000 -s C:\Users\hp\tableTop\gs\3dmodeling\data\3dcarm\extracted_frames_VID_20250725_175410_20 -m ./outputs/VID_20250725_175410_20_93 --eval --skip_train python metrics.py --model_paths .\outputs\VID_20250725_175410_20_93 python render.py --iteration 93000 -s C:\Users\hp\tableTop\gs\3dmodeling\data\3dcarm\extracted_frames_VID_20250725_173235_20 -m ./outputs/VID_20250725_173235_20_93 --eval --skip_train &amp;amp;&amp;amp; python render.py --iteration 7000 -s C:\Users\hp\tableTop\gs\3dmodeling\data\3dcarm\extracted_frames_VID_20250725_173235_20 -m ./outputs/VID_20250725_173235_20_93 --eval --skip_train &amp;amp;&amp;amp; python render.py --iteration 30000 -s C:\Users\hp\tableTop\gs\3dmodeling\data\3dcarm\extracted_frames_VID_20250725_173235_20 -m ./outputs/VID_20250725_173235_20_93 --eval --skip_train python metrics.py --model_paths .\outputs\VID_20250725_173235_20_93 Anterior Posterior AP Orientation At iteration 7000 SSIM : 0.9049926 PSNR : 25.9037876 LPIPS: 0.1312765 At iteration 30000 SSIM : 0.9583084 PSNR : 30.5848618 LPIPS: 0.0602733 At iteration 93000 SSIM : 0.9692391 PSNR : 32.1568565 LPIPS: 0.0452350 Medial Lateral Vertical camera frames At iteration 7000 SSIM : 0.9277247 PSNR : 26.7998466 LPIPS: 0.0987048 At iteration 30000 SSIM : 0.9654269 PSNR : 30.7180729 LPIPS: 0.0463373 At iteration 93000 SSIM : 0.9736962 PSNR : 32.5040054 LPIPS: 0.0358106 Horizontal camera frames At iteration 7000 SSIM : 0.8346600 PSNR : 22.8985519 LPIPS: 0.2103405 At iteration 30000 SSIM : 0.9369362 PSNR : 28.4063663 LPIPS: 0.0972803 At iteration 93000 SSIM : 0.9571357 PSNR : 30.3842888 LPIPS: 0.0668879 All in all, the Gaussian Splatting results are very good. Indeed, the more iterations, the better the quality. (albeit the improvement slows drastically) The videos were shot in two different camera orientations. The vertical and the horizontal show very similar end results. I think so long as the video quality is good and the reconstruction is accurate, Gaussian Splatting is quite robust that camera orientation does not matter. Splatviz # Run splat-vis for visualization python run_main.py --data_path=C:\Users\hp\tableTop\gs\gaussian-splatting\outputs\VID_20250725_172445_20_93_previous\point_cloud\iteration_93000 python run_main.py --data_path=C:\Users\hp\tableTop\gs\gaussian-splatting\outputs\VID_20250725_172445_20_93\point_cloud\iteration_30000 python run_main.py --data_path=C:\Users\hp\tableTop\gs\gaussian-splatting\outputs\VID_20250725_172445_20_93\point_cloud\iteration_93000 python run_main.py --data_path=C:\Users\hp\tableTop\gs\gaussian-splatting\outputs\VID_20250725_175410_20_93\point_cloud\iteration_93000 Anterior Posterior Medial Lateral Zoom in block to indicate filtering script for Gaussian ellipsoids display filter Gaussian ellipsoids mask = torch.linalg.norm(gs._scaling, dim=-1) &amp;lt; slider.x gs._xyz = gs._xyz[mask] gs._rotation = gs._rotation[mask] gs._scaling = gs._scaling[mask] gs._opacity = gs._opacity[mask] gs._features_dc = gs._features_dc[mask] gs._features_rest = gs._features_rest[mask] mask = torch.linalg.norm(gs._opacity, dim=-1) &amp;lt; slider.x gs._xyz = gs._xyz[mask] gs._rotation = gs._rotation[mask] gs._scaling = gs._scaling[mask] gs._opacity = gs._opacity[mask] gs._features_dc = gs._features_dc[mask] gs._features_rest = gs._features_rest[mask] 2DGS: 2D Gaussian Splatting for Geometrically Accurate Radiance Fields 2D Gaussian Splatting extends 3D Gaussian Splatting by addressing its limitation in representing mainly volumetric structures but not surfaces. Surfaces are better represented by 2D surfels, which are surface elements defined by: A 2D tangential disk with radius r and a center point. A normal vector pointing outward. Graphics data such as color, texture, and depth. Unlike meshes, surfels do not require connectivity between elements, allowing flexible level-of-detail adaptation: smaller and denser surfels for close-ups, larger and fewer for distant views, balancing quality and computation. Using 2D Gaussian Splatting for 3D Mesh Extraction of the C-arm Machine I chose 2D Gaussian Splatting over the 3D version to extract surface meshes of the C-arm machine. The surfel surfaces were exported as PLY meshes or point clouds. # mesh extraction and video generation python render.py -m C:\Users\hp\tableTop\gs\2d-gaussian-splatting\outputs\VID_20250725_172445_20_93 -s C:\Users\hp\tableTop\gs\3dmodeling\data\3dcarm\extracted_frames_VID_20250725_172445_20 --unbounded --mesh_res 2048 &amp;amp;&amp;amp; python render.py -m C:\Users\hp\tableTop\gs\2d-gaussian-splatting\outputs\VID_20250725_175410_20_93 -s C:\Users\hp\tableTop\gs\3dmodeling\data\3dcarm\extracted_frames_VID_20250725_175410_20 --unbounded --mesh_res 2048 python render.py -m C:\Users\hp\tableTop\gs\2d-gaussian-splatting\outputs\VID_20250725_173235_20_93 -s C:\Users\hp\tableTop\gs\3dmodeling\data\3dcarm\extracted_frames_VID_20250725_173235_20 --unbounded --mesh_res 2048 --render_path python render.py -m C:\Users\hp\tableTop\gs\2d-gaussian-splatting\outputs\VID_20250725_173235_20_93\15 -s C:\Users\hp\tableTop\gs\3dmodeling\data\3dcarm\extracted_frames_VID_20250725_173235_20\15 --unbounded --mesh_res 2048 --render_path Anterior Posterior Medial Lateral The results clearly show the C-arm but with some holes or missing parts in the mesh. Additionally, the scene contains background elements like signposts, since no mask was available to separate the C-arm (foreground) from the background. Manually cleaning is to remove non-C-arm faces and vertices of the mesh. These non-C-arm structure arises because when I perform the mesh extraction, I did not have an image mask to separate the C-arm from the background. It is hard to create such image mask, even though powerful segmentation model such as SAM2 is publicly available. I reckon the segmentation challenges are three-fold: Partial views: SAM2 segmentation model struggles to identify the whole C-arm in close-up images since only portions are visible. Low image resolution: The rendered images used for segmentation are much lower resolution than those SAM2 expects. Non-photorealistic renderings: Generated images sometimes lack photo-realism, limiting segmentation accuracy. Estimating Surface Mesh via TSDF To convert Gaussian splats into a dense volumetric scene, I used the Truncated Signed Distance Function (TSDF). Since, after training, we can render novel views, we use these noisy depth images from RGB rendered images (the camera poses are set according to how we would like to generate the scene). This integration operates on the voxel blocks. Important note: The size of the voxel blocks is critical. If too small relative to the scene, memory usage explodes causing crashes. For my scene, voxel size 0.002 struck a good balance. After volume reconstruction, the mesh is extracted using algorithms like Marching Cubes. Camera Pose Selection A good coverage of camera poses is essential to avoid holes or missing parts in the mesh (such as the base platform of the C-arm or wheels of the cart). Insufficient coverage leads to incomplete geometry recovery. A simpler approach is to use an unbounded camera pose normalization: Normalize the scene into a unit sphere. Uniformly sample cameras on the sphere surface for full coverage. Perform TSDF fusion per voxel block and assemble volumes. Anterior Posterior Medial Lateral Sparse Reconstruction Errors and Consequences Once an erroneous sparse reconstruction is used to train a Gaussian Splatting model, the error manifests itself in an interesting way. Take the above erroneous sparse reconstructed scene as an example, I trained a 2D Gaussian Splatting model and then extracted a mesh from it. The model created an extra phantom C-arm on the back, very interesting results. The misaligned images suggest to the model that there is another phantom arm. What is most intriguing about this result is that in terms of SSIM, PSNR, LPIPS, and reprojection errors, none suggests a huge error exists. That is why Splatvis or visualization is very important - a sure way to verify whether the result makes physical / common sense. Rendered Videos (Novel Views) of C-arm by 2D Gaussian Splatting Finally, this is the part we have been waiting for - rendering the exact same object / C-arm machine but in 100% novel views. To sample this novel camera poses, I defined a new trajectory (circular orbit) and placed cameras evenly. 2D Gaussian Splatting (Horizontal Camera filming) of a C-arm Machine in ML Orientation Depth Map: 2D Gaussian Splatting (Horizontal Camera filming) of a C-arm Machine in ML Orientation 2D Gaussian Splatting (Horizontal Camera filming) of a C-arm Machine in AP Orientation Depth map: 2D Gaussian Splatting (Horizontal Camera filming) of a C-arm Machine in AP Orientation 2D Gaussian Splatting (Vertical Camera filming) of a C-arm Machine in ML Orientation Depth Map: 2D Gaussian Splatting (Vertical Camera filming) of a C-arm Machine in ML Orientation 2D Gaussian Splatting (Vertical Camera filming) of a C-arm Machine in AP Orientation Depth Map: 2D Gaussian Splatting (Vertical Camera filming) of a C-arm Machine in AP Orientation Enhanced Quality and Fidelity My valuable lessons, failures and successes are written in Gaussian Splatting - Failure, Success, and Lesson Learned. Equipped with such hindsight, I improved the quality of the Gaussian Splatting outputs. 2D Gaussian Splatting using High Resolution Video of a C-arm Machine to enhance novel view generation Gaussian Splatting: High Resolution C-arm Video to Enhance Novel View Generation from Smartphone Normal Map - Gaussian Splatting: High Resolution C-arm Video to Enhance Novel View Generation from Smartphone Depth Map - Gaussian Splatting: High Resolution C-arm Video to Enhance Novel View Generation from Smartphone Following up next is Gaussian Splatting - Meshes and Beyond Blog posts on this topics Gaussian Splatting - Introduction Gaussian Splatting - Toy Example Gaussian Splatting - Camera Poses Gaussian Splatting - Gaussian Splatting Gaussian Splatting - Meshes and Beyond Gaussian Splatting - Failure, Success, and Lesson Learned</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="/assets/images/gaussianSplatting/cover.avif" /><media:content medium="image" url="/assets/images/gaussianSplatting/cover.avif" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Gaussian Splatting - Camera Poses</title><link href="/deep_learning/2025/08/12/GaussianSplatting_camera_poses/" rel="alternate" type="text/html" title="Gaussian Splatting - Camera Poses" /><published>2025-08-12T00:00:00+08:00</published><updated>2025-08-12T00:00:00+08:00</updated><id>/deep_learning/2025/08/12/GaussianSplatting_camera_poses</id><content type="html" xml:base="/deep_learning/2025/08/12/GaussianSplatting_camera_poses/">&lt;h1 id=&quot;my-experiment&quot;&gt;My Experiment&lt;/h1&gt;

&lt;p&gt;To gain a better understanding of this technology, I deployed the entire technology stack on a gaming laptop and worked out the details.&lt;/p&gt;

&lt;h2 id=&quot;methods&quot;&gt;Methods&lt;/h2&gt;

&lt;p&gt;We need to capture a video of the target that shows all its relevant parts. For this experiment, I recorded two C-arm videos: one in the anterior-posterior view and another in the medial-lateral view. It took a few attempts before I succeeded, and there are some useful tips for capturing these videos.&lt;/p&gt;

&lt;h2 id=&quot;recording-a-high-quality-video-for-reconstruction&quot;&gt;Recording a High-Quality Video for Reconstruction&lt;/h2&gt;

&lt;p&gt;The surface of the C-arm is quite plain and lacks distinct features and color. This often causes problems when running sparse image reconstruction algorithms to estimate the camera’s poses, as the images lack sufficient features for matching.&lt;/p&gt;

&lt;p&gt;To mitigate (though not completely eliminate) this issue, I placed “signposts” — objects that are easy to match and contain many visual features — around the C-arm. These included a keyboard, a colored tag, and a piece of paper with multiple printed icons and logos (see my results for details).&lt;/p&gt;

&lt;p&gt;The video quality should be high—for example, 1080x1920 resolution at 30 frames per second. This quality is sufficient for this experiment, although many modern smartphones can capture videos at much higher resolutions, so a smartphone would also suffice.&lt;/p&gt;

&lt;p&gt;A basic prerequisite for this work is obtaining fairly accurate camera poses and camera properties. The smartphone camera can be modeled as a pinhole camera, which is supported by many neural radiance field and Gaussian splatting packages.&lt;/p&gt;

&lt;p&gt;Since I did not have access to the camera’s properties and poses, I relied on COLMAP, a free and open-source software, to estimate both. COLMAP uses structure-from-motion algorithms to reconstruct scenes from images and is widely used in navigation, robotics, computer vision, and other fields.&lt;/p&gt;

&lt;p&gt;However, COLMAP accepts only images as input, not videos. To convert video frames into images, I wrote a simple Python Jupyter notebook to perform this task.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;cv2&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;os&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;shutil&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;clear_folder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;folder_path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exists&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;folder_path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;isdir&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;folder_path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;# Iterate over all files and subdirectories in the folder
&lt;/span&gt;        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;filename&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;listdir&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;folder_path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;file_path&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;join&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;folder_path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;filename&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;try&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
                &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;isfile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;file_path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;or&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;islink&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;file_path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
                    &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;remove&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;file_path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# Remove file or symbolic link
&lt;/span&gt;                &lt;span class=&quot;k&quot;&gt;elif&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;isdir&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;file_path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
                    &lt;span class=&quot;n&quot;&gt;shutil&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rmtree&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;file_path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# Remove directory and all its contents
&lt;/span&gt;            &lt;span class=&quot;k&quot;&gt;except&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;Exception&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;e&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
                &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;Failed to delete &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;file_path&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;. Reason: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;e&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;# If folder does not exist, create it
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;makedirs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;folder_path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;exist_ok&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;


&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;calculate_sharpness&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# Convert to grayscale
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;gray&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cvtColor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;COLOR_BGR2GRAY&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# Compute the Laplacian variance (higher means sharper)
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;laplacian&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Laplacian&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gray&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;CV_64F&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;sharpness&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;laplacian&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;var&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sharpness&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;calculate_contrast&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# Convert to grayscale
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;gray&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cvtColor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;COLOR_BGR2GRAY&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# Contrast as standard deviation of pixel intensities
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;contrast&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gray&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;contrast&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;is_good_quality&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sharpness_thresh&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;100.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;contrast_thresh&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;30.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;sharpness&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;calculate_sharpness&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;contrast&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;calculate_contrast&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# You can print or log these values for debugging
&lt;/span&gt;    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Sharpness: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sharpness&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;, Contrast: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;contrast&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sharpness&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sharpness_thresh&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;contrast&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;contrast_thresh&lt;/span&gt;


&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;get_info&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;source&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;video_path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# Open the video file
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;cap&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;VideoCapture&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;join&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;source&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;video_path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;isOpened&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Error: Could not open video.&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;fps&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;CAP_PROP_FPS&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# Frames per second
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;total_frames&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;CAP_PROP_FRAME_COUNT&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;fps &quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fps&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&apos; total frames &apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;total_frames&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;cap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;release&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;extract_frames&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;source&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;video_path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output_dir&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;interval_sec&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;interval_frame&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tag&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# Open the video file
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;cap&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;VideoCapture&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;join&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;source&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;video_path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;isOpened&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Error: Could not open video.&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# Create output directory if it doesn&apos;t exist
&lt;/span&gt;    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exists&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;output_dir&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;makedirs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;output_dir&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;clear folder&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;clear_folder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;output_dir&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;fps&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;CAP_PROP_FPS&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# Frames per second
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;total_frames&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;CAP_PROP_FRAME_COUNT&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;frame_count&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;saved_count&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;while&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;ret&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;frame&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ret&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;break&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# End of video
&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;# Decide whether to save this frame based on interval
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;save_frame&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;interval_sec&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;c1&quot;&gt;# Save frame every N seconds
&lt;/span&gt;            &lt;span class=&quot;n&quot;&gt;current_time_sec&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;frame_count&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fps&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;current_time_sec&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;interval_sec&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;save_frame&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;elif&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;interval_frame&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;c1&quot;&gt;# Save frame every N frames
&lt;/span&gt;            &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;frame_count&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;interval_frame&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;save_frame&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;c1&quot;&gt;# If no interval specified, save all frames
&lt;/span&gt;            &lt;span class=&quot;n&quot;&gt;save_frame&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;

        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;save_frame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;c1&quot;&gt;# Check image quality: frame should not be None or empty
&lt;/span&gt;            &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;frame&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;frame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
                &lt;span class=&quot;c1&quot;&gt;# the measurement is not reliable, it biases towards specific contents, leaving many parts of the video un-extracted
#                 if is_good_quality(frame): 
&lt;/span&gt;                &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tag&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
                    &lt;span class=&quot;n&quot;&gt;filename&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;join&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;output_dir&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;img_&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;saved_count&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;_&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tag&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;.jpg&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
                &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
                    &lt;span class=&quot;n&quot;&gt;filename&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;join&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;output_dir&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;img_&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;saved_count&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;.jpg&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;#                 frame = cv2.flip(frame, 0) # no need this time to flip them upside down
&lt;/span&gt;                &lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imwrite&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;filename&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;frame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;#                 print(f&quot;Saved {filename}&quot;)
&lt;/span&gt;                &lt;span class=&quot;n&quot;&gt;saved_count&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;

        &lt;span class=&quot;n&quot;&gt;frame_count&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;cap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;release&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Extraction complete. &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;saved_count&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; frames saved.&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;


&lt;span class=&quot;n&quot;&gt;video_file&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;VID_20250725_173235.mp4&quot;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;SOURCE&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;sa&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;C:\Users\hp\tableTop\gs\3dmodeling\data&quot;&lt;/span&gt;


&lt;span class=&quot;n&quot;&gt;full_SOURCE&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;join&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SOURCE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;2dcarm&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;output_folder&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;sa&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;C:\Users\hp\tableTop\gs\3dmodeling\data\2dcarm\extracted_frames_VID_20250725_173235_20\sequential&quot;&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;get_info&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;full_SOURCE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;video_file&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Extract frame every 2 seconds
# extract_frames(video_file, output_folder, interval_sec=20)
&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# Or extract every 30 frames
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;interval_frame&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;15&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;extract_frames&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;full_SOURCE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;video_file&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output_folder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;interval_frame&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;interval_frame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;While the video is recorded at 30 fps, meaning even a short video contains thousands of images, my laptop lacks the necessary hardware to handle the processing workload for such a large number of images, as this volume overwhelms its limits. After some experimentation, I found it acceptable to use only one out of every 15-20 frames. The downside of this sparse sampling is discussed later in the results section.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/gaussianSplatting/extracted_video_frames.png&quot; style=&quot;display: block; margin-left: auto; margin-right: auto; width: 90%;&quot; /&gt;&lt;/p&gt;
&lt;figcaption style=&quot;text-align: center;&quot;&gt; ABC &lt;/figcaption&gt;

&lt;p&gt;Such a simplistic approach to frame extraction relies on one key assumption: the camera moves steadily around the C-arm. I practiced several times to hold my hand steady while filming. I also learned to leave enough space around the C-arm for walking while filming and to avoid stopping at any spot for too long or too briefly.&lt;/p&gt;

&lt;h2 id=&quot;colmap-performs-sparse-reconstruction&quot;&gt;COLMAP performs Sparse Reconstruction&lt;/h2&gt;
&lt;p&gt;Step 1: Extract features from images using SIFT.&lt;/p&gt;

&lt;p&gt;Step 2: Estimate camera poses and properties.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/gaussianSplatting/camera_intrinsic.png&quot; style=&quot;display: block; margin-left: auto; margin-right: auto; width: 90%;&quot; /&gt;&lt;/p&gt;
&lt;figcaption style=&quot;text-align: center;&quot;&gt;CAMERA INTRINSIC&lt;/figcaption&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/gaussianSplatting/camera_properties.png&quot; style=&quot;display: block; margin-left: auto; margin-right: auto; width: 90%;&quot; /&gt;&lt;/p&gt;
&lt;figcaption style=&quot;text-align: center;&quot;&gt;CAMERA PROPERTIES&lt;/figcaption&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/gaussianSplatting/pose_prior.png&quot; style=&quot;display: block; margin-left: auto; margin-right: auto; width: 90%;&quot; /&gt;&lt;/p&gt;
&lt;figcaption style=&quot;text-align: center;&quot;&gt;POSE PRIOR&lt;/figcaption&gt;

&lt;p&gt;Step 3: Utilize the signposts strategically placed around the C-arm.&lt;/p&gt;

&lt;p&gt;Step 4: Perform image pair matching using an exhaustive approach since the recording forms a loop around the C-arm.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/gaussianSplatting/database.png&quot; style=&quot;display: block; margin-left: auto; margin-right: auto; width: 90%;&quot; /&gt;&lt;/p&gt;
&lt;figcaption style=&quot;text-align: center;&quot;&gt;DATABASE&lt;/figcaption&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/gaussianSplatting/matching.png&quot; style=&quot;display: block; margin-left: auto; margin-right: auto; width: 90%;&quot; /&gt;&lt;/p&gt;
&lt;figcaption style=&quot;text-align: center;&quot;&gt;MATCHING&lt;/figcaption&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/gaussianSplatting/match_pairs.png&quot; style=&quot;display: block; margin-left: auto; margin-right: auto; width: 90%;&quot; /&gt;&lt;/p&gt;
&lt;figcaption style=&quot;text-align: center;&quot;&gt;MATCH PAIRS&lt;/figcaption&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/gaussianSplatting/match_pairs_correspondingMatches.png&quot; style=&quot;display: block; margin-left: auto; margin-right: auto; width: 90%;&quot; /&gt;&lt;/p&gt;
&lt;figcaption style=&quot;text-align: center;&quot;&gt;MATCH PAIRS CORRESPONDING MATCHES&lt;/figcaption&gt;

&lt;p&gt;The red dots with a green straight line to denote their direction connection are numerous or abundant for all the signposts I deliberately put down on the floor next to the machine. Super helpful for matching images.&lt;/p&gt;

&lt;p&gt;Step 5: Conduct sparse reconstruction.&lt;/p&gt;

&lt;p&gt;Step 6: Note that the reconstruction quality need not be perfect—“high” quality is sufficient—and it is not necessary to use all images.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/gaussianSplatting/reconstruction.png&quot; style=&quot;display: block; margin-left: auto; margin-right: auto; width: 90%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Step 7. Review reconstruction results and errors&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/gaussianSplatting/horizontalFrame_reconstruction.png&quot; style=&quot;display: block; margin-left: auto; margin-right: auto; width: 90%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/gaussianSplatting/verticalframes_reconstruction.png&quot; style=&quot;display: block; margin-left: auto; margin-right: auto; width: 90%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Step 8: Export models; if distortion is present, undistort images before export. For example, if the camera were not a pinhole but a fisheye type, COLMAP can undistort fisheye images to pinhole-like images for sparse reconstruction.
&lt;img src=&quot;/assets/images/gaussianSplatting/undistorted.png&quot; style=&quot;display: block; margin-left: auto; margin-right: auto; width: 90%;&quot; /&gt;&lt;/p&gt;
&lt;figcaption style=&quot;text-align: center;&quot;&gt;UNDISTORTED&lt;/figcaption&gt;

&lt;p&gt;The reconstruction quality varies: a good reconstruction shows a red camera track that closely follows the path around the C-arm, providing a holistic view. Since no absolute ground truth exists for the smartphone camera poses, the alignment must be assessed subjectively by comparing the video filming with COLMAP’s estimates.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/gaussianSplatting/improved_reconstruction.png&quot; style=&quot;display: block; margin-left: auto; margin-right: auto; width: 90%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Conversely, a poor reconstruction is evident when the camera track breaks off, and many cameras are inaccurately clustered, pointing in wrong directions. Such misalignment indicates incorrect camera positioning, which can severely impact subsequent processing. Results of Gaussian Splatting on both good and poor reconstructions will reveal stark contrasts.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/gaussianSplatting/Error_reconstruction.png&quot; style=&quot;display: block; margin-left: auto; margin-right: auto; width: 90%;&quot; /&gt;&lt;/p&gt;

&lt;h5 id=&quot;vertical-camera-frames-vs-horizontal-camera-frames&quot;&gt;Vertical camera frames vs horizontal camera frames&lt;/h5&gt;
&lt;p&gt;Experience indicates that vertical frames do not increase reprojection errors in sparse scene reconstruction, but many frames are dropped because they cannot be fitted with the rest. Conversely, horizontal frames fit more easily with similar reprojection errors and offer many more observations per image.&lt;/p&gt;

&lt;p&gt;However, Gaussian Splatting is quite robust. Despite slightly higher errors and fewer images fitted into the scene, the final quality remains decent, as will be discussed later.
Following up next is &lt;a href=&quot;/deep_learning/2025/08/13/GaussianSplatting_gaussianSplatting/&quot;&gt;Gaussian Splatting - Gaussian Splatting&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;blog-posts-on-this-topics&quot;&gt;Blog posts on this topics&lt;/h1&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;/deep_learning/2025/08/10/GaussianSplatting_introduction/&quot;&gt;Gaussian Splatting - Introduction&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/deep_learning/2025/08/11/GaussianSplatting_toyExample/&quot;&gt;Gaussian Splatting - Toy Example&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/deep_learning/2025/08/12/GaussianSplatting_camera_poses/&quot;&gt;Gaussian Splatting - Camera Poses&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/deep_learning/2025/08/13/GaussianSplatting_gaussianSplatting/&quot;&gt;Gaussian Splatting - Gaussian Splatting&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/deep_learning/2025/08/14/GaussianSplatting_meshesAndBeyond/&quot;&gt;Gaussian Splatting - Meshes and Beyond&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/deep_learning/2025/08/19/GaussianSplatting_failuresAndLessons/&quot;&gt;Gaussian Splatting - Failure, Success, and Lesson Learned&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Wilson Fok</name></author><category term="Deep_Learning" /><summary type="html">My Experiment To gain a better understanding of this technology, I deployed the entire technology stack on a gaming laptop and worked out the details. Methods We need to capture a video of the target that shows all its relevant parts. For this experiment, I recorded two C-arm videos: one in the anterior-posterior view and another in the medial-lateral view. It took a few attempts before I succeeded, and there are some useful tips for capturing these videos. Recording a High-Quality Video for Reconstruction The surface of the C-arm is quite plain and lacks distinct features and color. This often causes problems when running sparse image reconstruction algorithms to estimate the camera’s poses, as the images lack sufficient features for matching. To mitigate (though not completely eliminate) this issue, I placed “signposts” — objects that are easy to match and contain many visual features — around the C-arm. These included a keyboard, a colored tag, and a piece of paper with multiple printed icons and logos (see my results for details). The video quality should be high—for example, 1080x1920 resolution at 30 frames per second. This quality is sufficient for this experiment, although many modern smartphones can capture videos at much higher resolutions, so a smartphone would also suffice. A basic prerequisite for this work is obtaining fairly accurate camera poses and camera properties. The smartphone camera can be modeled as a pinhole camera, which is supported by many neural radiance field and Gaussian splatting packages. Since I did not have access to the camera’s properties and poses, I relied on COLMAP, a free and open-source software, to estimate both. COLMAP uses structure-from-motion algorithms to reconstruct scenes from images and is widely used in navigation, robotics, computer vision, and other fields. However, COLMAP accepts only images as input, not videos. To convert video frames into images, I wrote a simple Python Jupyter notebook to perform this task. import cv2 import os import shutil def clear_folder(folder_path): if os.path.exists(folder_path) and os.path.isdir(folder_path): # Iterate over all files and subdirectories in the folder for filename in os.listdir(folder_path): file_path = os.path.join(folder_path, filename) try: if os.path.isfile(file_path) or os.path.islink(file_path): os.remove(file_path) # Remove file or symbolic link elif os.path.isdir(file_path): shutil.rmtree(file_path) # Remove directory and all its contents except Exception as e: print(f&apos;Failed to delete {file_path}. Reason: {e}&apos;) else: # If folder does not exist, create it os.makedirs(folder_path, exist_ok=True) def calculate_sharpness(image): # Convert to grayscale gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) # Compute the Laplacian variance (higher means sharper) laplacian = cv2.Laplacian(gray, cv2.CV_64F) sharpness = laplacian.var() return sharpness def calculate_contrast(image): # Convert to grayscale gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) # Contrast as standard deviation of pixel intensities contrast = gray.std() return contrast def is_good_quality(image, sharpness_thresh=100.0, contrast_thresh=30.0): sharpness = calculate_sharpness(image) contrast = calculate_contrast(image) # You can print or log these values for debugging print(f&quot;Sharpness: {sharpness}, Contrast: {contrast}&quot;) return sharpness &amp;gt;= sharpness_thresh and contrast &amp;gt;= contrast_thresh def get_info(source, video_path): # Open the video file cap = cv2.VideoCapture(os.path.join(source , video_path)) if not cap.isOpened(): print(&quot;Error: Could not open video.&quot;) return fps = cap.get(cv2.CAP_PROP_FPS) # Frames per second total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) print(&quot;fps &quot;, fps , &apos; total frames &apos;, total_frames) cap.release() def extract_frames(source, video_path, output_dir, interval_sec=None, interval_frame=None, tag=None): # Open the video file cap = cv2.VideoCapture(os.path.join(source , video_path)) if not cap.isOpened(): print(&quot;Error: Could not open video.&quot;) return # Create output directory if it doesn&apos;t exist if not os.path.exists(output_dir): os.makedirs(output_dir) else: print(&quot;clear folder&quot;) clear_folder(output_dir) fps = cap.get(cv2.CAP_PROP_FPS) # Frames per second total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) frame_count = 0 saved_count = 0 while True: ret, frame = cap.read() if not ret: break # End of video # Decide whether to save this frame based on interval save_frame = False if interval_sec is not None: # Save frame every N seconds current_time_sec = frame_count / fps if int(current_time_sec) % interval_sec == 0: save_frame = True elif interval_frame is not None: # Save frame every N frames if frame_count % interval_frame == 0: save_frame = True else: # If no interval specified, save all frames save_frame = True if save_frame: # Check image quality: frame should not be None or empty if frame is not None and frame.size &amp;gt; 0: # the measurement is not reliable, it biases towards specific contents, leaving many parts of the video un-extracted # if is_good_quality(frame): if tag: filename = os.path.join(output_dir, f&quot;img_{saved_count}_{tag}.jpg&quot;) else: filename = os.path.join(output_dir, f&quot;img_{saved_count}.jpg&quot;) # frame = cv2.flip(frame, 0) # no need this time to flip them upside down cv2.imwrite(filename, frame) # print(f&quot;Saved {filename}&quot;) saved_count += 1 frame_count += 1 cap.release() print(f&quot;Extraction complete. {saved_count} frames saved.&quot;) video_file = &quot;VID_20250725_173235.mp4&quot; SOURCE = r&quot;C:\Users\hp\tableTop\gs\3dmodeling\data&quot; full_SOURCE = os.path.join(SOURCE, &quot;2dcarm&quot;) output_folder = r&quot;C:\Users\hp\tableTop\gs\3dmodeling\data\2dcarm\extracted_frames_VID_20250725_173235_20\sequential&quot; get_info(full_SOURCE, video_file) # Extract frame every 2 seconds # extract_frames(video_file, output_folder, interval_sec=20) # Or extract every 30 frames interval_frame = 15 extract_frames(full_SOURCE, video_file, output_folder, interval_frame=interval_frame) While the video is recorded at 30 fps, meaning even a short video contains thousands of images, my laptop lacks the necessary hardware to handle the processing workload for such a large number of images, as this volume overwhelms its limits. After some experimentation, I found it acceptable to use only one out of every 15-20 frames. The downside of this sparse sampling is discussed later in the results section. ABC Such a simplistic approach to frame extraction relies on one key assumption: the camera moves steadily around the C-arm. I practiced several times to hold my hand steady while filming. I also learned to leave enough space around the C-arm for walking while filming and to avoid stopping at any spot for too long or too briefly. COLMAP performs Sparse Reconstruction Step 1: Extract features from images using SIFT. Step 2: Estimate camera poses and properties. CAMERA INTRINSIC CAMERA PROPERTIES POSE PRIOR Step 3: Utilize the signposts strategically placed around the C-arm. Step 4: Perform image pair matching using an exhaustive approach since the recording forms a loop around the C-arm. DATABASE MATCHING MATCH PAIRS MATCH PAIRS CORRESPONDING MATCHES The red dots with a green straight line to denote their direction connection are numerous or abundant for all the signposts I deliberately put down on the floor next to the machine. Super helpful for matching images. Step 5: Conduct sparse reconstruction. Step 6: Note that the reconstruction quality need not be perfect—“high” quality is sufficient—and it is not necessary to use all images. Step 7. Review reconstruction results and errors Step 8: Export models; if distortion is present, undistort images before export. For example, if the camera were not a pinhole but a fisheye type, COLMAP can undistort fisheye images to pinhole-like images for sparse reconstruction. UNDISTORTED The reconstruction quality varies: a good reconstruction shows a red camera track that closely follows the path around the C-arm, providing a holistic view. Since no absolute ground truth exists for the smartphone camera poses, the alignment must be assessed subjectively by comparing the video filming with COLMAP’s estimates. Conversely, a poor reconstruction is evident when the camera track breaks off, and many cameras are inaccurately clustered, pointing in wrong directions. Such misalignment indicates incorrect camera positioning, which can severely impact subsequent processing. Results of Gaussian Splatting on both good and poor reconstructions will reveal stark contrasts. Vertical camera frames vs horizontal camera frames Experience indicates that vertical frames do not increase reprojection errors in sparse scene reconstruction, but many frames are dropped because they cannot be fitted with the rest. Conversely, horizontal frames fit more easily with similar reprojection errors and offer many more observations per image. However, Gaussian Splatting is quite robust. Despite slightly higher errors and fewer images fitted into the scene, the final quality remains decent, as will be discussed later. Following up next is Gaussian Splatting - Gaussian Splatting Blog posts on this topics Gaussian Splatting - Introduction Gaussian Splatting - Toy Example Gaussian Splatting - Camera Poses Gaussian Splatting - Gaussian Splatting Gaussian Splatting - Meshes and Beyond Gaussian Splatting - Failure, Success, and Lesson Learned</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="/assets/images/gaussianSplatting/cover.avif" /><media:content medium="image" url="/assets/images/gaussianSplatting/cover.avif" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Gaussian Splatting - A toy example of C-arm machine</title><link href="/deep_learning/2025/08/11/GaussianSplatting_toyExample/" rel="alternate" type="text/html" title="Gaussian Splatting - A toy example of C-arm machine" /><published>2025-08-11T00:00:00+08:00</published><updated>2025-08-11T00:00:00+08:00</updated><id>/deep_learning/2025/08/11/GaussianSplatting_toyExample</id><content type="html" xml:base="/deep_learning/2025/08/11/GaussianSplatting_toyExample/">&lt;h1 id=&quot;my-journal-and-some-practical-toy-example&quot;&gt;My Journal and some practical toy example&lt;/h1&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;My previous blog post &lt;a href=&quot;/deep_learning/2025/08/10/GaussianSplatting_introduction/&quot;&gt;Gaussian Splatting - Camera Poses&lt;/a&gt; talks about all the pertain basic concepts, including Gaussian Splatting, Splatting, Unity, Blender, C-arm Machine and Neural Radiance Field.&lt;/p&gt;

&lt;p&gt;As a biomedical engineer working in the orthopedic domain, one very useful imaging tool we use heavily is the intra-operative imaging system called C-arm for 2D or O-arm for 3D scan of patient anatomical regions (spine). The most flexible feature is its arm that can rotate along the curvature, providing different viewing angles for various organs. In particular, for the spine, when the receiver (cylinder) is located at the bottom and the source (rectangle block) is located at the top, this pose is good for Anterior-Posterior view. Conversely, when the receiver and the source are located at the side, this pose is good for Medial-Lateral view.&lt;/p&gt;

&lt;p&gt;The system is portable, flexible, fast, and safe, the x-ray dose is quite low) can be use in the operating theater. Extremely useful for navigation during minimally invasive surgeries and checking after implant or guide tools insertion.&lt;/p&gt;

&lt;p&gt;Note: There is a lead metal shield in place so that when C-arm is in use, the shield prevents any leakage of X-ray.&lt;/p&gt;

&lt;h2 id=&quot;rationale&quot;&gt;Rationale&lt;/h2&gt;

&lt;p&gt;Neural Radiance Field is a breakthrough. It was first published in 2020, causing a revolution in the computer vision community. The link to the paper  &lt;a href=&quot;https://arxiv.org/abs/2003.08934&quot;&gt;NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The term neural radiance field has three keywords: neural, radiance, and field. Neural stands for neural network. Radiance stands for ray tracing technique for image rendering. Field is a mathematical terms , presenting a model.&lt;/p&gt;

&lt;p&gt;Together, neural radiance field uses a neural network (MLP) to “memorize” the color, opacity of a scene in three-dimensional space (field). Once the network has memorized or has been trained, the information is encoded by the network implicitly. We can then extract it at every location by querying the network at each location. To train the network, neural radiance field technique uses only a stack of images (RGB) with camera poses.&lt;/p&gt;

&lt;p&gt;Ray tracing is also included because the training needs feedback on its accuracy and this accuracy is determined by comparing the similarity between the images rendered by the network and actual ground-truth images, often obtained at the start.&lt;/p&gt;

&lt;p&gt;Ray tracing technique is a method to keep track of the light intensity attenuation (like a physics model) as it passes through the model in space. The path or trajectory it takes before hitting an image screen determines its final color and intensity we ultimately see. To obtain an RGB image, we place a camera towards the scene center (known camera pose or camera extrinsic), follow these camera rays (one for each pixel on the screen), query the network for information about color and density at each sample point on the ray’s trajectory, and then sum up all the attention to estimate the final pixel value (RGB) on screen.&lt;/p&gt;

&lt;p&gt;The really amazing idea is that ray tracing and its summation can be designed in a way that fits into the training framework of a neural network. Put simply, these operations are auto-differentiable and numerically stable.&lt;/p&gt;

&lt;p&gt;While the neural radiance field results look impressive (Peruse the original publication and subsequent work), the training and inference process is very slow. Much time is spent on querying the MLP for spatial information and moving along the ray trajectories.&lt;/p&gt;

&lt;h2 id=&quot;instead-of-ray-tracing-what-about-splatting&quot;&gt;Instead of ray tracing, what about splatting?&lt;/h2&gt;

&lt;p&gt;Instead of ray tracing, Gaussian splatting uses splatting to render an image, a process that is much much faster once the model has been trained.&lt;/p&gt;

&lt;p&gt;Splatting is fast because the objects in the scene are computed to work out their relative positions during training. During rendering, only draw objects that are visible (for example only draw a red ball that is in front of a green ball because the green ball is hidden behind the red and is invisible).&lt;/p&gt;

&lt;p&gt;Of course, our clever readers would point out that we cannot model reflection or other lighting effects. It turns out, we don’t need to include all lighting effects and trace all light rays to render reasonably photorealistic images.&lt;/p&gt;

&lt;p&gt;Following up next is &lt;a href=&quot;/deep_learning/2025/08/12/GaussianSplatting_camera_poses/&quot;&gt;Gaussian Splatting - Camera Poses&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;blog-posts-on-this-topics&quot;&gt;Blog posts on this topics&lt;/h1&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;/deep_learning/2025/08/10/GaussianSplatting_introduction/&quot;&gt;Gaussian Splatting - Introduction&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/deep_learning/2025/08/11/GaussianSplatting_toyExample/&quot;&gt;Gaussian Splatting - Toy Example&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/deep_learning/2025/08/12/GaussianSplatting_camera_poses/&quot;&gt;Gaussian Splatting - Camera Poses&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/deep_learning/2025/08/13/GaussianSplatting_gaussianSplatting/&quot;&gt;Gaussian Splatting - Gaussian Splatting&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/deep_learning/2025/08/14/GaussianSplatting_meshesAndBeyond/&quot;&gt;Gaussian Splatting - Meshes and Beyond&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/deep_learning/2025/08/19/GaussianSplatting_failuresAndLessons/&quot;&gt;Gaussian Splatting - Failure, Success, and Lesson Learned&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Wilson Fok</name></author><category term="Deep_Learning" /><summary type="html">My Journal and some practical toy example Introduction My previous blog post Gaussian Splatting - Camera Poses talks about all the pertain basic concepts, including Gaussian Splatting, Splatting, Unity, Blender, C-arm Machine and Neural Radiance Field. As a biomedical engineer working in the orthopedic domain, one very useful imaging tool we use heavily is the intra-operative imaging system called C-arm for 2D or O-arm for 3D scan of patient anatomical regions (spine). The most flexible feature is its arm that can rotate along the curvature, providing different viewing angles for various organs. In particular, for the spine, when the receiver (cylinder) is located at the bottom and the source (rectangle block) is located at the top, this pose is good for Anterior-Posterior view. Conversely, when the receiver and the source are located at the side, this pose is good for Medial-Lateral view. The system is portable, flexible, fast, and safe, the x-ray dose is quite low) can be use in the operating theater. Extremely useful for navigation during minimally invasive surgeries and checking after implant or guide tools insertion. Note: There is a lead metal shield in place so that when C-arm is in use, the shield prevents any leakage of X-ray. Rationale Neural Radiance Field is a breakthrough. It was first published in 2020, causing a revolution in the computer vision community. The link to the paper NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis The term neural radiance field has three keywords: neural, radiance, and field. Neural stands for neural network. Radiance stands for ray tracing technique for image rendering. Field is a mathematical terms , presenting a model. Together, neural radiance field uses a neural network (MLP) to “memorize” the color, opacity of a scene in three-dimensional space (field). Once the network has memorized or has been trained, the information is encoded by the network implicitly. We can then extract it at every location by querying the network at each location. To train the network, neural radiance field technique uses only a stack of images (RGB) with camera poses. Ray tracing is also included because the training needs feedback on its accuracy and this accuracy is determined by comparing the similarity between the images rendered by the network and actual ground-truth images, often obtained at the start. Ray tracing technique is a method to keep track of the light intensity attenuation (like a physics model) as it passes through the model in space. The path or trajectory it takes before hitting an image screen determines its final color and intensity we ultimately see. To obtain an RGB image, we place a camera towards the scene center (known camera pose or camera extrinsic), follow these camera rays (one for each pixel on the screen), query the network for information about color and density at each sample point on the ray’s trajectory, and then sum up all the attention to estimate the final pixel value (RGB) on screen. The really amazing idea is that ray tracing and its summation can be designed in a way that fits into the training framework of a neural network. Put simply, these operations are auto-differentiable and numerically stable. While the neural radiance field results look impressive (Peruse the original publication and subsequent work), the training and inference process is very slow. Much time is spent on querying the MLP for spatial information and moving along the ray trajectories. Instead of ray tracing, what about splatting? Instead of ray tracing, Gaussian splatting uses splatting to render an image, a process that is much much faster once the model has been trained. Splatting is fast because the objects in the scene are computed to work out their relative positions during training. During rendering, only draw objects that are visible (for example only draw a red ball that is in front of a green ball because the green ball is hidden behind the red and is invisible). Of course, our clever readers would point out that we cannot model reflection or other lighting effects. It turns out, we don’t need to include all lighting effects and trace all light rays to render reasonably photorealistic images. Following up next is Gaussian Splatting - Camera Poses Blog posts on this topics Gaussian Splatting - Introduction Gaussian Splatting - Toy Example Gaussian Splatting - Camera Poses Gaussian Splatting - Gaussian Splatting Gaussian Splatting - Meshes and Beyond Gaussian Splatting - Failure, Success, and Lesson Learned</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="/assets/images/gaussianSplatting/cover.avif" /><media:content medium="image" url="/assets/images/gaussianSplatting/cover.avif" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Gaussian Splatting - A brief introduction</title><link href="/deep_learning/2025/08/10/GaussianSplatting_introduction/" rel="alternate" type="text/html" title="Gaussian Splatting - A brief introduction" /><published>2025-08-10T00:00:00+08:00</published><updated>2025-08-10T00:00:00+08:00</updated><id>/deep_learning/2025/08/10/GaussianSplatting_introduction</id><content type="html" xml:base="/deep_learning/2025/08/10/GaussianSplatting_introduction/">&lt;p&gt;This article details my learning of a new, emerging and very interesting technology in recent years. No, it isn’t Large Language Model. It is Gaussian Splatting and Neural Radiance Field. A technology I think has the potential to change our way of creating digital assets in future.&lt;/p&gt;

&lt;h1 id=&quot;what-is-neural-radiance-field&quot;&gt;What is Neural Radiance Field?&lt;/h1&gt;
&lt;p&gt;Neural Radiance Field (NeRF) is an advanced technique in computer vision that creates detailed 3D scenes from a set of 2D photos taken from different angles. Imagine you take many pictures of a place or object, and NeRF learns how light interacts with the surfaces in that scene. It uses a special kind of artificial intelligence called a neural network to understand both the shape and appearance of everything in the scene.&lt;/p&gt;

&lt;p&gt;The neural network takes as input the position in 3D space and the direction you are looking from and predicts the color and density at that point. By doing this for many points along imaginary camera rays through the scene, NeRF can generate realistic images from new viewpoints that were not part of the original photos. from the research literature, we have discover that when the network takes in a plain vanilla input of XYZ coordinates and the viewing directions, the network fails to learn or generalize the geometry of the scene. A better approach is to transform the XYZ coordinates and the viewing directions with high frequency Fourier transform (sine / cosine transform). That is, by embedding or encoding the low frequency spatial signal in a high frequency domain. This higher frequencies matches the structure of this type of neural network architecture (often a Multilayer Perceptron, MLP). A theory that is supported in Neural Tangent Kernel Research. Therefore, the MLP can learn a continuous / non-linear volume representation.&lt;/p&gt;

&lt;p&gt;Along with faithfully reproducing the assets’ geometry and property, what really makes NeRF special is its ability to recreate complex lighting effects like reflections and transparency, producing photo-realistic 3D views. This process involves training a neural network to become a continuous function that models both geometry and how light behaves based on the input images. This ability is particularly important to those who would like to generate photo-realistic images from any arbitrary viewpoints under a similar lighting conditions without explicitly modelling a mesh.&lt;/p&gt;

&lt;p&gt;NeRF has many applications including virtual reality, gaming, medical imaging, robotic vision, and satellite mapping for urban planning of smart cities, where generating accurate 3D models from photos is essential. It allows seeing a scene from any angle with incredible detail and realistic lighting without needing complex physical models or extra hardware beyond a set of photos&lt;/p&gt;

&lt;h1 id=&quot;what-is-gaussian-splatting&quot;&gt;What is Gaussian Splatting?&lt;/h1&gt;
&lt;p&gt;Gaussian Splatting in computer vision is a modern technique used to create detailed 3D models of scenes from multiple photos or videos. Imagine you have many pictures of an object or place taken from different angles. Gaussian Splatting takes these images and first creates a rough 3D point cloud—a collection of points that represent the shape of the scene. Then, instead of simply using points, each point is transformed into a tiny 3D ellipsoid, called a Gaussian splat, that can be stretched, colored, and made semi-transparent.&lt;/p&gt;

&lt;p&gt;These Gaussian splats act like small, fuzzy blobs that together form an accurate and smooth 3D representation of the scene. The method then optimizes these blobs’ positions, sizes, colors, and transparency to best match the original images.&lt;/p&gt;

&lt;p&gt;The splats undergo an optimization process that adjusts their parameters to minimize the difference between rendered images from splats and the original input images. This process uses differentiable rasterization for projecting splats onto a 2D image plane, loss functions to measure image differences, and optimization algorithms to fine-tune the splats. Adaptive control may add or remove splats to maintain detail where necessary. Finally, the splats are projected onto a 2D screen to render realistic views of the scene from any angle in real time.&lt;/p&gt;

&lt;p&gt;Splatting is a rendering technique used in computer graphics where instead of rendering traditional geometric primitives like triangles. Each splat corresponds to a localized region of the scene, essentially forming a fuzzy or blurred footprint in the image space. During rendering, these splats are projected onto the 2D screen and composited together to form a final image. Because splats are volumetric and translucent, they can effectively represent complex shapes, surfaces, and lighting effects without the need for explicit meshes.&lt;/p&gt;

&lt;p&gt;In the context of Gaussian Splatting for 3D reconstruction, each Gaussian splat is a small 3D ellipsoid described by parameters such as position, shape, orientation, color, and transparency. This volumetric approach allows smooth blending of splats, capturing fine scene details and complex light interactions.&lt;/p&gt;

&lt;p&gt;During training or preprocessing, the Gaussian splats are sorted in a specific order, typically along the viewing direction or depth from the camera perspective. This sorting is crucial because rendering involves compositing splats back-to-front (or using other blending techniques like alpha blending) to correctly accumulate color and transparency information. Pre-sorting enables the renderer to quickly traverse the splats in the correct order during image synthesis, thereby accelerating rendering time considerably.&lt;/p&gt;

&lt;p&gt;Unlike Neural Radiance Fields (NeRF), Gaussian Splatting does not rely on heavy, layered neural networks, so it can achieve lightning-fast training and real-time rendering by leveraging its precomputed sort order to efficiently composite splats, enabling real-time rendering of new images from arbitrary viewpoints. It produces highly detailed, photorealistic 3D reconstructions with complex lighting and reflections, suitable for real-time applications where speed, quality and efficiency are crucial. The method excels in rendering quality and efficiency, producing smooth, seamless models from raw images or videos. Hence, what makes Gaussian Splatting special is that it produces highly detailed and photorealistic 3D renderings without relying on complex neural networks which is the speed bottleneck in real world practice.&lt;/p&gt;

&lt;h1 id=&quot;what-is-colmap&quot;&gt;What is COLMAP?&lt;/h1&gt;
&lt;p&gt;COLMAP is a widely-used computer vision software that reconstructs 3D models from overlapping photos by detecting and matching keypoints across images that are scale invariant. It estimates the cameras’ intrinsic parameters (internal characteristics like focal length and optical center) and extrinsic parameters (position and orientation relative to the scene) through Structure-from-Motion (SfM) and then generates dense 3D reconstructions through Multi-View Stereo (MVS).&lt;/p&gt;

&lt;h3 id=&quot;camera-models-in-3d-reconstruction&quot;&gt;Camera Models in 3D Reconstruction&lt;/h3&gt;

&lt;p&gt;Camera models mathematically describe how 3D points in the world are projected onto a 2D image plane. The most common is the &lt;strong&gt;pinhole camera model&lt;/strong&gt;, which assumes an idealized camera with a single point through which light rays pass (the optical center) and an image plane where the scene is projected.&lt;/p&gt;

&lt;p&gt;The key mathematical concept involves projecting a 3D point&lt;/p&gt;

\[P_W = (X, Y, Z, 1)^T\]

&lt;p&gt;in homogeneous world coordinates to a 2D point \(p = (x, y, 1)^T\) on the image plane using a &lt;strong&gt;projection matrix \(\mathbf{M}\)&lt;/strong&gt; that combines intrinsic and extrinsic parameters:&lt;/p&gt;

\[p = \mathbf{M} P_W = \mathbf{K} [\mathbf{R} \mid \mathbf{t}] P_W\]

&lt;p&gt;Where:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;\(\mathbf{K}\) is the &lt;strong&gt;intrinsic matrix&lt;/strong&gt; encoding the focal length \(f\), skew, and principal point \((c_x, c_y)\):&lt;/li&gt;
&lt;/ul&gt;

\[\mathbf{K} = \begin{bmatrix}
f_x &amp;amp; s &amp;amp; c_x \\
0 &amp;amp; f_y &amp;amp; c_y \\
0 &amp;amp; 0 &amp;amp; 1
\end{bmatrix}\]

&lt;ul&gt;
  &lt;li&gt;\(\mathbf{R}\) is a \(3 \times 3\) rotation matrix representing the camera orientation.&lt;/li&gt;
  &lt;li&gt;\(\mathbf{t}\) is a translation vector representing the camera position in the world.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The extrinsic parameters \((\mathbf{R}, \mathbf{t})\) describe the transformation from world coordinates to the camera coordinate system.&lt;/p&gt;

&lt;p&gt;This projection involves a nonlinear division by depth (the \(Z\) coordinate in camera space), which results in perspective effects like objects appearing smaller when they are farther away.&lt;/p&gt;

&lt;h3 id=&quot;colmaps-role&quot;&gt;COLMAP’s Role&lt;/h3&gt;

&lt;p&gt;COLMAP estimates these intrinsic \(\mathbf{K}\) and extrinsic \(\mathbf{R}, \mathbf{t}\) parameters by feature matching and optimizing reprojection error, enabling accurate mapping between 3D points and image pixels. This camera model forms the basis for subsequent 3D reconstruction using point clouds, meshes, or volumetric representations like NeRF and Gaussian Splatting.&lt;/p&gt;

&lt;p&gt;Put this in layman term, COLMAP allows us to set a camera model (a math equation that describes our smart phone camera for example). Then reconstruction works by first detecting distinctive features in each photo, then matching these features across multiple images to find common points in the scene. Using these matches, COLMAP estimates the positions and orientations of the cameras that took the photos and reconstructs a sparse 3D point cloud representing the scene’s structure. This process is known as Structure-from-Motion (SfM).&lt;/p&gt;

&lt;p&gt;After building the initial sparse 3D model, COLMAP can also perform Multi-View Stereo (MVS) to generate a much denser, detailed 3D reconstruction. This involves estimating depth and surface information from the photos to fill in the gaps and produce a realistic 3D surface.&lt;/p&gt;

&lt;p&gt;In essence, COLMAP recovers detailed camera geometry at the photo capture stage. Both NeRF and Gaussian splatting require an accurate reconstruction of the scene, detailing the camera’s intrinsic (FOV, center point) and extrinsic (SE3 Poses) properties. Therefore, COLMAP makes it possible for NeRF or Gaussian Splatting to render photorealistic novel views by understanding exactly how each photo was taken.&lt;/p&gt;

&lt;h1 id=&quot;what-is-unity&quot;&gt;What is Unity?&lt;/h1&gt;
&lt;p&gt;Developed over
 more than two decades, Unity is a popular game engine that helps people create video games and interactive experiences easily. Think of it like a powerful toolkit and workspace where game developers can build virtual worlds, characters, and gameplay without starting from scratch. Unity provides an editor environment where you can arrange objects, add colors, sounds, and behaviors—all visually and intuitively.&lt;/p&gt;

&lt;p&gt;In Unity, everything in a game is made from “GameObjects,” which are like containers that can hold different features called “Components.” For example, a GameObject can have a model to show what it looks like, physics to make it move naturally, and scripts—programs written in the C# language—that tell it how to behave. Developers can drag and drop assets such as 3D models, textures, and sounds into their scenes to create immersive games.&lt;/p&gt;

&lt;p&gt;Unity supports both 2D and 3D games and works on many platforms like computers, phones, and gaming consoles. It also includes tools for creating realistic lighting, animations, and physics effects. Because it’s user-friendly and powerful, Unity is widely used by indie developers and big studios alike for making games, VR experiences, simulations, and more. It speeds up game development by handling complex tasks so creators can focus on their ideas and creativity.&lt;/p&gt;

&lt;h1 id=&quot;what-is-blender&quot;&gt;What is Blender?&lt;/h1&gt;

&lt;p&gt;Blender is a free, open-source software used to create 3D graphics and animations. Imagine it as a digital art studio where you can build models of anything—from characters and objects to entire worlds—using your computer. It helps artists design movies, video games, visual effects, and even 3D printed objects. Blender includes tools for sculpting shapes, adding colors and textures, creating movements, and lighting scenes realistically. Because it’s open source, anyone can use it without paying and even contribute to improving it. It’s popular for its powerful features and supportive community, making 3D design accessible to beginners and professionals alike.&lt;/p&gt;

&lt;p&gt;While Unity focuses primarily on how a game works—its rules, behaviors, and the transitions between different states or actions—Blender is centered around crafting the visual assets that bring those games to life. Unity handles the logic and interactions, enabling developers to script gameplay, control physics, and manage how objects respond to player input or environmental factors. Meanwhile, Blender is the creative tool used to design and sculpt the detailed 3D models, textures, and animations that populate those game worlds. In essence, Blender builds the impressive visuals, and Unity gives those visuals purpose through interactive gameplay and programming. Together, they form a powerful pipeline for game development, with Blender focused on asset creation and Unity focused on game logic.&lt;/p&gt;

&lt;h1 id=&quot;what-is-a-c-arm-imaging-machine&quot;&gt;What is a C-arm imaging machine?&lt;/h1&gt;
&lt;p&gt;A C-arm imaging machine is a special X-ray device used in operating rooms to help doctors see inside the body in real time during surgeries. Named for its C-shaped arm, it connects an X-ray source on one end and a detector on the other, allowing images from various angles without moving the patient. It shows detailed, live X-ray pictures of bones, implants, and instruments, aiding surgeons in precise work, especially in orthopedic, cardiac, and emergency procedures. Its mobility and flexibility make it invaluable for guiding complex surgeries safely and accurately.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/gaussianSplatting/avantic.jpeg&quot; style=&quot;display: block; margin-left: auto; margin-right: auto; width: 90%;&quot; /&gt;&lt;/p&gt;
&lt;figcaption style=&quot;text-align: center;&quot;&gt;AVANTIC&lt;/figcaption&gt;

&lt;p&gt;Now that we have talked about all the basic concepts and applications. Let’s dive into my toy example of building a mesh for a 2D C-arm machine using a smartphone.&lt;/p&gt;

&lt;p&gt;Following up next is &lt;a href=&quot;/deep_learning/2025/08/11/GaussianSplatting_toyExample/&quot;&gt;Gaussian Splatting - Toy Example&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;blog-posts-on-this-topics&quot;&gt;Blog posts on this topics&lt;/h1&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;/deep_learning/2025/08/10/GaussianSplatting_introduction/&quot;&gt;Gaussian Splatting - Introduction&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/deep_learning/2025/08/11/GaussianSplatting_toyExample/&quot;&gt;Gaussian Splatting - Toy Example&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/deep_learning/2025/08/12/GaussianSplatting_camera_poses/&quot;&gt;Gaussian Splatting - Camera Poses&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/deep_learning/2025/08/13/GaussianSplatting_gaussianSplatting/&quot;&gt;Gaussian Splatting - Gaussian Splatting&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/deep_learning/2025/08/14/GaussianSplatting_meshesAndBeyond/&quot;&gt;Gaussian Splatting - Meshes and Beyond&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/deep_learning/2025/08/19/GaussianSplatting_failuresAndLessons/&quot;&gt;Gaussian Splatting - Failure, Success, and Lesson Learned&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Wilson Fok</name></author><category term="Deep_Learning" /><summary type="html">This article details my learning of a new, emerging and very interesting technology in recent years. No, it isn’t Large Language Model. It is Gaussian Splatting and Neural Radiance Field. A technology I think has the potential to change our way of creating digital assets in future. What is Neural Radiance Field? Neural Radiance Field (NeRF) is an advanced technique in computer vision that creates detailed 3D scenes from a set of 2D photos taken from different angles. Imagine you take many pictures of a place or object, and NeRF learns how light interacts with the surfaces in that scene. It uses a special kind of artificial intelligence called a neural network to understand both the shape and appearance of everything in the scene. The neural network takes as input the position in 3D space and the direction you are looking from and predicts the color and density at that point. By doing this for many points along imaginary camera rays through the scene, NeRF can generate realistic images from new viewpoints that were not part of the original photos. from the research literature, we have discover that when the network takes in a plain vanilla input of XYZ coordinates and the viewing directions, the network fails to learn or generalize the geometry of the scene. A better approach is to transform the XYZ coordinates and the viewing directions with high frequency Fourier transform (sine / cosine transform). That is, by embedding or encoding the low frequency spatial signal in a high frequency domain. This higher frequencies matches the structure of this type of neural network architecture (often a Multilayer Perceptron, MLP). A theory that is supported in Neural Tangent Kernel Research. Therefore, the MLP can learn a continuous / non-linear volume representation. Along with faithfully reproducing the assets’ geometry and property, what really makes NeRF special is its ability to recreate complex lighting effects like reflections and transparency, producing photo-realistic 3D views. This process involves training a neural network to become a continuous function that models both geometry and how light behaves based on the input images. This ability is particularly important to those who would like to generate photo-realistic images from any arbitrary viewpoints under a similar lighting conditions without explicitly modelling a mesh. NeRF has many applications including virtual reality, gaming, medical imaging, robotic vision, and satellite mapping for urban planning of smart cities, where generating accurate 3D models from photos is essential. It allows seeing a scene from any angle with incredible detail and realistic lighting without needing complex physical models or extra hardware beyond a set of photos What is Gaussian Splatting? Gaussian Splatting in computer vision is a modern technique used to create detailed 3D models of scenes from multiple photos or videos. Imagine you have many pictures of an object or place taken from different angles. Gaussian Splatting takes these images and first creates a rough 3D point cloud—a collection of points that represent the shape of the scene. Then, instead of simply using points, each point is transformed into a tiny 3D ellipsoid, called a Gaussian splat, that can be stretched, colored, and made semi-transparent. These Gaussian splats act like small, fuzzy blobs that together form an accurate and smooth 3D representation of the scene. The method then optimizes these blobs’ positions, sizes, colors, and transparency to best match the original images. The splats undergo an optimization process that adjusts their parameters to minimize the difference between rendered images from splats and the original input images. This process uses differentiable rasterization for projecting splats onto a 2D image plane, loss functions to measure image differences, and optimization algorithms to fine-tune the splats. Adaptive control may add or remove splats to maintain detail where necessary. Finally, the splats are projected onto a 2D screen to render realistic views of the scene from any angle in real time. Splatting is a rendering technique used in computer graphics where instead of rendering traditional geometric primitives like triangles. Each splat corresponds to a localized region of the scene, essentially forming a fuzzy or blurred footprint in the image space. During rendering, these splats are projected onto the 2D screen and composited together to form a final image. Because splats are volumetric and translucent, they can effectively represent complex shapes, surfaces, and lighting effects without the need for explicit meshes. In the context of Gaussian Splatting for 3D reconstruction, each Gaussian splat is a small 3D ellipsoid described by parameters such as position, shape, orientation, color, and transparency. This volumetric approach allows smooth blending of splats, capturing fine scene details and complex light interactions. During training or preprocessing, the Gaussian splats are sorted in a specific order, typically along the viewing direction or depth from the camera perspective. This sorting is crucial because rendering involves compositing splats back-to-front (or using other blending techniques like alpha blending) to correctly accumulate color and transparency information. Pre-sorting enables the renderer to quickly traverse the splats in the correct order during image synthesis, thereby accelerating rendering time considerably. Unlike Neural Radiance Fields (NeRF), Gaussian Splatting does not rely on heavy, layered neural networks, so it can achieve lightning-fast training and real-time rendering by leveraging its precomputed sort order to efficiently composite splats, enabling real-time rendering of new images from arbitrary viewpoints. It produces highly detailed, photorealistic 3D reconstructions with complex lighting and reflections, suitable for real-time applications where speed, quality and efficiency are crucial. The method excels in rendering quality and efficiency, producing smooth, seamless models from raw images or videos. Hence, what makes Gaussian Splatting special is that it produces highly detailed and photorealistic 3D renderings without relying on complex neural networks which is the speed bottleneck in real world practice. What is COLMAP? COLMAP is a widely-used computer vision software that reconstructs 3D models from overlapping photos by detecting and matching keypoints across images that are scale invariant. It estimates the cameras’ intrinsic parameters (internal characteristics like focal length and optical center) and extrinsic parameters (position and orientation relative to the scene) through Structure-from-Motion (SfM) and then generates dense 3D reconstructions through Multi-View Stereo (MVS). Camera Models in 3D Reconstruction Camera models mathematically describe how 3D points in the world are projected onto a 2D image plane. The most common is the pinhole camera model, which assumes an idealized camera with a single point through which light rays pass (the optical center) and an image plane where the scene is projected. The key mathematical concept involves projecting a 3D point \[P_W = (X, Y, Z, 1)^T\] in homogeneous world coordinates to a 2D point \(p = (x, y, 1)^T\) on the image plane using a projection matrix \(\mathbf{M}\) that combines intrinsic and extrinsic parameters: \[p = \mathbf{M} P_W = \mathbf{K} [\mathbf{R} \mid \mathbf{t}] P_W\] Where: \(\mathbf{K}\) is the intrinsic matrix encoding the focal length \(f\), skew, and principal point \((c_x, c_y)\): \[\mathbf{K} = \begin{bmatrix} f_x &amp;amp; s &amp;amp; c_x \\ 0 &amp;amp; f_y &amp;amp; c_y \\ 0 &amp;amp; 0 &amp;amp; 1 \end{bmatrix}\] \(\mathbf{R}\) is a \(3 \times 3\) rotation matrix representing the camera orientation. \(\mathbf{t}\) is a translation vector representing the camera position in the world. The extrinsic parameters \((\mathbf{R}, \mathbf{t})\) describe the transformation from world coordinates to the camera coordinate system. This projection involves a nonlinear division by depth (the \(Z\) coordinate in camera space), which results in perspective effects like objects appearing smaller when they are farther away. COLMAP’s Role COLMAP estimates these intrinsic \(\mathbf{K}\) and extrinsic \(\mathbf{R}, \mathbf{t}\) parameters by feature matching and optimizing reprojection error, enabling accurate mapping between 3D points and image pixels. This camera model forms the basis for subsequent 3D reconstruction using point clouds, meshes, or volumetric representations like NeRF and Gaussian Splatting. Put this in layman term, COLMAP allows us to set a camera model (a math equation that describes our smart phone camera for example). Then reconstruction works by first detecting distinctive features in each photo, then matching these features across multiple images to find common points in the scene. Using these matches, COLMAP estimates the positions and orientations of the cameras that took the photos and reconstructs a sparse 3D point cloud representing the scene’s structure. This process is known as Structure-from-Motion (SfM). After building the initial sparse 3D model, COLMAP can also perform Multi-View Stereo (MVS) to generate a much denser, detailed 3D reconstruction. This involves estimating depth and surface information from the photos to fill in the gaps and produce a realistic 3D surface. In essence, COLMAP recovers detailed camera geometry at the photo capture stage. Both NeRF and Gaussian splatting require an accurate reconstruction of the scene, detailing the camera’s intrinsic (FOV, center point) and extrinsic (SE3 Poses) properties. Therefore, COLMAP makes it possible for NeRF or Gaussian Splatting to render photorealistic novel views by understanding exactly how each photo was taken. What is Unity? Developed over more than two decades, Unity is a popular game engine that helps people create video games and interactive experiences easily. Think of it like a powerful toolkit and workspace where game developers can build virtual worlds, characters, and gameplay without starting from scratch. Unity provides an editor environment where you can arrange objects, add colors, sounds, and behaviors—all visually and intuitively. In Unity, everything in a game is made from “GameObjects,” which are like containers that can hold different features called “Components.” For example, a GameObject can have a model to show what it looks like, physics to make it move naturally, and scripts—programs written in the C# language—that tell it how to behave. Developers can drag and drop assets such as 3D models, textures, and sounds into their scenes to create immersive games. Unity supports both 2D and 3D games and works on many platforms like computers, phones, and gaming consoles. It also includes tools for creating realistic lighting, animations, and physics effects. Because it’s user-friendly and powerful, Unity is widely used by indie developers and big studios alike for making games, VR experiences, simulations, and more. It speeds up game development by handling complex tasks so creators can focus on their ideas and creativity. What is Blender? Blender is a free, open-source software used to create 3D graphics and animations. Imagine it as a digital art studio where you can build models of anything—from characters and objects to entire worlds—using your computer. It helps artists design movies, video games, visual effects, and even 3D printed objects. Blender includes tools for sculpting shapes, adding colors and textures, creating movements, and lighting scenes realistically. Because it’s open source, anyone can use it without paying and even contribute to improving it. It’s popular for its powerful features and supportive community, making 3D design accessible to beginners and professionals alike. While Unity focuses primarily on how a game works—its rules, behaviors, and the transitions between different states or actions—Blender is centered around crafting the visual assets that bring those games to life. Unity handles the logic and interactions, enabling developers to script gameplay, control physics, and manage how objects respond to player input or environmental factors. Meanwhile, Blender is the creative tool used to design and sculpt the detailed 3D models, textures, and animations that populate those game worlds. In essence, Blender builds the impressive visuals, and Unity gives those visuals purpose through interactive gameplay and programming. Together, they form a powerful pipeline for game development, with Blender focused on asset creation and Unity focused on game logic. What is a C-arm imaging machine? A C-arm imaging machine is a special X-ray device used in operating rooms to help doctors see inside the body in real time during surgeries. Named for its C-shaped arm, it connects an X-ray source on one end and a detector on the other, allowing images from various angles without moving the patient. It shows detailed, live X-ray pictures of bones, implants, and instruments, aiding surgeons in precise work, especially in orthopedic, cardiac, and emergency procedures. Its mobility and flexibility make it invaluable for guiding complex surgeries safely and accurately. AVANTIC Now that we have talked about all the basic concepts and applications. Let’s dive into my toy example of building a mesh for a 2D C-arm machine using a smartphone. Following up next is Gaussian Splatting - Toy Example Blog posts on this topics Gaussian Splatting - Introduction Gaussian Splatting - Toy Example Gaussian Splatting - Camera Poses Gaussian Splatting - Gaussian Splatting Gaussian Splatting - Meshes and Beyond Gaussian Splatting - Failure, Success, and Lesson Learned</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="/assets/images/gaussianSplatting/cover.avif" /><media:content medium="image" url="/assets/images/gaussianSplatting/cover.avif" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">ODOO and me</title><link href="/others/2025/01/04/ODOO/" rel="alternate" type="text/html" title="ODOO and me" /><published>2025-01-04T00:00:00+08:00</published><updated>2025-01-04T00:00:00+08:00</updated><id>/others/2025/01/04/ODOO</id><content type="html" xml:base="/others/2025/01/04/ODOO/">&lt;h1 id=&quot;what-is-odoo&quot;&gt;What is ODOO?&lt;/h1&gt;
&lt;p&gt;ODOO is a versatile open-source Enterprise Resource Planning (ERP) and Customer Relationship Management (CRM) software that provides a comprehensive, integrated platform for managing a wide range of business functions.
Its modules cover purchasing, inventory management, manufacturing, sales, logistics, products and services, quality control, e-commerce websites, accounting, human resources, project management, marketing, CRM, internal chat room, and much more. By offering these diverse capabilities, ODOO enables businesses to consolidate their operations into a single platform, providing a holistic view of their activities.
If ODOO operates as advertised, it can provide valuable insights into the health of a business. For example, it can generate instantaneous accounting statements, track sales performance, evaluate warehouse efficiency, and deliver other timely, actionable information.
The paid Enterprise version offers additional support, advanced features, and access to more business modules with excellent tutorials, making it an attractive option for companies looking for a robust ERP solution. However, the Community Edition (OCA) is also highly effective for many organizations, especially small and medium-sized enterprises, even though some advanced modules may be less readily available.
ODOO’s broad scope, flexibility, and free customization options make it particularly appealing to small and medium-sized businesses (SMBs) for managing their operations efficiently without significant upfront investment.&lt;/p&gt;

&lt;p&gt;A quick summary of ODOO’s capabilities and features:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Community and Marketplace&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Active Community: ODOO has a vibrant community of developers and users that contribute to the software’s development, share best practices, and create various modules to expand its functionality.&lt;/p&gt;

&lt;p&gt;ODOO Apps Store: The official marketplace provides a variety of third-party applications that can be integrated into ODOO, allowing businesses to customize their experience further and add specific functionalities.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;User Experience and Interface&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Intuitive Design: ODOO’s user interface is designed to be user-friendly and intuitive, which can facilitate easier onboarding for new users. This aspect is critical for adoption within organizations.&lt;/p&gt;

&lt;p&gt;Mobile Accessibility: ODOO offers mobile applications, enabling users to manage operations on-the-go, which enhances flexibility and responsiveness in business operations.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Automation Capabilities&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Workflows and Automation: ODOO allows businesses to automate repetitive tasks and workflows, which can significantly enhance efficiency and reduce manual errors.&lt;/p&gt;

&lt;p&gt;Email Marketing and CRM Automation: ODOO includes features for automating marketing campaigns and client interactions, helping businesses maintain effective customer relations without constant manual intervention.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Reporting and Analytics&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Advanced Reporting Tools: ODOO provides robust reporting and analytics features that allow organizations to generate detailed insights into various aspects of their business, supporting better decision-making.&lt;/p&gt;

&lt;p&gt;Customizable Dashboards: Users can create customized dashboards to track key business indicators and relevant metrics at a glance.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Integration Capabilities&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;API Availability: ODOO offers RESTful APIs that empower organizations to integrate it with other software systems, facilitating seamless data exchange and improving operational coherence.&lt;/p&gt;

&lt;p&gt;Third-Party Integrations: Many businesses leverage ODOO’s ability to connect with existing software tools (like e-commerce platforms, payment gateways, etc.) to enhance their operational ecosystem.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Localization and Scalability&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Multi-Language and Currency Support: ODOO supports multiple languages and currencies, making it suitable for international operations and businesses targeting diverse markets.&lt;/p&gt;

&lt;p&gt;Scalability: ODOO can scale with the business as it grows, adding more modules or functionalities as needed. This scalability makes it ideal for startups that anticipate growth.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Training and Resources&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Documentation and Tutorials: ODOO offers comprehensive documentation and tutorials, which are helpful for users navigating the system for the first time and seeking to deepen their understanding.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Regular Updates and Support&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Frequent Updates: ODOO is continually updated to incorporate new features, enhancements, and security updates, ensuring that users have access to the latest technology.&lt;/p&gt;

&lt;p&gt;Enterprise Support Options: The enterprise version provides access to dedicated support channels, which can be crucial for businesses that require quick assistance.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Industry-Specific Solutions&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Industry-Specific Modules: ODOO offers industry-specific modules that can address the unique challenges and requirements of various sectors, such as manufacturing, retail, health care, and more.&lt;/p&gt;

&lt;h1 id=&quot;the-real-challenge-how-easy-is-it-to-realize-odoos-potential&quot;&gt;The Real Challenge: How Easy is it to Realize ODOO’s Potential?&lt;/h1&gt;
&lt;p&gt;ODOO serves as a centralized platform where all business activities and information are collected, analyzed, and reported. More importantly, its advanced analytical capabilities allow businesses to forecast trends and implement strategies such as Just-In-Time (JIT) supply chain management.
However, while ODOO’s potential is impressive, implementing and optimizing it for business-specific needs can be a challenging task. Success often depends on understanding the intricate details of business workflows and aligning them with ODOO’s capabilities.&lt;/p&gt;

&lt;h1 id=&quot;what-i-have-learned-from-using-odoo&quot;&gt;What I Have Learned from Using ODOO&lt;/h1&gt;
&lt;p&gt;Before exploring ODOO, my understanding of financial and goods/services flows was primarily based on theoretical economic principles. However, diving into ODOO’s modules has given me a much deeper, hands-on understanding of these concepts.
I followed the tutorials for all the major modules, including purchasing, inventory management, manufacturing, sales, logistics, e-commerce, accounting, HR, project management, and CRM. By cross-referencing the entries I created in these modules, I gained insights into the natural connections and the cascading flow of information across functional departments. This exercise was enlightening and gave me a practical perspective on how businesses operate holistically.
To experiment, I created dummy data for a hypothetical manufacturing process. I defined some items as purchasable while others were assembled in-house. Through this process, I learned how production records are created and how principles of manufacturing are reflected in ODOO. This exercise demonstrated the goods and services flow, while the accompanying accounting entries—such as purchase orders, sales invoices, bills, and receipts—illustrated the money flow.
Although I am not an expert in accounting, ODOO helped me understand basic concepts such as current assets, revenues, expenses, accounts receivable, and accounts payable. By experimenting with fake sales orders, receipts, and refunds, I learned how physical activities and financial records interconnect. For example:
•	When refunds are issued, revenue and expenses must be adjusted accordingly, while the returned goods are recorded back into inventory or are sent to scrap at the warehouse.
•	Reconciliation plays a crucial role in ensuring that all numbers align accurately.
ODOO’s ability to simulate processes with dummy data makes it an excellent tool for learning and experimentation.&lt;/p&gt;

&lt;h1 id=&quot;who-would-benefit-from-odoo&quot;&gt;Who Would Benefit from ODOO?&lt;/h1&gt;
&lt;p&gt;I recommend ODOO to:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Small and Medium-Sized Businesses (SMBs): ODOO is cost-effective, and even its Enterprise version is reasonably priced. Its flexibility and comprehensive feature set can help SMBs streamline their operations without investing in expensive proprietary solutions.&lt;/li&gt;
  &lt;li&gt;Beginners in Business Operations: For those unfamiliar with accounting, logistics, manufacturing, or record-keeping, ODOO provides an excellent platform for learning. Its modular structure and tutorials make it easy to understand real-life business activities in a hands-on manner.&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&quot;why-am-i-using-odoo&quot;&gt;Why Am I Using ODOO?&lt;/h1&gt;
&lt;p&gt;Our company recently completed the research and development (R&amp;amp;D) phase for a minimally viable product. We are now transitioning from R&amp;amp;D to full-scale manufacturing—a phase that demands meticulous design transfer and process management.
This transition requires managing every step, including:
•	Developing work instructions, design drawings, guidelines, and checkpoints for handling components and suppliers.
•	Overseeing quality control, production, assembly, and quality assurance.
•	Testing integrated products, packaging, and shipping.
Throughout this process, every step must be traceable, reliable, and monitored to identify and resolve issues as they arise. ODOO offers the tools and flexibility to manage these complexities effectively, ensuring that our workflows are efficient and transparent.&lt;/p&gt;

&lt;h1 id=&quot;final-remarks-lessons-learned-and-challenges-ahead&quot;&gt;Final Remarks: Lessons Learned and Challenges Ahead&lt;/h1&gt;
&lt;p&gt;ODOO’s flexibility is one of its strongest assets. Over the years, the OCA community has developed and shared a wealth of modules that can be customized to meet specific needs. For example:
•	Input constraints can ensure data integrity, such as limiting certain fields to integers or validating entries using SQL or Python constraints.
•	Automated calculations reduce human error and streamline workflows.
However, the most valuable lesson I’ve learned is that the success of any ERP system, including ODOO, depends on the quality of the data and the commitment of the people involved. The saying “garbage in, garbage out” perfectly encapsulates this challenge. Even the most advanced software cannot deliver meaningful insights if the input data is inaccurate or incomplete.&lt;/p&gt;
&lt;h3 id=&quot;key-considerations-include&quot;&gt;Key considerations include:&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;Data Entry Discipline: Staff must understand the value of accurate data entry and remain committed to maintaining high standards.&lt;/li&gt;
  &lt;li&gt;Deciding What Matters: Not everything needs to be recorded. Businesses must carefully decide what data is essential and ensure it is recorded reliably.&lt;/li&gt;
  &lt;li&gt;Staff Buy-In: Continuous engagement and feedback from staff are critical to the platform’s success. Their insights and commitment make the system meaningful and powerful.
As Albert Einstein famously said:
    &lt;blockquote&gt;
      &lt;p&gt;“Not everything that can be counted counts, and not everything that counts can be counted.”&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The true power of ODOO lies in its ability to foster collaboration and insight across an organization. Its success ultimately hinges on the people who use it, their understanding of its value, and their willingness to adapt and grow with it.&lt;/p&gt;</content><author><name>Wilson Fok</name></author><category term="Others" /><summary type="html">What is ODOO? ODOO is a versatile open-source Enterprise Resource Planning (ERP) and Customer Relationship Management (CRM) software that provides a comprehensive, integrated platform for managing a wide range of business functions. Its modules cover purchasing, inventory management, manufacturing, sales, logistics, products and services, quality control, e-commerce websites, accounting, human resources, project management, marketing, CRM, internal chat room, and much more. By offering these diverse capabilities, ODOO enables businesses to consolidate their operations into a single platform, providing a holistic view of their activities. If ODOO operates as advertised, it can provide valuable insights into the health of a business. For example, it can generate instantaneous accounting statements, track sales performance, evaluate warehouse efficiency, and deliver other timely, actionable information. The paid Enterprise version offers additional support, advanced features, and access to more business modules with excellent tutorials, making it an attractive option for companies looking for a robust ERP solution. However, the Community Edition (OCA) is also highly effective for many organizations, especially small and medium-sized enterprises, even though some advanced modules may be less readily available. ODOO’s broad scope, flexibility, and free customization options make it particularly appealing to small and medium-sized businesses (SMBs) for managing their operations efficiently without significant upfront investment. A quick summary of ODOO’s capabilities and features: Community and Marketplace Active Community: ODOO has a vibrant community of developers and users that contribute to the software’s development, share best practices, and create various modules to expand its functionality. ODOO Apps Store: The official marketplace provides a variety of third-party applications that can be integrated into ODOO, allowing businesses to customize their experience further and add specific functionalities. User Experience and Interface Intuitive Design: ODOO’s user interface is designed to be user-friendly and intuitive, which can facilitate easier onboarding for new users. This aspect is critical for adoption within organizations. Mobile Accessibility: ODOO offers mobile applications, enabling users to manage operations on-the-go, which enhances flexibility and responsiveness in business operations. Automation Capabilities Workflows and Automation: ODOO allows businesses to automate repetitive tasks and workflows, which can significantly enhance efficiency and reduce manual errors. Email Marketing and CRM Automation: ODOO includes features for automating marketing campaigns and client interactions, helping businesses maintain effective customer relations without constant manual intervention. Reporting and Analytics Advanced Reporting Tools: ODOO provides robust reporting and analytics features that allow organizations to generate detailed insights into various aspects of their business, supporting better decision-making. Customizable Dashboards: Users can create customized dashboards to track key business indicators and relevant metrics at a glance. Integration Capabilities API Availability: ODOO offers RESTful APIs that empower organizations to integrate it with other software systems, facilitating seamless data exchange and improving operational coherence. Third-Party Integrations: Many businesses leverage ODOO’s ability to connect with existing software tools (like e-commerce platforms, payment gateways, etc.) to enhance their operational ecosystem. Localization and Scalability Multi-Language and Currency Support: ODOO supports multiple languages and currencies, making it suitable for international operations and businesses targeting diverse markets. Scalability: ODOO can scale with the business as it grows, adding more modules or functionalities as needed. This scalability makes it ideal for startups that anticipate growth. Training and Resources Documentation and Tutorials: ODOO offers comprehensive documentation and tutorials, which are helpful for users navigating the system for the first time and seeking to deepen their understanding. Regular Updates and Support Frequent Updates: ODOO is continually updated to incorporate new features, enhancements, and security updates, ensuring that users have access to the latest technology. Enterprise Support Options: The enterprise version provides access to dedicated support channels, which can be crucial for businesses that require quick assistance. Industry-Specific Solutions Industry-Specific Modules: ODOO offers industry-specific modules that can address the unique challenges and requirements of various sectors, such as manufacturing, retail, health care, and more. The Real Challenge: How Easy is it to Realize ODOO’s Potential? ODOO serves as a centralized platform where all business activities and information are collected, analyzed, and reported. More importantly, its advanced analytical capabilities allow businesses to forecast trends and implement strategies such as Just-In-Time (JIT) supply chain management. However, while ODOO’s potential is impressive, implementing and optimizing it for business-specific needs can be a challenging task. Success often depends on understanding the intricate details of business workflows and aligning them with ODOO’s capabilities. What I Have Learned from Using ODOO Before exploring ODOO, my understanding of financial and goods/services flows was primarily based on theoretical economic principles. However, diving into ODOO’s modules has given me a much deeper, hands-on understanding of these concepts. I followed the tutorials for all the major modules, including purchasing, inventory management, manufacturing, sales, logistics, e-commerce, accounting, HR, project management, and CRM. By cross-referencing the entries I created in these modules, I gained insights into the natural connections and the cascading flow of information across functional departments. This exercise was enlightening and gave me a practical perspective on how businesses operate holistically. To experiment, I created dummy data for a hypothetical manufacturing process. I defined some items as purchasable while others were assembled in-house. Through this process, I learned how production records are created and how principles of manufacturing are reflected in ODOO. This exercise demonstrated the goods and services flow, while the accompanying accounting entries—such as purchase orders, sales invoices, bills, and receipts—illustrated the money flow. Although I am not an expert in accounting, ODOO helped me understand basic concepts such as current assets, revenues, expenses, accounts receivable, and accounts payable. By experimenting with fake sales orders, receipts, and refunds, I learned how physical activities and financial records interconnect. For example: • When refunds are issued, revenue and expenses must be adjusted accordingly, while the returned goods are recorded back into inventory or are sent to scrap at the warehouse. • Reconciliation plays a crucial role in ensuring that all numbers align accurately. ODOO’s ability to simulate processes with dummy data makes it an excellent tool for learning and experimentation. Who Would Benefit from ODOO? I recommend ODOO to: Small and Medium-Sized Businesses (SMBs): ODOO is cost-effective, and even its Enterprise version is reasonably priced. Its flexibility and comprehensive feature set can help SMBs streamline their operations without investing in expensive proprietary solutions. Beginners in Business Operations: For those unfamiliar with accounting, logistics, manufacturing, or record-keeping, ODOO provides an excellent platform for learning. Its modular structure and tutorials make it easy to understand real-life business activities in a hands-on manner. Why Am I Using ODOO? Our company recently completed the research and development (R&amp;amp;D) phase for a minimally viable product. We are now transitioning from R&amp;amp;D to full-scale manufacturing—a phase that demands meticulous design transfer and process management. This transition requires managing every step, including: • Developing work instructions, design drawings, guidelines, and checkpoints for handling components and suppliers. • Overseeing quality control, production, assembly, and quality assurance. • Testing integrated products, packaging, and shipping. Throughout this process, every step must be traceable, reliable, and monitored to identify and resolve issues as they arise. ODOO offers the tools and flexibility to manage these complexities effectively, ensuring that our workflows are efficient and transparent. Final Remarks: Lessons Learned and Challenges Ahead ODOO’s flexibility is one of its strongest assets. Over the years, the OCA community has developed and shared a wealth of modules that can be customized to meet specific needs. For example: • Input constraints can ensure data integrity, such as limiting certain fields to integers or validating entries using SQL or Python constraints. • Automated calculations reduce human error and streamline workflows. However, the most valuable lesson I’ve learned is that the success of any ERP system, including ODOO, depends on the quality of the data and the commitment of the people involved. The saying “garbage in, garbage out” perfectly encapsulates this challenge. Even the most advanced software cannot deliver meaningful insights if the input data is inaccurate or incomplete. Key considerations include: Data Entry Discipline: Staff must understand the value of accurate data entry and remain committed to maintaining high standards. Deciding What Matters: Not everything needs to be recorded. Businesses must carefully decide what data is essential and ensure it is recorded reliably. Staff Buy-In: Continuous engagement and feedback from staff are critical to the platform’s success. Their insights and commitment make the system meaningful and powerful. As Albert Einstein famously said: “Not everything that can be counted counts, and not everything that counts can be counted.” The true power of ODOO lies in its ability to foster collaboration and insight across an organization. Its success ultimately hinges on the people who use it, their understanding of its value, and their willingness to adapt and grow with it.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="/assets/images/ODOO/odoo.png" /><media:content medium="image" url="/assets/images/ODOO/odoo.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">都是选择</title><link href="/toastmasters/2024/12/03/humorous_contest/" rel="alternate" type="text/html" title="都是选择" /><published>2024-12-03T00:00:00+08:00</published><updated>2024-12-03T00:00:00+08:00</updated><id>/toastmasters/2024/12/03/humorous_contest</id><content type="html" xml:base="/toastmasters/2024/12/03/humorous_contest/">&lt;h1 id=&quot;title-都是选择&quot;&gt;Title: 都是选择&lt;/h1&gt;

&lt;p&gt;各位来宾，参赛者，评委，大家好，&lt;/p&gt;

&lt;p&gt;请大家举手，我接下来我会描述我身边的朋友，如果你也有这样的朋友，请继续举手，否则把手放下。&lt;/p&gt;

&lt;p&gt;我身边的朋友都热爱工作， 在岗位上，全心投入，充满热情。回家后，和深爱着他们的灵魂伴侣，爱她就像爱自己一样, 看不一样的夜空。&lt;/p&gt;

&lt;p&gt;我当然没有，我在说笑话。。。
现实是，他们越装着卖力地工作， 老板们越装着大方地给工资，摸鱼只骗到一点点钱。回家后，和一个不爱的人，守着一段不完整的婚姻。他们的态度是。。。。消极地为别人而活。我发现——奴才。&lt;/p&gt;

&lt;p&gt;在校招时，同学们大排长龙，争相恐后，卖身给大私企，大国企，大央企。他们的态度是。。。积极向上。大城市里工作不单提供温饱，还可以满足同学们对户口，对象， 孩子教育， 买房，买车等高层次的理想。为了理想， 可以不恋爱，不结婚， 或者是不恋爱，只结婚。在这些全国顶尖的高校里，我发现——人才。&lt;/p&gt;

&lt;p&gt;为了避免和人才们卷，我发誓，我逃离他们喜欢的城市，避开他们喜欢的企业，只追他们不喜欢的女人。&lt;/p&gt;

&lt;p&gt;话说回来，任何一家公司想成功，只有奴才，没人才，註定要失敗。
我的组长，北大博士，在六十秒， 不是六十天内把握合作方的痛点和新功能的要点
并他指导我们他现有的成果。他用高效的管理，就是没有管我们怎样解决问题。
当问题解决后，他把自己的成果忘了，只记得别人的成果。有些人拿了别人的成果，来成就自己， 有些人忘了自己的成果，来成就别人。他们的态度是。。。。寻找共赢。我发现——奇才。奇才们容易被人嫉妒，不被喜欢。如果你们身边有女奇才，欢迎介绍给我。&lt;/p&gt;

&lt;p&gt;讲过同学们，同事们，同龄人，现在轮到自己。&lt;/p&gt;

&lt;p&gt;大家对没工作，就没钱，而焦虑，
我是对没工作，没机会被剥削，而不甘心。
找不找到工作固然跟个人有关，但是跟经济周期，大环境，宏观经济关系更密切。宏观经济听起来很虚，就像男朋友说我爱你。宏观经济容易被误解，就像那一刻心动，只是情绪的波动。但是宏观经济是真实的，就像分手后只能把心痛当拥有。&lt;/p&gt;

&lt;p&gt;将宏观经济比如为风的雷军说：在风口上，猪也能飞起来。有些人是先知先觉， 差一点的是后知后觉，我呢？ 。。。是不知不觉。虽为庸才，我是屡败屡试。 因此，找工作，不困惑，懂上落，有着落。&lt;/p&gt;

&lt;p&gt;总的来说, 一个人做人做事的态度，决定他一生的高度。人才，奇才，庸才，奴才都是选择。&lt;/p&gt;

&lt;h1 id=&quot;previous-drafts&quot;&gt;Previous Drafts&lt;/h1&gt;

&lt;p&gt;《三才》&lt;/p&gt;

&lt;p&gt;各位来宾，参赛者，评委，大家好，&lt;/p&gt;

&lt;p&gt;我喜欢看书，我今天带好多书来，但是我一本都没有看过。 因为读万卷书，不如行，万里路，今天我和大家分享我在我的人生路上见过的三种人。&lt;/p&gt;

&lt;p&gt;我在一家北京初创公司里当一名程序员。因为它刚刚融资不久，所以拼命地扩张。 那是18年，我经历我人生的第一次， 不是你们想象的那种。第一次体验北飘，第一次走进故宫，第一次爬长城，第一次见到满条大街的国企和央企。&lt;/p&gt;

&lt;p&gt;公司需要招聘应届毕业生，所以我被安排去学校帮忙面试他们。学校里人山人海，大家都忙着找工作和出路，但是我们的摊位门可罗雀， 没有学生来投简历。我很困惑。所以我出去看了一下。原来大家大排张龙去在忙着投大私企，大国企，大央企。 工作不仅是为了温饱，温是维持供暖，饱是要吃花卷。 除了温饱以外，北京的工作还可以满足同学们户口，对象， 孩子教育， 买房等高层次的理想。在这些全国顶尖的高校里，我发现——人才。只有人才能卷起来。&lt;/p&gt;

&lt;p&gt;如果一家初创公司要成功的话，只有人力，没人才是不行的。我的组长是很有能力。比如说，当时我们组要帮合作方添加一个新的软件功能，因为软件做不到他们需要的结果， 他能在短时间内把握合作方的痛点和新功能的要点，并把他现有的成果指导我们，同时他明白这些对解决问题没有直接帮助，他用高效的管理，就是没有管我们怎样解决问题。当我解决问题后找他时，他说你写的这个新功能挺好，我回答说这是在你原有的成果上添加， 他说我都忘了， 他很会表扬人。尽管他读了很多书，又是北大博士，书没有让他成为书呆子，失去处理问题的能力。我发现——奇才，只有奇才能避免读死书。&lt;/p&gt;

&lt;p&gt;讲过同学们，讲过我的同事，是时候轮到自己。&lt;/p&gt;

&lt;p&gt;我一直以来有一种找不到工作的焦虑， 一种没有机会被剥削的不甘心。那时候我在国外念书，我在学校的兼职，暑期工都是打杂，没有一份正式的工作。每当公司招聘应届毕业生和实习生时，我都写求职信，投简历， 甚至连一些我觉得可能性不大的公司我也尝试。同时，我也参加跟我专业相关的社交活动。结果都是渺无音讯。。。。 一直等到我正式成为全职打工人之后，站在求职者的对面，我才明白个中的道理。找不找到工作固然跟个人有关，但是跟经济周期，地区和行业关系更密切。有些人是先知先觉， 比如雷军说的在风口上，猪也能飞起来，差一点的是后知后觉，我呢？ 。。。是不知不觉。过来大半生，等到猪飞起来时我拉一把后，我现在才明白这个道理——找工作，不困惑，看上落，有着落。我发现——庸才， 只有庸才会缘木求鱼&lt;/p&gt;

&lt;p&gt;三种人，三种才，三种迥然不同的人生。愿大家笑口常开，笑世间可笑之人。
谢谢&lt;/p&gt;

&lt;h1 id=&quot;2nd-draft-based-on-friends-feedback&quot;&gt;2nd draft based on friend’s feedback&lt;/h1&gt;

&lt;p&gt;《三才》&lt;/p&gt;

&lt;p&gt;各位来宾，参赛者，评委，大家好，&lt;/p&gt;

&lt;p&gt;古语说读万卷书，不如行万里路。今天我和大家分享我在我的人生路上见过的三种人。&lt;/p&gt;

&lt;p&gt;我在一家北京初创公司里当一名程序员。因为它刚刚融资不久，所以拼命地扩张。 那是18年，我经历我人生的第一次， 不是你们想象的那种。第一次体验北飘，第一次走进故宫，第一次爬长城&lt;/p&gt;

&lt;p&gt;公司需要招聘应届毕业生，所以我被安排去学校帮忙面试他们。学校里人山人海，大家都忙着找工作和出路，但是我们的摊位门可罗雀， 没有学生来投简历。我很困惑。原来同学们大排长龙在忙着投大私企，大国企，大央企。 工作不仅是为了温饱。除了温饱以外，北京的工作还可以满足同学们户口，对象， 孩子教育， 买房等高层次的理想。说到理想，我们介绍自己时从来不说，只说工作，单位。因为工作就是理想。在这些全国顶尖的高校里，我发现——人才。为了避免和人才们卷，我发誓，我离开他们喜欢的城市，避开他们喜欢的企业，只追他们不喜欢的女人。&lt;/p&gt;

&lt;p&gt;如果一家初创公司要成功，只有人力，没人才是不行的。比如说，当时我们组的合作方一个新的软件功能，我们的组长，北大博士，能在短时间内把握合作方的痛点和新功能的要点，这是六十秒和六十天的差别啊，并他指导我们他现有的成果。他用高效的管理，就是没有管我们怎样解决问题。当我解决问题后找他时，他说你写的这个新功能挺好，我回答说这是在你原有的成果上添加， 他说我都忘记了。忘记了。有些人拿了别人的成果，成就自己， 有些人忘了自己的成果，来成就别人。我发现——奇才。奇才们容易被人嫉妒，不被喜欢。如果你们身边有女奇才，欢迎介绍给我。&lt;/p&gt;

&lt;p&gt;讲过同学们，讲过同事们，是时候轮到自己。&lt;/p&gt;

&lt;p&gt;我一直以来有一种找不到工作的焦虑， 一种没有机会被剥削的不甘心。那时候我在国外念书，我在学校的兼职，暑期工都是打杂，没有一份正式的工作。每当公司招聘应届毕业生和实习生时，我都写求职信，投简历，参加跟我专业相关的社交活动。结果都是渺无音讯。。。。 一直等到我正式成为全职打工人之后，站在求职者的对面，我明白了。找不找到工作固然跟个人有关，但是跟经济周期，地区和行业关系更密切。有些人是先知先觉， 比如雷军说的在风口上，猪也能飞起来，差一点的是后知后觉，我呢？ 。。。是不知不觉。等到猪飞起来时我拉一把后，我现在才明白这个道理——找工作，不困惑，看上落，有着落。我发现——庸才。庸才爱缘木求鱼&lt;/p&gt;

&lt;p&gt;见三才，逛北京，寻自己，我做了，你呢？&lt;/p&gt;

&lt;p&gt;谢谢&lt;/p&gt;

&lt;h1 id=&quot;3rd-draft&quot;&gt;3rd draft&lt;/h1&gt;

&lt;p&gt;《我身边的人》&lt;/p&gt;

&lt;p&gt;各位来宾，参赛者，评委，大家好，&lt;/p&gt;

&lt;p&gt;古语说读万卷书，不如行万里路。今天我和大家分享我身边的三种人。&lt;/p&gt;

&lt;p&gt;公司需要招聘应届毕业生，所以我被安排去学校帮忙面试他们。学校里人山人海，大家都忙着找工作和出路，但是我们的摊位门可罗雀， 没有学生来投简历。我很困惑。原来同学们大排长龙在卖身给大私企，大国企，大央企。 工作不仅是为了温饱。除了温饱以外，北京的工作还可以满足同学们户口，对象， 孩子教育， 买房等高层次的理想。说到理想，我们介绍自己时从来不说，只说工作，单位。因为工作就是理想。在这些全国顶尖的高校里，我发现——人才。为了避免和人才们卷，我发誓，我离开他们喜欢的城市，避开他们喜欢的企业，只追他们不喜欢的女人。&lt;/p&gt;

&lt;p&gt;如果一家初创公司要成功，只有人力，没人才是不行的。比如说，当时我们组的合作方想要一个新的软件功能，我们的组长，北大博士，能在短时间内把握合作方的痛点和新功能的要点，这是六十秒和六十天的差别啊，并他指导我们他现有的成果。他用高效的管理，就是没有管我们怎样解决问题。当我解决问题后找他时，他说你写的这个新功能挺好，我回答说这是在你原有的成果上添加， 他说我都忘记了。忘记了。有些人拿了别人的成果，成就自己， 有些人忘了自己的成果，来成就别人。我发现——奇才。奇才们容易被人嫉妒，不被喜欢。如果你们身边有女奇才，欢迎介绍给我。&lt;/p&gt;

&lt;p&gt;我一直以来有一种找不到工作的焦虑， 一种没有机会被剥削的不甘心。一直都是兼职和打杂，没有一份正式的工作。每当公司招聘应届毕业生和实习生时，我都写求职信，投简历，参加跟我专业相关的社交活动。结果都是渺无音讯。。。。 一直等到我正式成为全职打工人之后，站在求职者的对面，我明白了。找不找到工作固然跟个人有关，但是跟经济周期，地区和行业关系更密切。有些人是先知先觉， 比如雷军说的在风口上，猪也能飞起来，差一点的是后知后觉，我呢？ 。。。是不知不觉。等到猪飞起来时我拉一把后，我现在才明白这个道理——找工作，不困惑，看上落，有着落。
庸才。庸才爱缘木求鱼&lt;/p&gt;

&lt;p&gt;讲过同学们，同事们，自己，最后讲同龄人。&lt;/p&gt;

&lt;p&gt;我身边的朋友都热爱工作，对生活充满热情，把生命浪费？在梦想上。？ 这是笑话&lt;/p&gt;

&lt;p&gt;他们装着用心地卖力地工作， 他们的老板装着大方给工资，摸鱼骗都一点点的工资
王小波？？？？？ 爱你就想爱生命。。。。。这是笑话&lt;/p&gt;

&lt;p&gt;我身边的朋友都。。。。。。和一个不爱的人，守着一段不完整的婚姻，
伟大的朋友们为别人而活。我发现——奴才。 。&lt;/p&gt;

&lt;p&gt;无论你是人才，奇才，庸才，还是奴才，希望大家活得精彩，缤纷多彩。谢谢&lt;/p&gt;

&lt;p&gt;《你，我，他》&lt;/p&gt;

&lt;p&gt;各位来宾，参赛者，评委，大家好，&lt;/p&gt;

&lt;p&gt;古语说读万卷书，不如行万里路。今天我和大家分享我身边4 种人： 人才，奇才，庸才，奴才&lt;/p&gt;

&lt;p&gt;公司需要招聘应届毕业生，所以我被安排去学校帮忙面试他们。学校里人山人海，大家都忙着找工作和出路，但是我们的摊位门可罗雀， 没有学生来投简历。我很困惑。原来同学们大排长龙在忙着投大私企，大国企，大央企。 工作不仅是为了温饱。除了温饱以外，北京的工作还可以满足同学们户口，对象， 孩子教育， 买房等高层次的理想。说到理想，我们介绍自己时从来不说，只说工作，单位。因为工作就是理想， 为了理想， 不恋爱，不结婚， 或者是不恋爱，只结婚。在这些全国顶尖的高校里，我发现——人才。为了避免和人才们卷，我发誓，我离开他们喜欢的城市，避开他们喜欢的企业，只追他们不喜欢的女人。&lt;/p&gt;

&lt;p&gt;如果一家初创公司要成功，只有人力，没人才是不行的。比如说，当时我们组的合作方想要一个新的软件功能，我们的组长，北大博士，能在短时间内把握合作方的痛点和新功能的要点，这是六十秒和六十天的差别啊，并他指导我们他现有的成果。他用高效的管理，就是没有管我们怎样解决问题。当我解决问题后找他时，他说你写的这个新功能挺好，我回答说这是在你原有的成果上添加， 他说我都忘记了。忘记了。有些人拿了别人的成果，成就自己， 有些人忘了自己的成果，来成就别人。我发现——奇才。奇才们容易被人嫉妒，不被喜欢。如果你们身边有女奇才，欢迎介绍给我。&lt;/p&gt;

&lt;p&gt;我一直以来有一种找不到工作的焦虑， 一种没有机会被剥削的不甘心。一直都是兼职和打杂，没有一份正式的工作。每当公司招聘应届毕业生和实习生时，我都写求职信，投简历，参加跟我专业相关的社交活动。结果都是渺无音讯。。。。 一直等到我正式成为全职打工人之后，站在求职者的对面，我明白了。找不找到工作固然跟个人有关，但是跟经济周期，地区和行业关系更密切。有些人是先知先觉， 比如雷军说的在风口上，猪也能飞起来，差一点的是后知后觉，我呢？ 。。。是不知不觉。等到猪飞起来时我拉一把后，我现在才明白这个道理——找工作，不困惑，看上落，有着落。&lt;/p&gt;

&lt;p&gt;讲过同学们，同事们，自己，最后讲同龄人。他们装着用心地卖力， 他们的老板装着大方给工资，摸鱼骗都一点点的工资，重复地做无聊的事情，可以和一个不爱的人，守着一段不完整的婚姻，逃避深度思考，喜欢故步自封，听自己想听的话。 我发现——奴才。 伟大的奴才为别人而活。&lt;/p&gt;

&lt;p&gt;无论你是人才，奇才，还是奴才，希望大家活得精彩。谢谢
《都是选择》&lt;/p&gt;

&lt;p&gt;各位来宾，参赛者，评委，大家好，&lt;/p&gt;

&lt;p&gt;古语说读万卷书，不如行万里路。今天我和大家分享我身边4 种人： 人才，奇才，庸才，奴才&lt;/p&gt;

&lt;p&gt;公司需要招聘应届毕业生，所以我被安排去学校帮忙面试他们。学校里人山人海，大家都忙着找工作和出路，但是我们的摊位门可罗雀， 没有学生来投简历。我很困惑。原来同学们大排长龙在忙着投大私企，大国企，大央企。 工作不仅是为了温饱。除了温饱以外，北京的工作还可以满足同学们户口，对象， 孩子教育， 买房等高层次的理想。说到理想，我们介绍自己时从来不说，只说工作，单位。因为工作就是理想， 为了理想， 不恋爱，不结婚， 或者是不恋爱，只结婚。在这些全国顶尖的高校里，我发现——人才。为了避免和人才们卷，我发誓，我离开他们喜欢的城市，避开他们喜欢的企业，只追他们不喜欢的女人。&lt;/p&gt;

&lt;p&gt;如果一家初创公司要成功，只有人力，没人才是不行的。比如说，当时我们组的合作方想要一个新的软件功能，我们的组长，北大博士，能在短时间内把握合作方的痛点和新功能的要点，这是六十秒和六十天的差别啊，并他指导我们他现有的成果。他用高效的管理，就是没有管我们怎样解决问题。当我解决问题后找他时，他说你写的这个新功能挺好，我回答说这是在你原有的成果上添加， 他说我都忘记了。忘记了。有些人拿了别人的成果，成就自己， 有些人忘了自己的成果，来成就别人。我发现——奇才。奇才们容易被人嫉妒，不被喜欢。如果你们身边有女奇才，欢迎介绍给我。&lt;/p&gt;

&lt;p&gt;我一直以来有一种找不到工作的焦虑， 一种没有机会被剥削的不甘心。一直都是兼职和打杂，没有一份正式的工作。每当公司招聘应届毕业生和实习生时，我都写求职信，投简历，参加跟我专业相关的社交活动。结果都是渺无音讯。。。。 一直等到我正式成为全职打工人之后，站在求职者的对面，我明白了。找不找到工作固然跟个人有关，但是跟经济周期，地区和行业关系更密切。有些人是先知先觉， 比如雷军说的在风口上，猪也能飞起来，差一点的是后知后觉，我呢？ 。。。是不知不觉。等到猪飞起来时我拉一把后，我现在才明白这个道理——找工作，不困惑，看上落，有着落。&lt;/p&gt;

&lt;p&gt;讲过同学们，同事们，自己，最后讲同龄人。他们装着用心地卖力， 他们的老板装着大方给工资，摸鱼骗都一点点的工资，重复地做无聊的事情，可以和一个不爱的人，守着一段不完整的婚姻，逃避深度思考，喜欢故步自封，听自己想听的话。 我发现——奴才。 伟大的奴才为别人而活。&lt;/p&gt;

&lt;p&gt;无论你是人才，奇才，还是奴才，希望大家活得精彩。谢谢&lt;/p&gt;</content><author><name>Wilson Fok</name></author><category term="Toastmasters" /><summary type="html">Title: 都是选择 各位来宾，参赛者，评委，大家好， 请大家举手，我接下来我会描述我身边的朋友，如果你也有这样的朋友，请继续举手，否则把手放下。 我身边的朋友都热爱工作， 在岗位上，全心投入，充满热情。回家后，和深爱着他们的灵魂伴侣，爱她就像爱自己一样, 看不一样的夜空。 我当然没有，我在说笑话。。。 现实是，他们越装着卖力地工作， 老板们越装着大方地给工资，摸鱼只骗到一点点钱。回家后，和一个不爱的人，守着一段不完整的婚姻。他们的态度是。。。。消极地为别人而活。我发现——奴才。 在校招时，同学们大排长龙，争相恐后，卖身给大私企，大国企，大央企。他们的态度是。。。积极向上。大城市里工作不单提供温饱，还可以满足同学们对户口，对象， 孩子教育， 买房，买车等高层次的理想。为了理想， 可以不恋爱，不结婚， 或者是不恋爱，只结婚。在这些全国顶尖的高校里，我发现——人才。 为了避免和人才们卷，我发誓，我逃离他们喜欢的城市，避开他们喜欢的企业，只追他们不喜欢的女人。 话说回来，任何一家公司想成功，只有奴才，没人才，註定要失敗。 我的组长，北大博士，在六十秒， 不是六十天内把握合作方的痛点和新功能的要点 并他指导我们他现有的成果。他用高效的管理，就是没有管我们怎样解决问题。 当问题解决后，他把自己的成果忘了，只记得别人的成果。有些人拿了别人的成果，来成就自己， 有些人忘了自己的成果，来成就别人。他们的态度是。。。。寻找共赢。我发现——奇才。奇才们容易被人嫉妒，不被喜欢。如果你们身边有女奇才，欢迎介绍给我。 讲过同学们，同事们，同龄人，现在轮到自己。 大家对没工作，就没钱，而焦虑， 我是对没工作，没机会被剥削，而不甘心。 找不找到工作固然跟个人有关，但是跟经济周期，大环境，宏观经济关系更密切。宏观经济听起来很虚，就像男朋友说我爱你。宏观经济容易被误解，就像那一刻心动，只是情绪的波动。但是宏观经济是真实的，就像分手后只能把心痛当拥有。 将宏观经济比如为风的雷军说：在风口上，猪也能飞起来。有些人是先知先觉， 差一点的是后知后觉，我呢？ 。。。是不知不觉。虽为庸才，我是屡败屡试。 因此，找工作，不困惑，懂上落，有着落。 总的来说, 一个人做人做事的态度，决定他一生的高度。人才，奇才，庸才，奴才都是选择。 Previous Drafts 《三才》 各位来宾，参赛者，评委，大家好， 我喜欢看书，我今天带好多书来，但是我一本都没有看过。 因为读万卷书，不如行，万里路，今天我和大家分享我在我的人生路上见过的三种人。 我在一家北京初创公司里当一名程序员。因为它刚刚融资不久，所以拼命地扩张。 那是18年，我经历我人生的第一次， 不是你们想象的那种。第一次体验北飘，第一次走进故宫，第一次爬长城，第一次见到满条大街的国企和央企。 公司需要招聘应届毕业生，所以我被安排去学校帮忙面试他们。学校里人山人海，大家都忙着找工作和出路，但是我们的摊位门可罗雀， 没有学生来投简历。我很困惑。所以我出去看了一下。原来大家大排张龙去在忙着投大私企，大国企，大央企。 工作不仅是为了温饱，温是维持供暖，饱是要吃花卷。 除了温饱以外，北京的工作还可以满足同学们户口，对象， 孩子教育， 买房等高层次的理想。在这些全国顶尖的高校里，我发现——人才。只有人才能卷起来。 如果一家初创公司要成功的话，只有人力，没人才是不行的。我的组长是很有能力。比如说，当时我们组要帮合作方添加一个新的软件功能，因为软件做不到他们需要的结果， 他能在短时间内把握合作方的痛点和新功能的要点，并把他现有的成果指导我们，同时他明白这些对解决问题没有直接帮助，他用高效的管理，就是没有管我们怎样解决问题。当我解决问题后找他时，他说你写的这个新功能挺好，我回答说这是在你原有的成果上添加， 他说我都忘了， 他很会表扬人。尽管他读了很多书，又是北大博士，书没有让他成为书呆子，失去处理问题的能力。我发现——奇才，只有奇才能避免读死书。 讲过同学们，讲过我的同事，是时候轮到自己。 我一直以来有一种找不到工作的焦虑， 一种没有机会被剥削的不甘心。那时候我在国外念书，我在学校的兼职，暑期工都是打杂，没有一份正式的工作。每当公司招聘应届毕业生和实习生时，我都写求职信，投简历， 甚至连一些我觉得可能性不大的公司我也尝试。同时，我也参加跟我专业相关的社交活动。结果都是渺无音讯。。。。 一直等到我正式成为全职打工人之后，站在求职者的对面，我才明白个中的道理。找不找到工作固然跟个人有关，但是跟经济周期，地区和行业关系更密切。有些人是先知先觉， 比如雷军说的在风口上，猪也能飞起来，差一点的是后知后觉，我呢？ 。。。是不知不觉。过来大半生，等到猪飞起来时我拉一把后，我现在才明白这个道理——找工作，不困惑，看上落，有着落。我发现——庸才， 只有庸才会缘木求鱼 三种人，三种才，三种迥然不同的人生。愿大家笑口常开，笑世间可笑之人。 谢谢 2nd draft based on friend’s feedback 《三才》 各位来宾，参赛者，评委，大家好， 古语说读万卷书，不如行万里路。今天我和大家分享我在我的人生路上见过的三种人。 我在一家北京初创公司里当一名程序员。因为它刚刚融资不久，所以拼命地扩张。 那是18年，我经历我人生的第一次， 不是你们想象的那种。第一次体验北飘，第一次走进故宫，第一次爬长城 公司需要招聘应届毕业生，所以我被安排去学校帮忙面试他们。学校里人山人海，大家都忙着找工作和出路，但是我们的摊位门可罗雀， 没有学生来投简历。我很困惑。原来同学们大排长龙在忙着投大私企，大国企，大央企。 工作不仅是为了温饱。除了温饱以外，北京的工作还可以满足同学们户口，对象， 孩子教育， 买房等高层次的理想。说到理想，我们介绍自己时从来不说，只说工作，单位。因为工作就是理想。在这些全国顶尖的高校里，我发现——人才。为了避免和人才们卷，我发誓，我离开他们喜欢的城市，避开他们喜欢的企业，只追他们不喜欢的女人。 如果一家初创公司要成功，只有人力，没人才是不行的。比如说，当时我们组的合作方一个新的软件功能，我们的组长，北大博士，能在短时间内把握合作方的痛点和新功能的要点，这是六十秒和六十天的差别啊，并他指导我们他现有的成果。他用高效的管理，就是没有管我们怎样解决问题。当我解决问题后找他时，他说你写的这个新功能挺好，我回答说这是在你原有的成果上添加， 他说我都忘记了。忘记了。有些人拿了别人的成果，成就自己， 有些人忘了自己的成果，来成就别人。我发现——奇才。奇才们容易被人嫉妒，不被喜欢。如果你们身边有女奇才，欢迎介绍给我。 讲过同学们，讲过同事们，是时候轮到自己。 我一直以来有一种找不到工作的焦虑， 一种没有机会被剥削的不甘心。那时候我在国外念书，我在学校的兼职，暑期工都是打杂，没有一份正式的工作。每当公司招聘应届毕业生和实习生时，我都写求职信，投简历，参加跟我专业相关的社交活动。结果都是渺无音讯。。。。 一直等到我正式成为全职打工人之后，站在求职者的对面，我明白了。找不找到工作固然跟个人有关，但是跟经济周期，地区和行业关系更密切。有些人是先知先觉， 比如雷军说的在风口上，猪也能飞起来，差一点的是后知后觉，我呢？ 。。。是不知不觉。等到猪飞起来时我拉一把后，我现在才明白这个道理——找工作，不困惑，看上落，有着落。我发现——庸才。庸才爱缘木求鱼 见三才，逛北京，寻自己，我做了，你呢？ 谢谢 3rd draft 《我身边的人》 各位来宾，参赛者，评委，大家好， 古语说读万卷书，不如行万里路。今天我和大家分享我身边的三种人。 公司需要招聘应届毕业生，所以我被安排去学校帮忙面试他们。学校里人山人海，大家都忙着找工作和出路，但是我们的摊位门可罗雀， 没有学生来投简历。我很困惑。原来同学们大排长龙在卖身给大私企，大国企，大央企。 工作不仅是为了温饱。除了温饱以外，北京的工作还可以满足同学们户口，对象， 孩子教育， 买房等高层次的理想。说到理想，我们介绍自己时从来不说，只说工作，单位。因为工作就是理想。在这些全国顶尖的高校里，我发现——人才。为了避免和人才们卷，我发誓，我离开他们喜欢的城市，避开他们喜欢的企业，只追他们不喜欢的女人。 如果一家初创公司要成功，只有人力，没人才是不行的。比如说，当时我们组的合作方想要一个新的软件功能，我们的组长，北大博士，能在短时间内把握合作方的痛点和新功能的要点，这是六十秒和六十天的差别啊，并他指导我们他现有的成果。他用高效的管理，就是没有管我们怎样解决问题。当我解决问题后找他时，他说你写的这个新功能挺好，我回答说这是在你原有的成果上添加， 他说我都忘记了。忘记了。有些人拿了别人的成果，成就自己， 有些人忘了自己的成果，来成就别人。我发现——奇才。奇才们容易被人嫉妒，不被喜欢。如果你们身边有女奇才，欢迎介绍给我。 我一直以来有一种找不到工作的焦虑， 一种没有机会被剥削的不甘心。一直都是兼职和打杂，没有一份正式的工作。每当公司招聘应届毕业生和实习生时，我都写求职信，投简历，参加跟我专业相关的社交活动。结果都是渺无音讯。。。。 一直等到我正式成为全职打工人之后，站在求职者的对面，我明白了。找不找到工作固然跟个人有关，但是跟经济周期，地区和行业关系更密切。有些人是先知先觉， 比如雷军说的在风口上，猪也能飞起来，差一点的是后知后觉，我呢？ 。。。是不知不觉。等到猪飞起来时我拉一把后，我现在才明白这个道理——找工作，不困惑，看上落，有着落。 庸才。庸才爱缘木求鱼 讲过同学们，同事们，自己，最后讲同龄人。 我身边的朋友都热爱工作，对生活充满热情，把生命浪费？在梦想上。？ 这是笑话 他们装着用心地卖力地工作， 他们的老板装着大方给工资，摸鱼骗都一点点的工资 王小波？？？？？ 爱你就想爱生命。。。。。这是笑话 我身边的朋友都。。。。。。和一个不爱的人，守着一段不完整的婚姻， 伟大的朋友们为别人而活。我发现——奴才。 。 无论你是人才，奇才，庸才，还是奴才，希望大家活得精彩，缤纷多彩。谢谢 《你，我，他》 各位来宾，参赛者，评委，大家好， 古语说读万卷书，不如行万里路。今天我和大家分享我身边4 种人： 人才，奇才，庸才，奴才 公司需要招聘应届毕业生，所以我被安排去学校帮忙面试他们。学校里人山人海，大家都忙着找工作和出路，但是我们的摊位门可罗雀， 没有学生来投简历。我很困惑。原来同学们大排长龙在忙着投大私企，大国企，大央企。 工作不仅是为了温饱。除了温饱以外，北京的工作还可以满足同学们户口，对象， 孩子教育， 买房等高层次的理想。说到理想，我们介绍自己时从来不说，只说工作，单位。因为工作就是理想， 为了理想， 不恋爱，不结婚， 或者是不恋爱，只结婚。在这些全国顶尖的高校里，我发现——人才。为了避免和人才们卷，我发誓，我离开他们喜欢的城市，避开他们喜欢的企业，只追他们不喜欢的女人。 如果一家初创公司要成功，只有人力，没人才是不行的。比如说，当时我们组的合作方想要一个新的软件功能，我们的组长，北大博士，能在短时间内把握合作方的痛点和新功能的要点，这是六十秒和六十天的差别啊，并他指导我们他现有的成果。他用高效的管理，就是没有管我们怎样解决问题。当我解决问题后找他时，他说你写的这个新功能挺好，我回答说这是在你原有的成果上添加， 他说我都忘记了。忘记了。有些人拿了别人的成果，成就自己， 有些人忘了自己的成果，来成就别人。我发现——奇才。奇才们容易被人嫉妒，不被喜欢。如果你们身边有女奇才，欢迎介绍给我。 我一直以来有一种找不到工作的焦虑， 一种没有机会被剥削的不甘心。一直都是兼职和打杂，没有一份正式的工作。每当公司招聘应届毕业生和实习生时，我都写求职信，投简历，参加跟我专业相关的社交活动。结果都是渺无音讯。。。。 一直等到我正式成为全职打工人之后，站在求职者的对面，我明白了。找不找到工作固然跟个人有关，但是跟经济周期，地区和行业关系更密切。有些人是先知先觉， 比如雷军说的在风口上，猪也能飞起来，差一点的是后知后觉，我呢？ 。。。是不知不觉。等到猪飞起来时我拉一把后，我现在才明白这个道理——找工作，不困惑，看上落，有着落。 讲过同学们，同事们，自己，最后讲同龄人。他们装着用心地卖力， 他们的老板装着大方给工资，摸鱼骗都一点点的工资，重复地做无聊的事情，可以和一个不爱的人，守着一段不完整的婚姻，逃避深度思考，喜欢故步自封，听自己想听的话。 我发现——奴才。 伟大的奴才为别人而活。 无论你是人才，奇才，还是奴才，希望大家活得精彩。谢谢 《都是选择》 各位来宾，参赛者，评委，大家好， 古语说读万卷书，不如行万里路。今天我和大家分享我身边4 种人： 人才，奇才，庸才，奴才 公司需要招聘应届毕业生，所以我被安排去学校帮忙面试他们。学校里人山人海，大家都忙着找工作和出路，但是我们的摊位门可罗雀， 没有学生来投简历。我很困惑。原来同学们大排长龙在忙着投大私企，大国企，大央企。 工作不仅是为了温饱。除了温饱以外，北京的工作还可以满足同学们户口，对象， 孩子教育， 买房等高层次的理想。说到理想，我们介绍自己时从来不说，只说工作，单位。因为工作就是理想， 为了理想， 不恋爱，不结婚， 或者是不恋爱，只结婚。在这些全国顶尖的高校里，我发现——人才。为了避免和人才们卷，我发誓，我离开他们喜欢的城市，避开他们喜欢的企业，只追他们不喜欢的女人。 如果一家初创公司要成功，只有人力，没人才是不行的。比如说，当时我们组的合作方想要一个新的软件功能，我们的组长，北大博士，能在短时间内把握合作方的痛点和新功能的要点，这是六十秒和六十天的差别啊，并他指导我们他现有的成果。他用高效的管理，就是没有管我们怎样解决问题。当我解决问题后找他时，他说你写的这个新功能挺好，我回答说这是在你原有的成果上添加， 他说我都忘记了。忘记了。有些人拿了别人的成果，成就自己， 有些人忘了自己的成果，来成就别人。我发现——奇才。奇才们容易被人嫉妒，不被喜欢。如果你们身边有女奇才，欢迎介绍给我。 我一直以来有一种找不到工作的焦虑， 一种没有机会被剥削的不甘心。一直都是兼职和打杂，没有一份正式的工作。每当公司招聘应届毕业生和实习生时，我都写求职信，投简历，参加跟我专业相关的社交活动。结果都是渺无音讯。。。。 一直等到我正式成为全职打工人之后，站在求职者的对面，我明白了。找不找到工作固然跟个人有关，但是跟经济周期，地区和行业关系更密切。有些人是先知先觉， 比如雷军说的在风口上，猪也能飞起来，差一点的是后知后觉，我呢？ 。。。是不知不觉。等到猪飞起来时我拉一把后，我现在才明白这个道理——找工作，不困惑，看上落，有着落。 讲过同学们，同事们，自己，最后讲同龄人。他们装着用心地卖力， 他们的老板装着大方给工资，摸鱼骗都一点点的工资，重复地做无聊的事情，可以和一个不爱的人，守着一段不完整的婚姻，逃避深度思考，喜欢故步自封，听自己想听的话。 我发现——奴才。 伟大的奴才为别人而活。 无论你是人才，奇才，还是奴才，希望大家活得精彩。谢谢</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="/assets/images/toastmasters/cover.jpg" /><media:content medium="image" url="/assets/images/toastmasters/cover.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Level One Completion</title><link href="/toastmasters/2024/11/26/levelone/" rel="alternate" type="text/html" title="Level One Completion" /><published>2024-11-26T00:00:00+08:00</published><updated>2024-11-26T00:00:00+08:00</updated><id>/toastmasters/2024/11/26/levelone</id><content type="html" xml:base="/toastmasters/2024/11/26/levelone/">&lt;h1 id=&quot;level-one-completion&quot;&gt;Level One Completion&lt;/h1&gt;
&lt;p&gt;I must congratulate myself for completing Level 1.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/toastmasters/certificateL1.png&quot; alt=&quot;&quot; width=&quot;1100px&quot; /&gt;&lt;/p&gt;</content><author><name>Wilson Fok</name></author><category term="Toastmasters" /><summary type="html">Level One Completion I must congratulate myself for completing Level 1.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="/assets/images/toastmasters/cover.jpg" /><media:content medium="image" url="/assets/images/toastmasters/cover.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">The Pace of Life</title><link href="/toastmasters/2024/11/25/feedback_NZ/" rel="alternate" type="text/html" title="The Pace of Life" /><published>2024-11-25T00:00:00+08:00</published><updated>2024-11-25T00:00:00+08:00</updated><id>/toastmasters/2024/11/25/feedback_NZ</id><content type="html" xml:base="/toastmasters/2024/11/25/feedback_NZ/">&lt;h1 id=&quot;1st-speech&quot;&gt;1st Speech&lt;/h1&gt;
&lt;h1 id=&quot;title-the-pace-of-life&quot;&gt;Title: The Pace of Life&lt;/h1&gt;

&lt;p&gt;Hello Toastmasters and guests,&lt;/p&gt;

&lt;p&gt;It is my great honor to deliver a speech during the national holidays, especially with the weather being such a boon for outdoor leisure activities.&lt;/p&gt;

&lt;p&gt;Has anyone here traveled to New Zealand for work, study, or vacation?
Have you watched The Lord of the Rings trilogy?
The movies were filmed in New Zealand by Peter Jackson, a renowned director from there.&lt;/p&gt;

&lt;p&gt;For those of you who have watched the films may have notice that New Zealand offers spectacular scenery. To know why, let me explain the geologic history of New Zealand.&lt;/p&gt;

&lt;p&gt;Location&lt;/p&gt;

&lt;p&gt;New Zealand is an island country situated in the southwestern Pacific Ocean and consists of two main islands: the North Island and the South Island. Located at the fringes of the Pacific and Indian-Australian tectonic plates, New Zealand has undergone a painful birth,&lt;/p&gt;

&lt;p&gt;In the North, the Pacific Plate is forced beneath the Indian-Australian Plate, while in the South, the Pacific Plate rubs against the Indian-Australian Plate and forces it downward. As part of the Ring of Fire that runs through Indonesia, the Philippines, and Japan in Asia, New Zealand experiences frequent geological activities, such as volcanic eruptions and earthquakes.&lt;/p&gt;

&lt;p&gt;Over time, the land has been pushed upward to form mountain ranges, and water has been trapped as snow on its summits. Today, we see the Southern Alps, which stretch over 500 kilometers with 18 peaks like a backbone across the South Island. The highest, Mount Cook, stands at 3,724 meters with snow on its summit. Lake Taupo, the largest freshwater lake in New Zealand, is about the same size as Singapore and was formed after a massive volcanic eruption.  Gradually, it has collected water to fill itself to the fullest. Nature crafts the spectacular landscapes at its own pace.&lt;/p&gt;

&lt;p&gt;My Travel Experience&lt;/p&gt;

&lt;p&gt;New Zealand is a paradise for those who love nature and the outdoors. I’d like to share some of my travel experiences there, which were incredibly relaxing and slow-paced.&lt;/p&gt;

&lt;p&gt;My first stop was at a national park. The park offers many trails, each showcasing different landscapes and plant life. Completing just one trail took me 8 hours. Despite the large number of tourists, the park was so vast that it never felt crowded. Some visitors moved quickly, seeking off-track adventures, while others took their time, savoring the scenery, especially the colorful lakes. Everyone moved at their own pace, according to their abilities and goals. Just being there in that moment was enough to qualify me as a tourist and check it off my to-do list.&lt;/p&gt;

&lt;p&gt;Next, I visited the coastline. New Zealand has more than 700 small islands, and this particular one is home to a variety of wildlife and beautiful beaches. I strolled from one beach to the next, enjoying the ocean views and sea breezes. Some beaches in New Zealand are great for surfing while others are covered with black sand formed by volcanic rocks. Along the shore, I spotted wild elephant seals basking in the sun, lying flat, and hanging out together.&lt;/p&gt;

&lt;p&gt;The Pace of Life&lt;/p&gt;

&lt;p&gt;New Zealand offers us a chance to enjoy the natural beauty of the earth. Life in New Zealand is relaxed and unhurried. My travels have taught me that it’s okay to slow down. As spiritual beings having a human experience, we need to remember that the pace of life is far slower than the fast, stressful, and artificial life many of us lead in post-industrial urban environments. Life is like a marathon. Sometimes, we need to speed up, but other times, we need to slow down. Ultimately, we must find our own pace, because we can’t hope to finish the race happily if we’re always trying to keep up with someone else.&lt;/p&gt;

&lt;p&gt;Instead of rushing by, try slowing down&lt;/p&gt;

&lt;p&gt;Instead of hurrying up, try standing still and enjoying the moment…&lt;/p&gt;

&lt;p&gt;Instead of feeling squeezed, try stepping back and finding more breathing space.&lt;/p&gt;

&lt;p&gt;Thank you for listening.&lt;/p&gt;

&lt;h1 id=&quot;2nd-speech&quot;&gt;2nd Speech&lt;/h1&gt;

&lt;h1 id=&quot;title-the-pace-of-life-1&quot;&gt;Title: The Pace of Life&lt;/h1&gt;

&lt;p&gt;Hello Toastmasters and guests,&lt;/p&gt;

&lt;p&gt;Have you traveled to New Zealand for work, study or vacation?
Raise your hand if you have.&lt;/p&gt;

&lt;p&gt;Have you watched The Lord of the Rings trilogy?
The movies were filmed in New Zealand by Peter Jackson, a renowned director from there.&lt;/p&gt;

&lt;p&gt;My Travel Experience&lt;/p&gt;

&lt;p&gt;I’d like to share some of my travel experiences there, which were incredibly relaxing and slow-paced.
New Zealand is a paradise for those who love nature and the outdoors.&lt;/p&gt;

&lt;p&gt;My first stop was at a national park. The park offers many trails, each showcasing different landscapes and plant life/ plants. Completing just one trail took me 8 hours. Although the park attracted many tourists, the park was so vast that it never felt crowded. 
My friend moved quickly, seeking off-track adventures, while I took my time, savoring the beautiful scenery, the mighty mountain and the colorful lakes. We followed our own pace, just right for our fitness and ambition.&lt;/p&gt;

&lt;p&gt;In addition to the parks, I visited the beaches. New Zealand is an island country, with more than 700 small islands. 
This particular ISLAND ! you are looking at is home to endangered birds and wildlife / plants.&lt;/p&gt;

&lt;p&gt;I strolled from one beach to the next, enjoying the ocean breezes and the simple vastness.
Some of them are good for surfing while others are covered with black sand formed by volcanic rocks.
Along the shore, I spotted some wild elephant seals !!!!! basking in the sun, lying flat, and hanging out together.&lt;/p&gt;

&lt;p&gt;Where does the beautiful scenery come from?
The scenery comes from the craft of nature.&lt;/p&gt;

&lt;p&gt;New Zealand is situated at the fringes of the Pacific and Indian Australian TECTONIC plates, so she experiences frequent and violent geological activities, such as volcanic eruptions and earthquakes. Over a long long period of time, the land has been pushed upward, forming what we now see today, the Southern Alps, which stretch over 500 kilometers with 18 peaks like a backbone across the South Island. The highest, Mount Cook, stands at 3,724 meters with snow on its summit. Formed after a massive volcanic eruption, Lake Taupo, the largest freshwater lake in New Zealand, is about the same size as Singapore.&lt;/p&gt;

&lt;p&gt;Nature takes its time to craft such magnificent landscape. Such magnificent landscape exists before I arrived in New Zealand, and it continues to exists after I left. 
But the moment I was there as a traveler, I was relaxed, happy and content, for I had enjoyed the beauty of nature at my own pace.&lt;/p&gt;

&lt;p&gt;LIFE is like that too. Economies, companies, regulations, societies, and all troubles of the world exist before we are born, and they shall continue to exist after we are long dead. But the moment we are here as a traveler, we,  must live our life to the fullest, the truest, for there is nothing more enjoyable than to express the true nature of ourselves. 
We cannot ever hope to achieve that, if we constantly struggle to catch up with someone’s pace. IT is never fair to compete against someone at their pace, right? So, why do we do it then?
We must eat, sleep, play, work and even do toastmasters in ways that fit our true nature. 
This is my advice for you.&lt;/p&gt;

&lt;p&gt;Instead of rushing by, try standing still&lt;/p&gt;

&lt;p&gt;Instead of hurrying up, try slowing down&lt;/p&gt;

&lt;p&gt;Instead of squeezing hard, try letting go&lt;/p&gt;

&lt;p&gt;Thanks&lt;/p&gt;

&lt;h1 id=&quot;beautiful-scenery&quot;&gt;Beautiful Scenery&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/toastmasters/NZ/img0.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; /&gt; 
&lt;img src=&quot;/assets/images/toastmasters/NZ/img1.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; /&gt; 
&lt;img src=&quot;/assets/images/toastmasters/NZ/img2.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; /&gt; 
&lt;img src=&quot;/assets/images/toastmasters/NZ/img3.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; /&gt; 
&lt;img src=&quot;/assets/images/toastmasters/NZ/img4.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; /&gt; 
&lt;img src=&quot;/assets/images/toastmasters/NZ/img5.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; /&gt; 
&lt;img src=&quot;/assets/images/toastmasters/NZ/img6.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; /&gt; 
&lt;img src=&quot;/assets/images/toastmasters/NZ/img7.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; /&gt; 
&lt;img src=&quot;/assets/images/toastmasters/NZ/img8.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;</content><author><name>Wilson Fok</name></author><category term="Toastmasters" /><summary type="html">1st Speech Title: The Pace of Life Hello Toastmasters and guests, It is my great honor to deliver a speech during the national holidays, especially with the weather being such a boon for outdoor leisure activities. Has anyone here traveled to New Zealand for work, study, or vacation? Have you watched The Lord of the Rings trilogy? The movies were filmed in New Zealand by Peter Jackson, a renowned director from there. For those of you who have watched the films may have notice that New Zealand offers spectacular scenery. To know why, let me explain the geologic history of New Zealand. Location New Zealand is an island country situated in the southwestern Pacific Ocean and consists of two main islands: the North Island and the South Island. Located at the fringes of the Pacific and Indian-Australian tectonic plates, New Zealand has undergone a painful birth, In the North, the Pacific Plate is forced beneath the Indian-Australian Plate, while in the South, the Pacific Plate rubs against the Indian-Australian Plate and forces it downward. As part of the Ring of Fire that runs through Indonesia, the Philippines, and Japan in Asia, New Zealand experiences frequent geological activities, such as volcanic eruptions and earthquakes. Over time, the land has been pushed upward to form mountain ranges, and water has been trapped as snow on its summits. Today, we see the Southern Alps, which stretch over 500 kilometers with 18 peaks like a backbone across the South Island. The highest, Mount Cook, stands at 3,724 meters with snow on its summit. Lake Taupo, the largest freshwater lake in New Zealand, is about the same size as Singapore and was formed after a massive volcanic eruption. Gradually, it has collected water to fill itself to the fullest. Nature crafts the spectacular landscapes at its own pace. My Travel Experience New Zealand is a paradise for those who love nature and the outdoors. I’d like to share some of my travel experiences there, which were incredibly relaxing and slow-paced. My first stop was at a national park. The park offers many trails, each showcasing different landscapes and plant life. Completing just one trail took me 8 hours. Despite the large number of tourists, the park was so vast that it never felt crowded. Some visitors moved quickly, seeking off-track adventures, while others took their time, savoring the scenery, especially the colorful lakes. Everyone moved at their own pace, according to their abilities and goals. Just being there in that moment was enough to qualify me as a tourist and check it off my to-do list. Next, I visited the coastline. New Zealand has more than 700 small islands, and this particular one is home to a variety of wildlife and beautiful beaches. I strolled from one beach to the next, enjoying the ocean views and sea breezes. Some beaches in New Zealand are great for surfing while others are covered with black sand formed by volcanic rocks. Along the shore, I spotted wild elephant seals basking in the sun, lying flat, and hanging out together. The Pace of Life New Zealand offers us a chance to enjoy the natural beauty of the earth. Life in New Zealand is relaxed and unhurried. My travels have taught me that it’s okay to slow down. As spiritual beings having a human experience, we need to remember that the pace of life is far slower than the fast, stressful, and artificial life many of us lead in post-industrial urban environments. Life is like a marathon. Sometimes, we need to speed up, but other times, we need to slow down. Ultimately, we must find our own pace, because we can’t hope to finish the race happily if we’re always trying to keep up with someone else. Instead of rushing by, try slowing down Instead of hurrying up, try standing still and enjoying the moment… Instead of feeling squeezed, try stepping back and finding more breathing space. Thank you for listening. 2nd Speech Title: The Pace of Life Hello Toastmasters and guests, Have you traveled to New Zealand for work, study or vacation? Raise your hand if you have. Have you watched The Lord of the Rings trilogy? The movies were filmed in New Zealand by Peter Jackson, a renowned director from there. My Travel Experience I’d like to share some of my travel experiences there, which were incredibly relaxing and slow-paced. New Zealand is a paradise for those who love nature and the outdoors. My first stop was at a national park. The park offers many trails, each showcasing different landscapes and plant life/ plants. Completing just one trail took me 8 hours. Although the park attracted many tourists, the park was so vast that it never felt crowded. My friend moved quickly, seeking off-track adventures, while I took my time, savoring the beautiful scenery, the mighty mountain and the colorful lakes. We followed our own pace, just right for our fitness and ambition. In addition to the parks, I visited the beaches. New Zealand is an island country, with more than 700 small islands. This particular ISLAND ! you are looking at is home to endangered birds and wildlife / plants. I strolled from one beach to the next, enjoying the ocean breezes and the simple vastness. Some of them are good for surfing while others are covered with black sand formed by volcanic rocks. Along the shore, I spotted some wild elephant seals !!!!! basking in the sun, lying flat, and hanging out together. Where does the beautiful scenery come from? The scenery comes from the craft of nature. New Zealand is situated at the fringes of the Pacific and Indian Australian TECTONIC plates, so she experiences frequent and violent geological activities, such as volcanic eruptions and earthquakes. Over a long long period of time, the land has been pushed upward, forming what we now see today, the Southern Alps, which stretch over 500 kilometers with 18 peaks like a backbone across the South Island. The highest, Mount Cook, stands at 3,724 meters with snow on its summit. Formed after a massive volcanic eruption, Lake Taupo, the largest freshwater lake in New Zealand, is about the same size as Singapore. Nature takes its time to craft such magnificent landscape. Such magnificent landscape exists before I arrived in New Zealand, and it continues to exists after I left. But the moment I was there as a traveler, I was relaxed, happy and content, for I had enjoyed the beauty of nature at my own pace. LIFE is like that too. Economies, companies, regulations, societies, and all troubles of the world exist before we are born, and they shall continue to exist after we are long dead. But the moment we are here as a traveler, we, must live our life to the fullest, the truest, for there is nothing more enjoyable than to express the true nature of ourselves. We cannot ever hope to achieve that, if we constantly struggle to catch up with someone’s pace. IT is never fair to compete against someone at their pace, right? So, why do we do it then? We must eat, sleep, play, work and even do toastmasters in ways that fit our true nature. This is my advice for you. Instead of rushing by, try standing still Instead of hurrying up, try slowing down Instead of squeezing hard, try letting go Thanks Beautiful Scenery</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="/assets/images/toastmasters/cover.jpg" /><media:content medium="image" url="/assets/images/toastmasters/cover.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>