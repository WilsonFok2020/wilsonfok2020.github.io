<!DOCTYPE html> <html lang=" en "><head> <meta charset="utf-8"> <!-- Begin Jekyll SEO tag v2.8.0 --> <title>Learning object detection part 3 - network training and evaluation | Wilson Fok</title> <meta name="generator" content="Jekyll v3.9.3" /> <meta property="og:title" content="Learning object detection part 3 - network training and evaluation" /> <meta name="author" content="Wilson Fok" /> <meta property="og:locale" content="en_US" /> <meta name="description" content="Object Detection In this context, object detection refers to the localization of the circle, rectangle and triangle on an image with the use of bounding boxes. Classification refers to identification of object colors, namely red, green and blue. RetinaNet uses resNet as the model’s backbone. I have selected a small resNet, resNet 18. The details of the implementation can be found in the code below. The outputs of the resNet are three feature maps of different spatial resolutions, denoted by the white rectangles with gray boarders on the left (see the figure from the original publication below, Focal Loss for Dense Object Detection). RetinaNet and how it works These feature maps are fed into a feature pyramid net (in green) which extends the hierarchical scales of spatial resolutions from three to five. One way to visualize the feature pyramid is to look at the green pyramid in the figure below. Note that resolution increases as we move down, and vice versa. This concept is particularly important because these scales help ensure the network can detect objects of various sizes as they appear across the scales. For example, a person may appear quite big in a portrait but tiny in a busy traffic scene as one amongst many pedestrians. FCOS model. This figure of feature pyramid diagram is far clearer than the diagram in the RetinaNet paper class PyramidFeatures(nn.Module): def __init__(self, C3_size, C4_size, C5_size, feature_size=256): super(PyramidFeatures, self).__init__() # upsample C5 to get P5 from the FPN paper self.P5_1 = nn.Conv2d(C5_size, feature_size, kernel_size=1, stride=1, padding=0) self.P5_upsampled = nn.Upsample(scale_factor=2, mode=&#39;nearest&#39;) self.P5_2 = nn.Conv2d(feature_size, feature_size, kernel_size=3, stride=1, padding=1) # add P5 elementwise to C4 self.P4_1 = nn.Conv2d(C4_size, feature_size, kernel_size=1, stride=1, padding=0) self.P4_upsampled = nn.Upsample(scale_factor=2, mode=&#39;nearest&#39;) self.P4_2 = nn.Conv2d(feature_size, feature_size, kernel_size=3, stride=1, padding=1) # add P4 elementwise to C3 self.P3_1 = nn.Conv2d(C3_size, feature_size, kernel_size=1, stride=1, padding=0) self.P3_2 = nn.Conv2d(feature_size, feature_size, kernel_size=3, stride=1, padding=1) # &quot;P6 is obtained via a 3x3 stride-2 conv on C5&quot; self.P6 = nn.Conv2d(C5_size, feature_size, kernel_size=3, stride=2, padding=1) # &quot;P7 is computed by applying ReLU followed by a 3x3 stride-2 conv on P6&quot; self.P7_1 = nn.ReLU() self.P7_2 = nn.Conv2d(feature_size, feature_size, kernel_size=3, stride=2, padding=1) def forward(self, inputs): C3, C4, C5 = inputs P5_x = self.P5_1(C5) P5_upsampled_x = self.P5_upsampled(P5_x) P5_x = self.P5_2(P5_x) P4_x = self.P4_1(C4) P4_x = P5_upsampled_x + P4_x P4_upsampled_x = self.P4_upsampled(P4_x) P4_x = self.P4_2(P4_x) P3_x = self.P3_1(C3) P3_x = P3_x + P4_upsampled_x P3_x = self.P3_2(P3_x) P6_x = self.P6(C5) P7_x = self.P7_1(P6_x) P7_x = self.P7_2(P7_x) return [P3_x, P4_x, P5_x, P6_x, P7_x] One of the most important ideas is that objects can appear anywhere. To detect them successfully, we need to scan the entire image evenly. To do so, we cover an image with pre-defined regions (anchor boxes) that break up the entire image. We can set the ratios and scales of these boxes such that they can capture objects of various sizes and shapes. The left-most diagram shows how anchor boxes systematically cover the entire image. Anchor boxes and how they work. Source: Anchor Boxes for Object Detection - MATLAB &amp; Simulink class Anchors(nn.Module): def __init__(self, pyramid_levels=None, strides=None, sizes=None, ratios=None, scales=None): super(Anchors, self).__init__() if pyramid_levels is None: self.pyramid_levels = [3, 4, 5, 6, 7] if strides is None: self.strides = [2 ** x for x in self.pyramid_levels] if sizes is None: self.sizes = [2 ** (x + 2) for x in self.pyramid_levels] if ratios is None: self.ratios = np.array([0.5, 1, 2]) if scales is None: self.scales = np.array([2**-1.0, 2**-0.5, 2 ** 0, 2 ** (1.0 / 3.0), 2 ** (2.0 / 3.0)]) print (&#39;-----Anchor boxes setting------&#39;) print (&#39;pyramid_levels {}&#39;.format(self.pyramid_levels)) print (&#39;strides {}&#39;.format(self.strides)) print (&#39;sizes {}&#39;.format(self.sizes)) print (&#39;ratios {}&#39;.format(self.ratios)) print (&#39;scales {}&#39;.format(self.scales)) self.num_anchors = len(self.ratios) * len(self.scales) print (&#39;num_anchors {}&#39;.format(self.num_anchors)) def forward(self, image): image_shape = image.shape[2:] image_shape = np.array(image_shape) image_shapes = [(image_shape + 2 ** x - 1) // (2 ** x) for x in self.pyramid_levels] # compute anchors over all pyramid levels all_anchors = np.zeros((0, 4)).astype(TORCH_DATATYPE) for idx, p in enumerate(self.pyramid_levels): anchors = generate_anchors(base_size=self.sizes[idx], ratios=self.ratios, scales=self.scales) shifted_anchors = shift(image_shapes[idx], self.strides[idx], anchors) all_anchors = np.append(all_anchors, shifted_anchors, axis=0) all_anchors = np.expand_dims(all_anchors, axis=0) all_anchors = torch.from_numpy(all_anchors.astype(TORCH_DATATYPE)) if torch.cuda.is_available(): all_anchors = all_anchors.cuda() return all_anchors def generate_anchors(base_size=16, ratios=None, scales=None): &quot;&quot;&quot; Generate anchor (reference) windows by enumerating aspect ratios X scales w.r.t. a reference window. &quot;&quot;&quot; if ratios is None: ratios = np.array([0.5, 1, 2]) if scales is None: scales = np.array([2 ** 0, 2 ** (1.0 / 3.0), 2 ** (2.0 / 3.0)]) num_anchors = len(ratios) * len(scales) # initialize output anchors anchors = np.zeros((num_anchors, 4)) # scale base_size anchors[:, 2:] = base_size * np.tile(scales, (2, len(ratios))).T # compute areas of anchors areas = anchors[:, 2] * anchors[:, 3] # correct for ratios anchors[:, 2] = np.sqrt(areas / np.repeat(ratios, len(scales))) anchors[:, 3] = anchors[:, 2] * np.repeat(ratios, len(scales)) # transform from (x_ctr, y_ctr, w, h) -&gt; (x1, y1, x2, y2) anchors[:, 0::2] -= np.tile(anchors[:, 2] * 0.5, (2, 1)).T anchors[:, 1::2] -= np.tile(anchors[:, 3] * 0.5, (2, 1)).T return anchors def compute_shape(image_shape, pyramid_levels): &quot;&quot;&quot;Compute shapes based on pyramid levels. :param image_shape: :param pyramid_levels: :return: &quot;&quot;&quot; image_shape = np.array(image_shape[:2]) image_shapes = [(image_shape + 2 ** x - 1) // (2 ** x) for x in pyramid_levels] return image_shapes def anchors_for_shape( image_shape, pyramid_levels=None, ratios=None, scales=None, strides=None, sizes=None, shapes_callback=None, ): image_shapes = compute_shape(image_shape, pyramid_levels) # compute anchors over all pyramid levels all_anchors = np.zeros((0, 4)) for idx, p in enumerate(pyramid_levels): anchors = generate_anchors(base_size=sizes[idx], ratios=ratios, scales=scales) shifted_anchors = shift(image_shapes[idx], strides[idx], anchors) all_anchors = np.append(all_anchors, shifted_anchors, axis=0) return all_anchors def shift(shape, stride, anchors): shift_x = (np.arange(0, shape[1]) + 0.5) * stride shift_y = (np.arange(0, shape[0]) + 0.5) * stride shift_x, shift_y = np.meshgrid(shift_x, shift_y) shifts = np.vstack(( shift_x.ravel(), shift_y.ravel(), shift_x.ravel(), shift_y.ravel() )).transpose() # add A anchors (1, A, 4) to # cell K shifts (K, 1, 4) to get # shift anchors (K, A, 4) # reshape to (K*A, 4) shifted anchors A = anchors.shape[0] K = shifts.shape[0] all_anchors = (anchors.reshape((1, A, 4)) + shifts.reshape((1, K, 4)).transpose((1, 0, 2))) all_anchors = all_anchors.reshape((K * A, 4)) return all_anchors In this toy example, I have tried Ratios = [0.5, 1, 2], Scales = [0.5, 0.7, 1, 1.25, 1,6], totaling 15 combinations. A ratio of 1 is good for circle and square. By imposing a minimum area in the generation of the toy dataset, I make sure the samples are easy for the network because I do not need to look at scales that are below 0.5. But, what if a circle, triangle, or rectangle does not fall in these pre-determine box sizes? The solution to this is a class + box subnet branches. The class + box subnet comprises of two independent branches. The class subnet branch (ClassificationModel) predicts the classes of the object, may it be circle, rectangle or triangle. K is the number of shapes. In this case, K is 3. A represents the number of pre-defined anchor boxes. The box subnet branch (RegressionModel)predicts how the pre-defined anchor boxes ought to be adjusted to fit the object tightly. It regresses the change needed to apply to the corners of the most relevant anchor boxes or those with a circle, triangle or rectangle inside (see diagram in the middle and far-right). Note that the class + box subnet’s weights are shared across all levels and locations. class RegressionModel(nn.Module): def __init__(self, num_features_in, num_anchors=9, feature_size=256): super(RegressionModel, self).__init__() self.conv1 = nn.Conv2d(num_features_in, feature_size, kernel_size=3, padding=1) self.act1 = nn.ReLU() self.conv2 = nn.Conv2d(feature_size, feature_size, kernel_size=3, padding=1) self.act2 = nn.ReLU() self.conv3 = nn.Conv2d(feature_size, feature_size, kernel_size=3, padding=1) self.act3 = nn.ReLU() self.conv4 = nn.Conv2d(feature_size, feature_size, kernel_size=3, padding=1) self.act4 = nn.ReLU() self.output = nn.Conv2d(feature_size, num_anchors * 4, kernel_size=3, padding=1) def forward(self, x): out = self.conv1(x) out = self.act1(out) out = self.conv2(out) out = self.act2(out) out = self.conv3(out) out = self.act3(out) out = self.conv4(out) out = self.act4(out) out = self.output(out) # out is B x C x W x H, with C = 4*num_anchors out = out.permute(0, 2, 3, 1) return out.contiguous().view(out.shape[0], -1, 4) class ClassificationModel(nn.Module): def __init__(self, num_features_in, num_anchors=9, num_classes=80, prior=0.01, feature_size=256): super(ClassificationModel, self).__init__() self.num_classes = num_classes self.num_anchors = num_anchors self.conv1 = nn.Conv2d(num_features_in, feature_size, kernel_size=3, padding=1) self.act1 = nn.ReLU() self.conv2 = nn.Conv2d(feature_size, feature_size, kernel_size=3, padding=1) self.act2 = nn.ReLU() self.conv3 = nn.Conv2d(feature_size, feature_size, kernel_size=3, padding=1) self.act3 = nn.ReLU() self.conv4 = nn.Conv2d(feature_size, feature_size, kernel_size=3, padding=1) self.act4 = nn.ReLU() self.output = nn.Conv2d(feature_size, num_anchors * num_classes, kernel_size=3, padding=1) self.output_act = nn.Sigmoid() def forward(self, x): out = self.conv1(x) out = self.act1(out) out = self.conv2(out) out = self.act2(out) out = self.conv3(out) out = self.act3(out) out = self.conv4(out) out = self.act4(out) out = self.output(out) out = self.output_act(out) # out is B x C x W x H, with C = n_classes + n_anchors out1 = out.permute(0, 2, 3, 1) batch_size, width, height, channels = out1.shape out2 = out1.view(batch_size, width, height, self.num_anchors, self.num_classes) return out2.contiguous().view(x.shape[0], -1, self.num_classes) When we interpret the network’s classification and detected bounding boxes, we need to decide which detected boxes are chosen to represent the detected object. In other words, when multiple anchor boxes overlap (measured by intersection-over-union) with each other for a single object to a greater or lesser degree, we choose the box with the highest classification scores and with good overlaps among competing detected boxes. Such situation calls for non-maximum suppression algorithm. The threshold in this model is 0.5 (standard). transformed_anchors = self.regressBoxes(anchors, regression) transformed_anchors = self.clipBoxes(transformed_anchors, img_batch) finalResult = [[], [], []] finalScores = torch.Tensor([]) finalAnchorBoxesIndexes = torch.Tensor([]).long() finalAnchorBoxesCoordinates = torch.Tensor([]) if torch.cuda.is_available(): finalScores = finalScores.cuda() finalAnchorBoxesIndexes = finalAnchorBoxesIndexes.cuda() finalAnchorBoxesCoordinates = finalAnchorBoxesCoordinates.cuda() for i in range(classification.shape[2]): scores = torch.squeeze(classification[:, :, i]) scores_over_thresh = (scores &gt; 0.05) if scores_over_thresh.sum() == 0: # no boxes to NMS, just continue continue scores = scores[scores_over_thresh] anchorBoxes = torch.squeeze(transformed_anchors) anchorBoxes = anchorBoxes[scores_over_thresh] # post-processing non-maximum suppression anchors_nms_idx = nms(anchorBoxes, scores, 0.5) finalResult[0].extend(scores[anchors_nms_idx]) finalResult[1].extend(torch.tensor([i] * anchors_nms_idx.shape[0])) finalResult[2].extend(anchorBoxes[anchors_nms_idx]) finalScores = torch.cat((finalScores, scores[anchors_nms_idx])) # control the number of predicted classes, one for each box finalAnchorBoxesIndexesValue = torch.tensor([i] * anchors_nms_idx.shape[0]) if torch.cuda.is_available(): finalAnchorBoxesIndexesValue = finalAnchorBoxesIndexesValue.cuda() finalAnchorBoxesIndexes = torch.cat((finalAnchorBoxesIndexes, finalAnchorBoxesIndexesValue)) finalAnchorBoxesCoordinates = torch.cat((finalAnchorBoxesCoordinates, anchorBoxes[anchors_nms_idx])) Below, I have draw the computational graph of the RetinaNet. However, it is far to complicated to really draw the steps taken inside the network, but I show you the graph anyway. RetinaNet TensorboardX can keep track of the computations being done throughout the network as long as the inputs and outputs are pytorch tensors. That means I have only drawn the parts from input up to right after class + box subnets and focal loss operation. Thus, I draw the computational graph by doing the followings: import os from tensorboardX import SummaryWriter from model_retinaNet import resnet18 writer = SummaryWriter(logdir=logdir, flush_secs=2) alpha = 0.25 gamma = 2.0 model = resnet18(num_classes = 3, alpha, gamma) model.eval() # not training model.cuda() # on gpu model.calculate_focalLoss = False dummy_input = torch.rand(1,3,224,224).cuda() # random input on gpu &#39;&#39;&#39; very important to note that 1) the input must be tensor 2) write can only keep track of tensor, not native python objects. this becomes problematic when the model transforms anchor boxes &#39;&#39;&#39; write.add_graph(model, input_to_model=dummy_input, verbose=True) writer.close() Focal Loss Given the size of the canvas versus the number of objects, we can tell that a vast majority of the anchor boxes do not contain anything and therefore are irrelevant. This imbalance overwhelms network learning with too many easy negative examples. One trick to make training possible is to reduce the weight of the loss on negative samples. Focal loss is one way to achieve this. The basic component of focal loss is binary cross entropy loss, but focal loss uses an extra factor (1-p) ^exponent and alpha to control the contribution of well-classified and less well-classified examples in the loss. As the gamma / exponent rises, the contribution of loss from well-classified examples shrinks. The gamma, a hyperparameter, is very crucial. I figured out that gamma = 2.0 gets the job done (others have found similar values work well too, but the value is really task-dependent. For another task, gamma = 0 or cross entropy loss performs better). Source: Focal Loss for Dense Object Detection With this knowledge, here is the implementation of a small RetinaNet for detecting circles, rectangles and triangles. class FocalLoss(nn.Module): def __init__(self, alpha, gamma): super(FocalLoss, self).__init__() self.alpha = alpha self.gamma = gamma def forward(self, classifications, regressions, anchors, annotations): batch_size = classifications.shape[0] classification_losses = [] regression_losses = [] anchor = anchors[0, :, :] anchor_widths = anchor[:, 2] - anchor[:, 0] anchor_heights = anchor[:, 3] - anchor[:, 1] anchor_ctr_x = anchor[:, 0] + 0.5 * anchor_widths anchor_ctr_y = anchor[:, 1] + 0.5 * anchor_heights for j in range(batch_size): classification = classifications[j, :, :] regression = regressions[j, :, :] bbox_annotation = annotations[j, :, :] bbox_annotation = bbox_annotation[bbox_annotation[:, 4] != -1] # prevent issues with log, cannot be too close to 0 or 1 classification = torch.clamp(classification, 1e-4, 1.0 - 1e-4) &#39;&#39;&#39; calculate focal loss &#39;&#39;&#39; &#39;&#39;&#39; special case : no object per image &#39;&#39;&#39; if bbox_annotation.shape[0] == 0: alpha_factor = torch.ones(classification.shape)* self.alpha alpha_factor = place_on_cpu_gpu(alpha_factor) alpha_factor = 1. - alpha_factor focal_weight = classification focal_weight = alpha_factor * torch.pow(focal_weight, self.gamma) bce = -(torch.log(1.0 - classification)) IoU = calc_iou(anchors[0, :, :], bbox_annotation[:, :4]) # num_anchors x num_annotations IoU_max, IoU_argmax = torch.max(IoU, dim=1) # num_anchors x 1 #import pdb #pdb.set_trace() # compute the loss for classification targets = torch.ones(classification.shape) * -1 targets = place_on_cpu_gpu(targets) # create a one-hot-like encoding by simple thresholding targets[torch.lt(IoU_max, 0.4), :] = 0 positive_indices = torch.ge(IoU_max, 0.5) num_positive_anchors = positive_indices.sum() assigned_annotations = bbox_annotation[IoU_argmax, :] targets[positive_indices, :] = 0 targets[positive_indices, assigned_annotations[positive_indices, 4].long()] = 1 alpha_factor = torch.ones(targets.shape) * self.alpha alpha_factor = place_on_cpu_gpu(alpha_factor) alpha_factor = torch.where(torch.eq(targets, 1.), alpha_factor, 1. - alpha_factor) focal_weight = torch.where(torch.eq(targets, 1.), 1. - classification, classification) focal_weight = alpha_factor * torch.pow(focal_weight, self.gamma) bce = -(targets * torch.log(classification) + (1.0 - targets) * torch.log(1.0 - classification)) # cls_loss = focal_weight * torch.pow(bce, gamma) cls_loss = focal_weight * bce zero_holder = torch.zeros(cls_loss.shape) zero_holder = place_on_cpu_gpu(zero_holder) cls_loss = torch.where(torch.ne(targets, -1.0), cls_loss, zero_holder) cls_loss = place_on_cpu_gpu(cls_loss) # avoid division by zero error classification_losses.append(cls_loss.sum()/torch.clamp(num_positive_anchors.float(), min=1.0)) # compute the loss for regression if positive_indices.sum() &gt; 0: assigned_annotations = assigned_annotations[positive_indices, :] anchor_widths_pi = anchor_widths[positive_indices] anchor_heights_pi = anchor_heights[positive_indices] anchor_ctr_x_pi = anchor_ctr_x[positive_indices] anchor_ctr_y_pi = anchor_ctr_y[positive_indices] gt_widths = assigned_annotations[:, 2] - assigned_annotations[:, 0] gt_heights = assigned_annotations[:, 3] - assigned_annotations[:, 1] gt_ctr_x = assigned_annotations[:, 0] + 0.5 * gt_widths gt_ctr_y = assigned_annotations[:, 1] + 0.5 * gt_heights # clip widths to 1 gt_widths = torch.clamp(gt_widths, min=1) gt_heights = torch.clamp(gt_heights, min=1) targets_dx = (gt_ctr_x - anchor_ctr_x_pi) / anchor_widths_pi targets_dy = (gt_ctr_y - anchor_ctr_y_pi) / anchor_heights_pi targets_dw = torch.log(gt_widths / anchor_widths_pi) targets_dh = torch.log(gt_heights / anchor_heights_pi) targets = torch.stack((targets_dx, targets_dy, targets_dw, targets_dh)) targets = targets.t() targets = place_on_cpu_gpu(targets) targets_scale = torch.Tensor([[0.1, 0.1, 0.2, 0.2]]) targets_scale = place_on_cpu_gpu(targets_scale) # negative_indices = 1 + (~positive_indices) regression_diff = torch.abs(targets - regression[positive_indices, :]) # squared error # torch where is like an if statement, if true then A otherwise B regression_loss = torch.where( torch.le(regression_diff, 1.0 / 9.0), 0.5 * 9.0 * torch.pow(regression_diff, 2), regression_diff - 0.5 / 9.0 ) regression_losses.append(regression_loss.mean()) else: regression_loss = torch.tensor(0).float() regression_loss = place_on_cpu_gpu(regression_loss) regression_losses.append(regression_loss) return torch.stack(classification_losses), torch.stack(regression_losses) Color Classification Since we are trying to also identify the color of the shapes, I construct a color classifier based on convolutional neural network with a few layers. Technically, RGB channels provide the exact color information and therefore multi-layer network is really not called for. Nonetheless, it is interesting to see that the network generalizes very well and does not show any sign of over fitting (more about this later in the result section). Below, I have draw the computational graph of the color classification network. Because the color classifier is nothing more than a few cascading blocks with a convolutional layer + batch norm + non-linearity layer (ReLU) + max pooling, the graph looks very neat. color classifier class ColorClassifier(nn.Module): def __init__(self, feature_size, num_features_in=3, num_classes=80): super(ColorClassifier, self).__init__() self.num_classes = num_classes self.act = nn.ReLU(inplace=True) self.conv1 = nn.Conv2d(num_features_in, feature_size, kernel_size=3, padding=0) self.bn1 = nn.BatchNorm2d(feature_size) self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1) self.conv2 = nn.Conv2d(feature_size, feature_size, kernel_size=3, padding=0) self.bn2 = nn.BatchNorm2d(feature_size) self.conv3 = nn.Conv2d(feature_size, feature_size, kernel_size=3, padding=0) self.bn3 = nn.BatchNorm2d(feature_size) self.output = nn.Linear(3*3*3, num_classes) def forward(self, x): out = self.conv1(x) out = self.bn1(out) out = self.act(out) out = self.maxpool(out) out = self.conv2(out) out = self.bn2(out) out = self.act(out) out = self.maxpool(out) out = self.conv3(out) out = self.bn3(out) out = self.act(out) out = self.maxpool(out) out = out.view(out.size(0), -1) out = self.output(out) return out Please feel free to play around with the code in my repository. Reference Parts of the code are adapted from the following resources: https://lilianweng.github.io/lil-log/2018/12/27/object-detection-part-4.html https://www.jeremyjordan.me/object-detection-one-stage/ https://github.com/signatrix/efficientdet http://pjreddie.com/yolo9000/ https://github.com/yhenon/pytorch-retinanet" /> <meta property="og:description" content="Object Detection In this context, object detection refers to the localization of the circle, rectangle and triangle on an image with the use of bounding boxes. Classification refers to identification of object colors, namely red, green and blue. RetinaNet uses resNet as the model’s backbone. I have selected a small resNet, resNet 18. The details of the implementation can be found in the code below. The outputs of the resNet are three feature maps of different spatial resolutions, denoted by the white rectangles with gray boarders on the left (see the figure from the original publication below, Focal Loss for Dense Object Detection). RetinaNet and how it works These feature maps are fed into a feature pyramid net (in green) which extends the hierarchical scales of spatial resolutions from three to five. One way to visualize the feature pyramid is to look at the green pyramid in the figure below. Note that resolution increases as we move down, and vice versa. This concept is particularly important because these scales help ensure the network can detect objects of various sizes as they appear across the scales. For example, a person may appear quite big in a portrait but tiny in a busy traffic scene as one amongst many pedestrians. FCOS model. This figure of feature pyramid diagram is far clearer than the diagram in the RetinaNet paper class PyramidFeatures(nn.Module): def __init__(self, C3_size, C4_size, C5_size, feature_size=256): super(PyramidFeatures, self).__init__() # upsample C5 to get P5 from the FPN paper self.P5_1 = nn.Conv2d(C5_size, feature_size, kernel_size=1, stride=1, padding=0) self.P5_upsampled = nn.Upsample(scale_factor=2, mode=&#39;nearest&#39;) self.P5_2 = nn.Conv2d(feature_size, feature_size, kernel_size=3, stride=1, padding=1) # add P5 elementwise to C4 self.P4_1 = nn.Conv2d(C4_size, feature_size, kernel_size=1, stride=1, padding=0) self.P4_upsampled = nn.Upsample(scale_factor=2, mode=&#39;nearest&#39;) self.P4_2 = nn.Conv2d(feature_size, feature_size, kernel_size=3, stride=1, padding=1) # add P4 elementwise to C3 self.P3_1 = nn.Conv2d(C3_size, feature_size, kernel_size=1, stride=1, padding=0) self.P3_2 = nn.Conv2d(feature_size, feature_size, kernel_size=3, stride=1, padding=1) # &quot;P6 is obtained via a 3x3 stride-2 conv on C5&quot; self.P6 = nn.Conv2d(C5_size, feature_size, kernel_size=3, stride=2, padding=1) # &quot;P7 is computed by applying ReLU followed by a 3x3 stride-2 conv on P6&quot; self.P7_1 = nn.ReLU() self.P7_2 = nn.Conv2d(feature_size, feature_size, kernel_size=3, stride=2, padding=1) def forward(self, inputs): C3, C4, C5 = inputs P5_x = self.P5_1(C5) P5_upsampled_x = self.P5_upsampled(P5_x) P5_x = self.P5_2(P5_x) P4_x = self.P4_1(C4) P4_x = P5_upsampled_x + P4_x P4_upsampled_x = self.P4_upsampled(P4_x) P4_x = self.P4_2(P4_x) P3_x = self.P3_1(C3) P3_x = P3_x + P4_upsampled_x P3_x = self.P3_2(P3_x) P6_x = self.P6(C5) P7_x = self.P7_1(P6_x) P7_x = self.P7_2(P7_x) return [P3_x, P4_x, P5_x, P6_x, P7_x] One of the most important ideas is that objects can appear anywhere. To detect them successfully, we need to scan the entire image evenly. To do so, we cover an image with pre-defined regions (anchor boxes) that break up the entire image. We can set the ratios and scales of these boxes such that they can capture objects of various sizes and shapes. The left-most diagram shows how anchor boxes systematically cover the entire image. Anchor boxes and how they work. Source: Anchor Boxes for Object Detection - MATLAB &amp; Simulink class Anchors(nn.Module): def __init__(self, pyramid_levels=None, strides=None, sizes=None, ratios=None, scales=None): super(Anchors, self).__init__() if pyramid_levels is None: self.pyramid_levels = [3, 4, 5, 6, 7] if strides is None: self.strides = [2 ** x for x in self.pyramid_levels] if sizes is None: self.sizes = [2 ** (x + 2) for x in self.pyramid_levels] if ratios is None: self.ratios = np.array([0.5, 1, 2]) if scales is None: self.scales = np.array([2**-1.0, 2**-0.5, 2 ** 0, 2 ** (1.0 / 3.0), 2 ** (2.0 / 3.0)]) print (&#39;-----Anchor boxes setting------&#39;) print (&#39;pyramid_levels {}&#39;.format(self.pyramid_levels)) print (&#39;strides {}&#39;.format(self.strides)) print (&#39;sizes {}&#39;.format(self.sizes)) print (&#39;ratios {}&#39;.format(self.ratios)) print (&#39;scales {}&#39;.format(self.scales)) self.num_anchors = len(self.ratios) * len(self.scales) print (&#39;num_anchors {}&#39;.format(self.num_anchors)) def forward(self, image): image_shape = image.shape[2:] image_shape = np.array(image_shape) image_shapes = [(image_shape + 2 ** x - 1) // (2 ** x) for x in self.pyramid_levels] # compute anchors over all pyramid levels all_anchors = np.zeros((0, 4)).astype(TORCH_DATATYPE) for idx, p in enumerate(self.pyramid_levels): anchors = generate_anchors(base_size=self.sizes[idx], ratios=self.ratios, scales=self.scales) shifted_anchors = shift(image_shapes[idx], self.strides[idx], anchors) all_anchors = np.append(all_anchors, shifted_anchors, axis=0) all_anchors = np.expand_dims(all_anchors, axis=0) all_anchors = torch.from_numpy(all_anchors.astype(TORCH_DATATYPE)) if torch.cuda.is_available(): all_anchors = all_anchors.cuda() return all_anchors def generate_anchors(base_size=16, ratios=None, scales=None): &quot;&quot;&quot; Generate anchor (reference) windows by enumerating aspect ratios X scales w.r.t. a reference window. &quot;&quot;&quot; if ratios is None: ratios = np.array([0.5, 1, 2]) if scales is None: scales = np.array([2 ** 0, 2 ** (1.0 / 3.0), 2 ** (2.0 / 3.0)]) num_anchors = len(ratios) * len(scales) # initialize output anchors anchors = np.zeros((num_anchors, 4)) # scale base_size anchors[:, 2:] = base_size * np.tile(scales, (2, len(ratios))).T # compute areas of anchors areas = anchors[:, 2] * anchors[:, 3] # correct for ratios anchors[:, 2] = np.sqrt(areas / np.repeat(ratios, len(scales))) anchors[:, 3] = anchors[:, 2] * np.repeat(ratios, len(scales)) # transform from (x_ctr, y_ctr, w, h) -&gt; (x1, y1, x2, y2) anchors[:, 0::2] -= np.tile(anchors[:, 2] * 0.5, (2, 1)).T anchors[:, 1::2] -= np.tile(anchors[:, 3] * 0.5, (2, 1)).T return anchors def compute_shape(image_shape, pyramid_levels): &quot;&quot;&quot;Compute shapes based on pyramid levels. :param image_shape: :param pyramid_levels: :return: &quot;&quot;&quot; image_shape = np.array(image_shape[:2]) image_shapes = [(image_shape + 2 ** x - 1) // (2 ** x) for x in pyramid_levels] return image_shapes def anchors_for_shape( image_shape, pyramid_levels=None, ratios=None, scales=None, strides=None, sizes=None, shapes_callback=None, ): image_shapes = compute_shape(image_shape, pyramid_levels) # compute anchors over all pyramid levels all_anchors = np.zeros((0, 4)) for idx, p in enumerate(pyramid_levels): anchors = generate_anchors(base_size=sizes[idx], ratios=ratios, scales=scales) shifted_anchors = shift(image_shapes[idx], strides[idx], anchors) all_anchors = np.append(all_anchors, shifted_anchors, axis=0) return all_anchors def shift(shape, stride, anchors): shift_x = (np.arange(0, shape[1]) + 0.5) * stride shift_y = (np.arange(0, shape[0]) + 0.5) * stride shift_x, shift_y = np.meshgrid(shift_x, shift_y) shifts = np.vstack(( shift_x.ravel(), shift_y.ravel(), shift_x.ravel(), shift_y.ravel() )).transpose() # add A anchors (1, A, 4) to # cell K shifts (K, 1, 4) to get # shift anchors (K, A, 4) # reshape to (K*A, 4) shifted anchors A = anchors.shape[0] K = shifts.shape[0] all_anchors = (anchors.reshape((1, A, 4)) + shifts.reshape((1, K, 4)).transpose((1, 0, 2))) all_anchors = all_anchors.reshape((K * A, 4)) return all_anchors In this toy example, I have tried Ratios = [0.5, 1, 2], Scales = [0.5, 0.7, 1, 1.25, 1,6], totaling 15 combinations. A ratio of 1 is good for circle and square. By imposing a minimum area in the generation of the toy dataset, I make sure the samples are easy for the network because I do not need to look at scales that are below 0.5. But, what if a circle, triangle, or rectangle does not fall in these pre-determine box sizes? The solution to this is a class + box subnet branches. The class + box subnet comprises of two independent branches. The class subnet branch (ClassificationModel) predicts the classes of the object, may it be circle, rectangle or triangle. K is the number of shapes. In this case, K is 3. A represents the number of pre-defined anchor boxes. The box subnet branch (RegressionModel)predicts how the pre-defined anchor boxes ought to be adjusted to fit the object tightly. It regresses the change needed to apply to the corners of the most relevant anchor boxes or those with a circle, triangle or rectangle inside (see diagram in the middle and far-right). Note that the class + box subnet’s weights are shared across all levels and locations. class RegressionModel(nn.Module): def __init__(self, num_features_in, num_anchors=9, feature_size=256): super(RegressionModel, self).__init__() self.conv1 = nn.Conv2d(num_features_in, feature_size, kernel_size=3, padding=1) self.act1 = nn.ReLU() self.conv2 = nn.Conv2d(feature_size, feature_size, kernel_size=3, padding=1) self.act2 = nn.ReLU() self.conv3 = nn.Conv2d(feature_size, feature_size, kernel_size=3, padding=1) self.act3 = nn.ReLU() self.conv4 = nn.Conv2d(feature_size, feature_size, kernel_size=3, padding=1) self.act4 = nn.ReLU() self.output = nn.Conv2d(feature_size, num_anchors * 4, kernel_size=3, padding=1) def forward(self, x): out = self.conv1(x) out = self.act1(out) out = self.conv2(out) out = self.act2(out) out = self.conv3(out) out = self.act3(out) out = self.conv4(out) out = self.act4(out) out = self.output(out) # out is B x C x W x H, with C = 4*num_anchors out = out.permute(0, 2, 3, 1) return out.contiguous().view(out.shape[0], -1, 4) class ClassificationModel(nn.Module): def __init__(self, num_features_in, num_anchors=9, num_classes=80, prior=0.01, feature_size=256): super(ClassificationModel, self).__init__() self.num_classes = num_classes self.num_anchors = num_anchors self.conv1 = nn.Conv2d(num_features_in, feature_size, kernel_size=3, padding=1) self.act1 = nn.ReLU() self.conv2 = nn.Conv2d(feature_size, feature_size, kernel_size=3, padding=1) self.act2 = nn.ReLU() self.conv3 = nn.Conv2d(feature_size, feature_size, kernel_size=3, padding=1) self.act3 = nn.ReLU() self.conv4 = nn.Conv2d(feature_size, feature_size, kernel_size=3, padding=1) self.act4 = nn.ReLU() self.output = nn.Conv2d(feature_size, num_anchors * num_classes, kernel_size=3, padding=1) self.output_act = nn.Sigmoid() def forward(self, x): out = self.conv1(x) out = self.act1(out) out = self.conv2(out) out = self.act2(out) out = self.conv3(out) out = self.act3(out) out = self.conv4(out) out = self.act4(out) out = self.output(out) out = self.output_act(out) # out is B x C x W x H, with C = n_classes + n_anchors out1 = out.permute(0, 2, 3, 1) batch_size, width, height, channels = out1.shape out2 = out1.view(batch_size, width, height, self.num_anchors, self.num_classes) return out2.contiguous().view(x.shape[0], -1, self.num_classes) When we interpret the network’s classification and detected bounding boxes, we need to decide which detected boxes are chosen to represent the detected object. In other words, when multiple anchor boxes overlap (measured by intersection-over-union) with each other for a single object to a greater or lesser degree, we choose the box with the highest classification scores and with good overlaps among competing detected boxes. Such situation calls for non-maximum suppression algorithm. The threshold in this model is 0.5 (standard). transformed_anchors = self.regressBoxes(anchors, regression) transformed_anchors = self.clipBoxes(transformed_anchors, img_batch) finalResult = [[], [], []] finalScores = torch.Tensor([]) finalAnchorBoxesIndexes = torch.Tensor([]).long() finalAnchorBoxesCoordinates = torch.Tensor([]) if torch.cuda.is_available(): finalScores = finalScores.cuda() finalAnchorBoxesIndexes = finalAnchorBoxesIndexes.cuda() finalAnchorBoxesCoordinates = finalAnchorBoxesCoordinates.cuda() for i in range(classification.shape[2]): scores = torch.squeeze(classification[:, :, i]) scores_over_thresh = (scores &gt; 0.05) if scores_over_thresh.sum() == 0: # no boxes to NMS, just continue continue scores = scores[scores_over_thresh] anchorBoxes = torch.squeeze(transformed_anchors) anchorBoxes = anchorBoxes[scores_over_thresh] # post-processing non-maximum suppression anchors_nms_idx = nms(anchorBoxes, scores, 0.5) finalResult[0].extend(scores[anchors_nms_idx]) finalResult[1].extend(torch.tensor([i] * anchors_nms_idx.shape[0])) finalResult[2].extend(anchorBoxes[anchors_nms_idx]) finalScores = torch.cat((finalScores, scores[anchors_nms_idx])) # control the number of predicted classes, one for each box finalAnchorBoxesIndexesValue = torch.tensor([i] * anchors_nms_idx.shape[0]) if torch.cuda.is_available(): finalAnchorBoxesIndexesValue = finalAnchorBoxesIndexesValue.cuda() finalAnchorBoxesIndexes = torch.cat((finalAnchorBoxesIndexes, finalAnchorBoxesIndexesValue)) finalAnchorBoxesCoordinates = torch.cat((finalAnchorBoxesCoordinates, anchorBoxes[anchors_nms_idx])) Below, I have draw the computational graph of the RetinaNet. However, it is far to complicated to really draw the steps taken inside the network, but I show you the graph anyway. RetinaNet TensorboardX can keep track of the computations being done throughout the network as long as the inputs and outputs are pytorch tensors. That means I have only drawn the parts from input up to right after class + box subnets and focal loss operation. Thus, I draw the computational graph by doing the followings: import os from tensorboardX import SummaryWriter from model_retinaNet import resnet18 writer = SummaryWriter(logdir=logdir, flush_secs=2) alpha = 0.25 gamma = 2.0 model = resnet18(num_classes = 3, alpha, gamma) model.eval() # not training model.cuda() # on gpu model.calculate_focalLoss = False dummy_input = torch.rand(1,3,224,224).cuda() # random input on gpu &#39;&#39;&#39; very important to note that 1) the input must be tensor 2) write can only keep track of tensor, not native python objects. this becomes problematic when the model transforms anchor boxes &#39;&#39;&#39; write.add_graph(model, input_to_model=dummy_input, verbose=True) writer.close() Focal Loss Given the size of the canvas versus the number of objects, we can tell that a vast majority of the anchor boxes do not contain anything and therefore are irrelevant. This imbalance overwhelms network learning with too many easy negative examples. One trick to make training possible is to reduce the weight of the loss on negative samples. Focal loss is one way to achieve this. The basic component of focal loss is binary cross entropy loss, but focal loss uses an extra factor (1-p) ^exponent and alpha to control the contribution of well-classified and less well-classified examples in the loss. As the gamma / exponent rises, the contribution of loss from well-classified examples shrinks. The gamma, a hyperparameter, is very crucial. I figured out that gamma = 2.0 gets the job done (others have found similar values work well too, but the value is really task-dependent. For another task, gamma = 0 or cross entropy loss performs better). Source: Focal Loss for Dense Object Detection With this knowledge, here is the implementation of a small RetinaNet for detecting circles, rectangles and triangles. class FocalLoss(nn.Module): def __init__(self, alpha, gamma): super(FocalLoss, self).__init__() self.alpha = alpha self.gamma = gamma def forward(self, classifications, regressions, anchors, annotations): batch_size = classifications.shape[0] classification_losses = [] regression_losses = [] anchor = anchors[0, :, :] anchor_widths = anchor[:, 2] - anchor[:, 0] anchor_heights = anchor[:, 3] - anchor[:, 1] anchor_ctr_x = anchor[:, 0] + 0.5 * anchor_widths anchor_ctr_y = anchor[:, 1] + 0.5 * anchor_heights for j in range(batch_size): classification = classifications[j, :, :] regression = regressions[j, :, :] bbox_annotation = annotations[j, :, :] bbox_annotation = bbox_annotation[bbox_annotation[:, 4] != -1] # prevent issues with log, cannot be too close to 0 or 1 classification = torch.clamp(classification, 1e-4, 1.0 - 1e-4) &#39;&#39;&#39; calculate focal loss &#39;&#39;&#39; &#39;&#39;&#39; special case : no object per image &#39;&#39;&#39; if bbox_annotation.shape[0] == 0: alpha_factor = torch.ones(classification.shape)* self.alpha alpha_factor = place_on_cpu_gpu(alpha_factor) alpha_factor = 1. - alpha_factor focal_weight = classification focal_weight = alpha_factor * torch.pow(focal_weight, self.gamma) bce = -(torch.log(1.0 - classification)) IoU = calc_iou(anchors[0, :, :], bbox_annotation[:, :4]) # num_anchors x num_annotations IoU_max, IoU_argmax = torch.max(IoU, dim=1) # num_anchors x 1 #import pdb #pdb.set_trace() # compute the loss for classification targets = torch.ones(classification.shape) * -1 targets = place_on_cpu_gpu(targets) # create a one-hot-like encoding by simple thresholding targets[torch.lt(IoU_max, 0.4), :] = 0 positive_indices = torch.ge(IoU_max, 0.5) num_positive_anchors = positive_indices.sum() assigned_annotations = bbox_annotation[IoU_argmax, :] targets[positive_indices, :] = 0 targets[positive_indices, assigned_annotations[positive_indices, 4].long()] = 1 alpha_factor = torch.ones(targets.shape) * self.alpha alpha_factor = place_on_cpu_gpu(alpha_factor) alpha_factor = torch.where(torch.eq(targets, 1.), alpha_factor, 1. - alpha_factor) focal_weight = torch.where(torch.eq(targets, 1.), 1. - classification, classification) focal_weight = alpha_factor * torch.pow(focal_weight, self.gamma) bce = -(targets * torch.log(classification) + (1.0 - targets) * torch.log(1.0 - classification)) # cls_loss = focal_weight * torch.pow(bce, gamma) cls_loss = focal_weight * bce zero_holder = torch.zeros(cls_loss.shape) zero_holder = place_on_cpu_gpu(zero_holder) cls_loss = torch.where(torch.ne(targets, -1.0), cls_loss, zero_holder) cls_loss = place_on_cpu_gpu(cls_loss) # avoid division by zero error classification_losses.append(cls_loss.sum()/torch.clamp(num_positive_anchors.float(), min=1.0)) # compute the loss for regression if positive_indices.sum() &gt; 0: assigned_annotations = assigned_annotations[positive_indices, :] anchor_widths_pi = anchor_widths[positive_indices] anchor_heights_pi = anchor_heights[positive_indices] anchor_ctr_x_pi = anchor_ctr_x[positive_indices] anchor_ctr_y_pi = anchor_ctr_y[positive_indices] gt_widths = assigned_annotations[:, 2] - assigned_annotations[:, 0] gt_heights = assigned_annotations[:, 3] - assigned_annotations[:, 1] gt_ctr_x = assigned_annotations[:, 0] + 0.5 * gt_widths gt_ctr_y = assigned_annotations[:, 1] + 0.5 * gt_heights # clip widths to 1 gt_widths = torch.clamp(gt_widths, min=1) gt_heights = torch.clamp(gt_heights, min=1) targets_dx = (gt_ctr_x - anchor_ctr_x_pi) / anchor_widths_pi targets_dy = (gt_ctr_y - anchor_ctr_y_pi) / anchor_heights_pi targets_dw = torch.log(gt_widths / anchor_widths_pi) targets_dh = torch.log(gt_heights / anchor_heights_pi) targets = torch.stack((targets_dx, targets_dy, targets_dw, targets_dh)) targets = targets.t() targets = place_on_cpu_gpu(targets) targets_scale = torch.Tensor([[0.1, 0.1, 0.2, 0.2]]) targets_scale = place_on_cpu_gpu(targets_scale) # negative_indices = 1 + (~positive_indices) regression_diff = torch.abs(targets - regression[positive_indices, :]) # squared error # torch where is like an if statement, if true then A otherwise B regression_loss = torch.where( torch.le(regression_diff, 1.0 / 9.0), 0.5 * 9.0 * torch.pow(regression_diff, 2), regression_diff - 0.5 / 9.0 ) regression_losses.append(regression_loss.mean()) else: regression_loss = torch.tensor(0).float() regression_loss = place_on_cpu_gpu(regression_loss) regression_losses.append(regression_loss) return torch.stack(classification_losses), torch.stack(regression_losses) Color Classification Since we are trying to also identify the color of the shapes, I construct a color classifier based on convolutional neural network with a few layers. Technically, RGB channels provide the exact color information and therefore multi-layer network is really not called for. Nonetheless, it is interesting to see that the network generalizes very well and does not show any sign of over fitting (more about this later in the result section). Below, I have draw the computational graph of the color classification network. Because the color classifier is nothing more than a few cascading blocks with a convolutional layer + batch norm + non-linearity layer (ReLU) + max pooling, the graph looks very neat. color classifier class ColorClassifier(nn.Module): def __init__(self, feature_size, num_features_in=3, num_classes=80): super(ColorClassifier, self).__init__() self.num_classes = num_classes self.act = nn.ReLU(inplace=True) self.conv1 = nn.Conv2d(num_features_in, feature_size, kernel_size=3, padding=0) self.bn1 = nn.BatchNorm2d(feature_size) self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1) self.conv2 = nn.Conv2d(feature_size, feature_size, kernel_size=3, padding=0) self.bn2 = nn.BatchNorm2d(feature_size) self.conv3 = nn.Conv2d(feature_size, feature_size, kernel_size=3, padding=0) self.bn3 = nn.BatchNorm2d(feature_size) self.output = nn.Linear(3*3*3, num_classes) def forward(self, x): out = self.conv1(x) out = self.bn1(out) out = self.act(out) out = self.maxpool(out) out = self.conv2(out) out = self.bn2(out) out = self.act(out) out = self.maxpool(out) out = self.conv3(out) out = self.bn3(out) out = self.act(out) out = self.maxpool(out) out = out.view(out.size(0), -1) out = self.output(out) return out Please feel free to play around with the code in my repository. Reference Parts of the code are adapted from the following resources: https://lilianweng.github.io/lil-log/2018/12/27/object-detection-part-4.html https://www.jeremyjordan.me/object-detection-one-stage/ https://github.com/signatrix/efficientdet http://pjreddie.com/yolo9000/ https://github.com/yhenon/pytorch-retinanet" /> <link rel="canonical" href="/deep_learning/2020/08/02/Learning_Object_Detection_part3/" /> <meta property="og:url" content="/deep_learning/2020/08/02/Learning_Object_Detection_part3/" /> <meta property="og:site_name" content="Wilson Fok" /> <meta property="og:image" content="/assets/images/toy/retinaNet.png" /> <meta property="og:type" content="article" /> <meta property="article:published_time" content="2020-08-02T00:00:00+08:00" /> <meta name="twitter:card" content="summary_large_image" /> <meta property="twitter:image" content="/assets/images/toy/retinaNet.png" /> <meta property="twitter:title" content="Learning object detection part 3 - network training and evaluation" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Wilson Fok"},"dateModified":"2020-08-02T00:00:00+08:00","datePublished":"2020-08-02T00:00:00+08:00","description":"Object Detection In this context, object detection refers to the localization of the circle, rectangle and triangle on an image with the use of bounding boxes. Classification refers to identification of object colors, namely red, green and blue. RetinaNet uses resNet as the model’s backbone. I have selected a small resNet, resNet 18. The details of the implementation can be found in the code below. The outputs of the resNet are three feature maps of different spatial resolutions, denoted by the white rectangles with gray boarders on the left (see the figure from the original publication below, Focal Loss for Dense Object Detection). RetinaNet and how it works These feature maps are fed into a feature pyramid net (in green) which extends the hierarchical scales of spatial resolutions from three to five. One way to visualize the feature pyramid is to look at the green pyramid in the figure below. Note that resolution increases as we move down, and vice versa. This concept is particularly important because these scales help ensure the network can detect objects of various sizes as they appear across the scales. For example, a person may appear quite big in a portrait but tiny in a busy traffic scene as one amongst many pedestrians. FCOS model. This figure of feature pyramid diagram is far clearer than the diagram in the RetinaNet paper class PyramidFeatures(nn.Module): def __init__(self, C3_size, C4_size, C5_size, feature_size=256): super(PyramidFeatures, self).__init__() # upsample C5 to get P5 from the FPN paper self.P5_1 = nn.Conv2d(C5_size, feature_size, kernel_size=1, stride=1, padding=0) self.P5_upsampled = nn.Upsample(scale_factor=2, mode=&#39;nearest&#39;) self.P5_2 = nn.Conv2d(feature_size, feature_size, kernel_size=3, stride=1, padding=1) # add P5 elementwise to C4 self.P4_1 = nn.Conv2d(C4_size, feature_size, kernel_size=1, stride=1, padding=0) self.P4_upsampled = nn.Upsample(scale_factor=2, mode=&#39;nearest&#39;) self.P4_2 = nn.Conv2d(feature_size, feature_size, kernel_size=3, stride=1, padding=1) # add P4 elementwise to C3 self.P3_1 = nn.Conv2d(C3_size, feature_size, kernel_size=1, stride=1, padding=0) self.P3_2 = nn.Conv2d(feature_size, feature_size, kernel_size=3, stride=1, padding=1) # &quot;P6 is obtained via a 3x3 stride-2 conv on C5&quot; self.P6 = nn.Conv2d(C5_size, feature_size, kernel_size=3, stride=2, padding=1) # &quot;P7 is computed by applying ReLU followed by a 3x3 stride-2 conv on P6&quot; self.P7_1 = nn.ReLU() self.P7_2 = nn.Conv2d(feature_size, feature_size, kernel_size=3, stride=2, padding=1) def forward(self, inputs): C3, C4, C5 = inputs P5_x = self.P5_1(C5) P5_upsampled_x = self.P5_upsampled(P5_x) P5_x = self.P5_2(P5_x) P4_x = self.P4_1(C4) P4_x = P5_upsampled_x + P4_x P4_upsampled_x = self.P4_upsampled(P4_x) P4_x = self.P4_2(P4_x) P3_x = self.P3_1(C3) P3_x = P3_x + P4_upsampled_x P3_x = self.P3_2(P3_x) P6_x = self.P6(C5) P7_x = self.P7_1(P6_x) P7_x = self.P7_2(P7_x) return [P3_x, P4_x, P5_x, P6_x, P7_x] One of the most important ideas is that objects can appear anywhere. To detect them successfully, we need to scan the entire image evenly. To do so, we cover an image with pre-defined regions (anchor boxes) that break up the entire image. We can set the ratios and scales of these boxes such that they can capture objects of various sizes and shapes. The left-most diagram shows how anchor boxes systematically cover the entire image. Anchor boxes and how they work. Source: Anchor Boxes for Object Detection - MATLAB &amp; Simulink class Anchors(nn.Module): def __init__(self, pyramid_levels=None, strides=None, sizes=None, ratios=None, scales=None): super(Anchors, self).__init__() if pyramid_levels is None: self.pyramid_levels = [3, 4, 5, 6, 7] if strides is None: self.strides = [2 ** x for x in self.pyramid_levels] if sizes is None: self.sizes = [2 ** (x + 2) for x in self.pyramid_levels] if ratios is None: self.ratios = np.array([0.5, 1, 2]) if scales is None: self.scales = np.array([2**-1.0, 2**-0.5, 2 ** 0, 2 ** (1.0 / 3.0), 2 ** (2.0 / 3.0)]) print (&#39;-----Anchor boxes setting------&#39;) print (&#39;pyramid_levels {}&#39;.format(self.pyramid_levels)) print (&#39;strides {}&#39;.format(self.strides)) print (&#39;sizes {}&#39;.format(self.sizes)) print (&#39;ratios {}&#39;.format(self.ratios)) print (&#39;scales {}&#39;.format(self.scales)) self.num_anchors = len(self.ratios) * len(self.scales) print (&#39;num_anchors {}&#39;.format(self.num_anchors)) def forward(self, image): image_shape = image.shape[2:] image_shape = np.array(image_shape) image_shapes = [(image_shape + 2 ** x - 1) // (2 ** x) for x in self.pyramid_levels] # compute anchors over all pyramid levels all_anchors = np.zeros((0, 4)).astype(TORCH_DATATYPE) for idx, p in enumerate(self.pyramid_levels): anchors = generate_anchors(base_size=self.sizes[idx], ratios=self.ratios, scales=self.scales) shifted_anchors = shift(image_shapes[idx], self.strides[idx], anchors) all_anchors = np.append(all_anchors, shifted_anchors, axis=0) all_anchors = np.expand_dims(all_anchors, axis=0) all_anchors = torch.from_numpy(all_anchors.astype(TORCH_DATATYPE)) if torch.cuda.is_available(): all_anchors = all_anchors.cuda() return all_anchors def generate_anchors(base_size=16, ratios=None, scales=None): &quot;&quot;&quot; Generate anchor (reference) windows by enumerating aspect ratios X scales w.r.t. a reference window. &quot;&quot;&quot; if ratios is None: ratios = np.array([0.5, 1, 2]) if scales is None: scales = np.array([2 ** 0, 2 ** (1.0 / 3.0), 2 ** (2.0 / 3.0)]) num_anchors = len(ratios) * len(scales) # initialize output anchors anchors = np.zeros((num_anchors, 4)) # scale base_size anchors[:, 2:] = base_size * np.tile(scales, (2, len(ratios))).T # compute areas of anchors areas = anchors[:, 2] * anchors[:, 3] # correct for ratios anchors[:, 2] = np.sqrt(areas / np.repeat(ratios, len(scales))) anchors[:, 3] = anchors[:, 2] * np.repeat(ratios, len(scales)) # transform from (x_ctr, y_ctr, w, h) -&gt; (x1, y1, x2, y2) anchors[:, 0::2] -= np.tile(anchors[:, 2] * 0.5, (2, 1)).T anchors[:, 1::2] -= np.tile(anchors[:, 3] * 0.5, (2, 1)).T return anchors def compute_shape(image_shape, pyramid_levels): &quot;&quot;&quot;Compute shapes based on pyramid levels. :param image_shape: :param pyramid_levels: :return: &quot;&quot;&quot; image_shape = np.array(image_shape[:2]) image_shapes = [(image_shape + 2 ** x - 1) // (2 ** x) for x in pyramid_levels] return image_shapes def anchors_for_shape( image_shape, pyramid_levels=None, ratios=None, scales=None, strides=None, sizes=None, shapes_callback=None, ): image_shapes = compute_shape(image_shape, pyramid_levels) # compute anchors over all pyramid levels all_anchors = np.zeros((0, 4)) for idx, p in enumerate(pyramid_levels): anchors = generate_anchors(base_size=sizes[idx], ratios=ratios, scales=scales) shifted_anchors = shift(image_shapes[idx], strides[idx], anchors) all_anchors = np.append(all_anchors, shifted_anchors, axis=0) return all_anchors def shift(shape, stride, anchors): shift_x = (np.arange(0, shape[1]) + 0.5) * stride shift_y = (np.arange(0, shape[0]) + 0.5) * stride shift_x, shift_y = np.meshgrid(shift_x, shift_y) shifts = np.vstack(( shift_x.ravel(), shift_y.ravel(), shift_x.ravel(), shift_y.ravel() )).transpose() # add A anchors (1, A, 4) to # cell K shifts (K, 1, 4) to get # shift anchors (K, A, 4) # reshape to (K*A, 4) shifted anchors A = anchors.shape[0] K = shifts.shape[0] all_anchors = (anchors.reshape((1, A, 4)) + shifts.reshape((1, K, 4)).transpose((1, 0, 2))) all_anchors = all_anchors.reshape((K * A, 4)) return all_anchors In this toy example, I have tried Ratios = [0.5, 1, 2], Scales = [0.5, 0.7, 1, 1.25, 1,6], totaling 15 combinations. A ratio of 1 is good for circle and square. By imposing a minimum area in the generation of the toy dataset, I make sure the samples are easy for the network because I do not need to look at scales that are below 0.5. But, what if a circle, triangle, or rectangle does not fall in these pre-determine box sizes? The solution to this is a class + box subnet branches. The class + box subnet comprises of two independent branches. The class subnet branch (ClassificationModel) predicts the classes of the object, may it be circle, rectangle or triangle. K is the number of shapes. In this case, K is 3. A represents the number of pre-defined anchor boxes. The box subnet branch (RegressionModel)predicts how the pre-defined anchor boxes ought to be adjusted to fit the object tightly. It regresses the change needed to apply to the corners of the most relevant anchor boxes or those with a circle, triangle or rectangle inside (see diagram in the middle and far-right). Note that the class + box subnet’s weights are shared across all levels and locations. class RegressionModel(nn.Module): def __init__(self, num_features_in, num_anchors=9, feature_size=256): super(RegressionModel, self).__init__() self.conv1 = nn.Conv2d(num_features_in, feature_size, kernel_size=3, padding=1) self.act1 = nn.ReLU() self.conv2 = nn.Conv2d(feature_size, feature_size, kernel_size=3, padding=1) self.act2 = nn.ReLU() self.conv3 = nn.Conv2d(feature_size, feature_size, kernel_size=3, padding=1) self.act3 = nn.ReLU() self.conv4 = nn.Conv2d(feature_size, feature_size, kernel_size=3, padding=1) self.act4 = nn.ReLU() self.output = nn.Conv2d(feature_size, num_anchors * 4, kernel_size=3, padding=1) def forward(self, x): out = self.conv1(x) out = self.act1(out) out = self.conv2(out) out = self.act2(out) out = self.conv3(out) out = self.act3(out) out = self.conv4(out) out = self.act4(out) out = self.output(out) # out is B x C x W x H, with C = 4*num_anchors out = out.permute(0, 2, 3, 1) return out.contiguous().view(out.shape[0], -1, 4) class ClassificationModel(nn.Module): def __init__(self, num_features_in, num_anchors=9, num_classes=80, prior=0.01, feature_size=256): super(ClassificationModel, self).__init__() self.num_classes = num_classes self.num_anchors = num_anchors self.conv1 = nn.Conv2d(num_features_in, feature_size, kernel_size=3, padding=1) self.act1 = nn.ReLU() self.conv2 = nn.Conv2d(feature_size, feature_size, kernel_size=3, padding=1) self.act2 = nn.ReLU() self.conv3 = nn.Conv2d(feature_size, feature_size, kernel_size=3, padding=1) self.act3 = nn.ReLU() self.conv4 = nn.Conv2d(feature_size, feature_size, kernel_size=3, padding=1) self.act4 = nn.ReLU() self.output = nn.Conv2d(feature_size, num_anchors * num_classes, kernel_size=3, padding=1) self.output_act = nn.Sigmoid() def forward(self, x): out = self.conv1(x) out = self.act1(out) out = self.conv2(out) out = self.act2(out) out = self.conv3(out) out = self.act3(out) out = self.conv4(out) out = self.act4(out) out = self.output(out) out = self.output_act(out) # out is B x C x W x H, with C = n_classes + n_anchors out1 = out.permute(0, 2, 3, 1) batch_size, width, height, channels = out1.shape out2 = out1.view(batch_size, width, height, self.num_anchors, self.num_classes) return out2.contiguous().view(x.shape[0], -1, self.num_classes) When we interpret the network’s classification and detected bounding boxes, we need to decide which detected boxes are chosen to represent the detected object. In other words, when multiple anchor boxes overlap (measured by intersection-over-union) with each other for a single object to a greater or lesser degree, we choose the box with the highest classification scores and with good overlaps among competing detected boxes. Such situation calls for non-maximum suppression algorithm. The threshold in this model is 0.5 (standard). transformed_anchors = self.regressBoxes(anchors, regression) transformed_anchors = self.clipBoxes(transformed_anchors, img_batch) finalResult = [[], [], []] finalScores = torch.Tensor([]) finalAnchorBoxesIndexes = torch.Tensor([]).long() finalAnchorBoxesCoordinates = torch.Tensor([]) if torch.cuda.is_available(): finalScores = finalScores.cuda() finalAnchorBoxesIndexes = finalAnchorBoxesIndexes.cuda() finalAnchorBoxesCoordinates = finalAnchorBoxesCoordinates.cuda() for i in range(classification.shape[2]): scores = torch.squeeze(classification[:, :, i]) scores_over_thresh = (scores &gt; 0.05) if scores_over_thresh.sum() == 0: # no boxes to NMS, just continue continue scores = scores[scores_over_thresh] anchorBoxes = torch.squeeze(transformed_anchors) anchorBoxes = anchorBoxes[scores_over_thresh] # post-processing non-maximum suppression anchors_nms_idx = nms(anchorBoxes, scores, 0.5) finalResult[0].extend(scores[anchors_nms_idx]) finalResult[1].extend(torch.tensor([i] * anchors_nms_idx.shape[0])) finalResult[2].extend(anchorBoxes[anchors_nms_idx]) finalScores = torch.cat((finalScores, scores[anchors_nms_idx])) # control the number of predicted classes, one for each box finalAnchorBoxesIndexesValue = torch.tensor([i] * anchors_nms_idx.shape[0]) if torch.cuda.is_available(): finalAnchorBoxesIndexesValue = finalAnchorBoxesIndexesValue.cuda() finalAnchorBoxesIndexes = torch.cat((finalAnchorBoxesIndexes, finalAnchorBoxesIndexesValue)) finalAnchorBoxesCoordinates = torch.cat((finalAnchorBoxesCoordinates, anchorBoxes[anchors_nms_idx])) Below, I have draw the computational graph of the RetinaNet. However, it is far to complicated to really draw the steps taken inside the network, but I show you the graph anyway. RetinaNet TensorboardX can keep track of the computations being done throughout the network as long as the inputs and outputs are pytorch tensors. That means I have only drawn the parts from input up to right after class + box subnets and focal loss operation. Thus, I draw the computational graph by doing the followings: import os from tensorboardX import SummaryWriter from model_retinaNet import resnet18 writer = SummaryWriter(logdir=logdir, flush_secs=2) alpha = 0.25 gamma = 2.0 model = resnet18(num_classes = 3, alpha, gamma) model.eval() # not training model.cuda() # on gpu model.calculate_focalLoss = False dummy_input = torch.rand(1,3,224,224).cuda() # random input on gpu &#39;&#39;&#39; very important to note that 1) the input must be tensor 2) write can only keep track of tensor, not native python objects. this becomes problematic when the model transforms anchor boxes &#39;&#39;&#39; write.add_graph(model, input_to_model=dummy_input, verbose=True) writer.close() Focal Loss Given the size of the canvas versus the number of objects, we can tell that a vast majority of the anchor boxes do not contain anything and therefore are irrelevant. This imbalance overwhelms network learning with too many easy negative examples. One trick to make training possible is to reduce the weight of the loss on negative samples. Focal loss is one way to achieve this. The basic component of focal loss is binary cross entropy loss, but focal loss uses an extra factor (1-p) ^exponent and alpha to control the contribution of well-classified and less well-classified examples in the loss. As the gamma / exponent rises, the contribution of loss from well-classified examples shrinks. The gamma, a hyperparameter, is very crucial. I figured out that gamma = 2.0 gets the job done (others have found similar values work well too, but the value is really task-dependent. For another task, gamma = 0 or cross entropy loss performs better). Source: Focal Loss for Dense Object Detection With this knowledge, here is the implementation of a small RetinaNet for detecting circles, rectangles and triangles. class FocalLoss(nn.Module): def __init__(self, alpha, gamma): super(FocalLoss, self).__init__() self.alpha = alpha self.gamma = gamma def forward(self, classifications, regressions, anchors, annotations): batch_size = classifications.shape[0] classification_losses = [] regression_losses = [] anchor = anchors[0, :, :] anchor_widths = anchor[:, 2] - anchor[:, 0] anchor_heights = anchor[:, 3] - anchor[:, 1] anchor_ctr_x = anchor[:, 0] + 0.5 * anchor_widths anchor_ctr_y = anchor[:, 1] + 0.5 * anchor_heights for j in range(batch_size): classification = classifications[j, :, :] regression = regressions[j, :, :] bbox_annotation = annotations[j, :, :] bbox_annotation = bbox_annotation[bbox_annotation[:, 4] != -1] # prevent issues with log, cannot be too close to 0 or 1 classification = torch.clamp(classification, 1e-4, 1.0 - 1e-4) &#39;&#39;&#39; calculate focal loss &#39;&#39;&#39; &#39;&#39;&#39; special case : no object per image &#39;&#39;&#39; if bbox_annotation.shape[0] == 0: alpha_factor = torch.ones(classification.shape)* self.alpha alpha_factor = place_on_cpu_gpu(alpha_factor) alpha_factor = 1. - alpha_factor focal_weight = classification focal_weight = alpha_factor * torch.pow(focal_weight, self.gamma) bce = -(torch.log(1.0 - classification)) IoU = calc_iou(anchors[0, :, :], bbox_annotation[:, :4]) # num_anchors x num_annotations IoU_max, IoU_argmax = torch.max(IoU, dim=1) # num_anchors x 1 #import pdb #pdb.set_trace() # compute the loss for classification targets = torch.ones(classification.shape) * -1 targets = place_on_cpu_gpu(targets) # create a one-hot-like encoding by simple thresholding targets[torch.lt(IoU_max, 0.4), :] = 0 positive_indices = torch.ge(IoU_max, 0.5) num_positive_anchors = positive_indices.sum() assigned_annotations = bbox_annotation[IoU_argmax, :] targets[positive_indices, :] = 0 targets[positive_indices, assigned_annotations[positive_indices, 4].long()] = 1 alpha_factor = torch.ones(targets.shape) * self.alpha alpha_factor = place_on_cpu_gpu(alpha_factor) alpha_factor = torch.where(torch.eq(targets, 1.), alpha_factor, 1. - alpha_factor) focal_weight = torch.where(torch.eq(targets, 1.), 1. - classification, classification) focal_weight = alpha_factor * torch.pow(focal_weight, self.gamma) bce = -(targets * torch.log(classification) + (1.0 - targets) * torch.log(1.0 - classification)) # cls_loss = focal_weight * torch.pow(bce, gamma) cls_loss = focal_weight * bce zero_holder = torch.zeros(cls_loss.shape) zero_holder = place_on_cpu_gpu(zero_holder) cls_loss = torch.where(torch.ne(targets, -1.0), cls_loss, zero_holder) cls_loss = place_on_cpu_gpu(cls_loss) # avoid division by zero error classification_losses.append(cls_loss.sum()/torch.clamp(num_positive_anchors.float(), min=1.0)) # compute the loss for regression if positive_indices.sum() &gt; 0: assigned_annotations = assigned_annotations[positive_indices, :] anchor_widths_pi = anchor_widths[positive_indices] anchor_heights_pi = anchor_heights[positive_indices] anchor_ctr_x_pi = anchor_ctr_x[positive_indices] anchor_ctr_y_pi = anchor_ctr_y[positive_indices] gt_widths = assigned_annotations[:, 2] - assigned_annotations[:, 0] gt_heights = assigned_annotations[:, 3] - assigned_annotations[:, 1] gt_ctr_x = assigned_annotations[:, 0] + 0.5 * gt_widths gt_ctr_y = assigned_annotations[:, 1] + 0.5 * gt_heights # clip widths to 1 gt_widths = torch.clamp(gt_widths, min=1) gt_heights = torch.clamp(gt_heights, min=1) targets_dx = (gt_ctr_x - anchor_ctr_x_pi) / anchor_widths_pi targets_dy = (gt_ctr_y - anchor_ctr_y_pi) / anchor_heights_pi targets_dw = torch.log(gt_widths / anchor_widths_pi) targets_dh = torch.log(gt_heights / anchor_heights_pi) targets = torch.stack((targets_dx, targets_dy, targets_dw, targets_dh)) targets = targets.t() targets = place_on_cpu_gpu(targets) targets_scale = torch.Tensor([[0.1, 0.1, 0.2, 0.2]]) targets_scale = place_on_cpu_gpu(targets_scale) # negative_indices = 1 + (~positive_indices) regression_diff = torch.abs(targets - regression[positive_indices, :]) # squared error # torch where is like an if statement, if true then A otherwise B regression_loss = torch.where( torch.le(regression_diff, 1.0 / 9.0), 0.5 * 9.0 * torch.pow(regression_diff, 2), regression_diff - 0.5 / 9.0 ) regression_losses.append(regression_loss.mean()) else: regression_loss = torch.tensor(0).float() regression_loss = place_on_cpu_gpu(regression_loss) regression_losses.append(regression_loss) return torch.stack(classification_losses), torch.stack(regression_losses) Color Classification Since we are trying to also identify the color of the shapes, I construct a color classifier based on convolutional neural network with a few layers. Technically, RGB channels provide the exact color information and therefore multi-layer network is really not called for. Nonetheless, it is interesting to see that the network generalizes very well and does not show any sign of over fitting (more about this later in the result section). Below, I have draw the computational graph of the color classification network. Because the color classifier is nothing more than a few cascading blocks with a convolutional layer + batch norm + non-linearity layer (ReLU) + max pooling, the graph looks very neat. color classifier class ColorClassifier(nn.Module): def __init__(self, feature_size, num_features_in=3, num_classes=80): super(ColorClassifier, self).__init__() self.num_classes = num_classes self.act = nn.ReLU(inplace=True) self.conv1 = nn.Conv2d(num_features_in, feature_size, kernel_size=3, padding=0) self.bn1 = nn.BatchNorm2d(feature_size) self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1) self.conv2 = nn.Conv2d(feature_size, feature_size, kernel_size=3, padding=0) self.bn2 = nn.BatchNorm2d(feature_size) self.conv3 = nn.Conv2d(feature_size, feature_size, kernel_size=3, padding=0) self.bn3 = nn.BatchNorm2d(feature_size) self.output = nn.Linear(3*3*3, num_classes) def forward(self, x): out = self.conv1(x) out = self.bn1(out) out = self.act(out) out = self.maxpool(out) out = self.conv2(out) out = self.bn2(out) out = self.act(out) out = self.maxpool(out) out = self.conv3(out) out = self.bn3(out) out = self.act(out) out = self.maxpool(out) out = out.view(out.size(0), -1) out = self.output(out) return out Please feel free to play around with the code in my repository. Reference Parts of the code are adapted from the following resources: https://lilianweng.github.io/lil-log/2018/12/27/object-detection-part-4.html https://www.jeremyjordan.me/object-detection-one-stage/ https://github.com/signatrix/efficientdet http://pjreddie.com/yolo9000/ https://github.com/yhenon/pytorch-retinanet","headline":"Learning object detection part 3 - network training and evaluation","image":"/assets/images/toy/retinaNet.png","mainEntityOfPage":{"@type":"WebPage","@id":"/deep_learning/2020/08/02/Learning_Object_Detection_part3/"},"url":"/deep_learning/2020/08/02/Learning_Object_Detection_part3/"}</script> <!-- End Jekyll SEO tag --> <title>Wilson Fok - A data science enthusiast</title> <meta http-equip="X-UA-Compatible" content="IE=edge"> <meta name="viewport" content="width=device-width, initial-scale=1"> <meta name="description" content="Learning object detection part 3 - network training and evaluation" /> <meta name="keywords" content="Learning object detection part 3 - network training and evaluation, Wilson Fok, Deep_Learning" /> <link rel="alternate" type="application/rss+xml" title="RSS" href="/feed.xml"> <meta content="" property="fb:app_id"> <meta content="Wilson Fok" property="og:site_name"> <meta content="Learning object detection part 3 - network training and evaluation" property="og:title"> <meta content="article" property="og:type"> <meta content="Solving object detection and classification problems" property="og:description"> <meta content="/deep_learning/2020/08/02/Learning_Object_Detection_part3/" property="og:url"> <meta content="2020-08-02T00:00:00+08:00" property="article:published_time"> <meta content="/about/" property="article:author"> <meta content="/assets/img/posts//assets/images/toy/retinaNet.png" property="og:image"> <meta content="Deep_Learning" property="article:section"> <meta name="twitter:card" content="summary"> <meta name="twitter:site" content="@"> <meta name="twitter:creator" content="@"> <meta name="twitter:title" content="Learning object detection part 3 - network training and evaluation"> <meta content="Wilson Fok" property="og:site_name"> <meta name="twitter:url" content="/deep_learning/2020/08/02/Learning_Object_Detection_part3/"> <meta name="twitter:description" content="Hello, My name is Wilson Fok. I love to extract useful insights and knowledge from big data. I also like to make new friends and connections. Let's connect! "> <!-- load layout style css --> <link rel="stylesheet" href="/assets/css/main.css" /> <link rel="stylesheet" href="/assets/css/custom-style.css" /> <link rel="stylesheet" href="/assets/bower_components/lightgallery/dist/css/lightgallery.min.css"/> <link rel="stylesheet" href="/assets/bower_components/bootstrap/dist/css/bootstrap.min.css" /> <link rel="stylesheet" href="/assets/bower_components/font-awesome/web-fonts-with-css/css/fontawesome-all.min.css" /> <!-- Favicon --> <link rel="icon" href="/assets/img/favicon.ico" type="image/gif" sizes="16x16"> <!-- Jquery --> <!-- one or more of the below scripts does fancy word animation and dropdown menu --> <script src="/assets/extra_js/jquery-3.4.1.min.js"></script> <script src="/assets/extra_js/picturefill min/picturefill.min.js"></script> <script src="/assets/extra_js/instantsearch min/instantsearch.min.js"></script> <script src="/assets/extra_js/moment min/moment.min.js"></script> <script src="/assets/bower_components/jquery.easing/jquery.easing.min.js"></script> <script src="/assets/bower_components/bootstrap/dist/js/bootstrap.bundle.min.js"></script> <script src="/assets/bower_components/jquery-mousewheel/jquery.mousewheel.min.js"></script> <script src="/assets/bower_components/lightgallery/dist/js/lightgallery-all.min.js"></script> <script src="/assets/bower_components/imagesloaded/imagesloaded.pkgd.min.js"></script> <script src="/assets/bower_components/nanobar/nanobar.min.js"></script> <script src="/assets/bower_components/typewrite/dist/typewrite.min.js"></script> <script type="text/javascript" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script> </head><body> <div class="container-fluid"><header> <script src="https://cdn.knightlab.com/libs/juxtapose/latest/js/juxtapose.min.js"></script> <link rel="stylesheet" href="https://cdn.knightlab.com/libs/juxtapose/latest/css/juxtapose.css"> <div class="col-lg-12"> <div class="row"> <nav class="navbar navbar-expand-lg fixed-top navbar-dark " id="topNav"> <!-- <a class="navbar-brand" href="#">Wilson Fok</a> --> <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="navbar-toggler-icon"></span> </button> <a class="navbar-brand" href="/">Wilson Fok</a> <div class="collapse navbar-collapse" id="navbarNav"> <ul class="navbar-nav"> <li class="nav-item"> <a class="nav-link" href="/about">About Me</a> </li> <li class="nav-item"> <a class="nav-link" href="/blog">Blog</a> </li> <li class="nav-item"> <a class="nav-link" href="/blog/categories">Categories</a> </li> <li class="nav-item"> <a class="nav-link" href="/gallery">Gallery</a> </li> <li class="nav-item"> <a class="nav-link" href="/contact">Contact Me</a> </li> </ul> </div> <ul class="nav justify-content-end"> <!-- <li class="nav-item"> <a class="nav-link" id="search-icon" href="/search/"><i class="fa fa-search" aria-hidden="true"></i></a> </li> --> <li class="nav-item"> <input class="nav-link switch" id="theme-toggle" onclick="modeSwitcher() "type="checkbox" name="checkbox" > </li> </ul> </nav> </div> </div> </header><div class="col-lg-12"> <!-- Blog Post Breadcrumbs --><div class="col-lg-12"> <nav aria-label="breadcrumb" role="navigation"> <ol class="breadcrumb"> <li class="breadcrumb-item"> <a href="/blog"><i class="fa fa-home" aria-hidden="true"></i></a> </li> <li class="breadcrumb-item active" aria-current="page"><a href="/deep_learning/2020/08/02/Learning_Object_Detection_part3/">Learning object detection part 3 - network training and evaluation</a></li> </ol> </nav> </div><div class="row" id="blog-post-container"> <div class="col-lg-8 offset-md-2"><article class="card" itemscope itemtype="http://schema.org/BlogPosting"> <div class="card-header"> <h1 class="post-title" itemprop="name headline">Learning object detection part 3 - network training and evaluation</h1> <p></p> <h6 class="post-meta"> <i> Summary : Solving object detection and classification problems</i> </h6> <p class="post-summary">Posted by : <img src="/assets/img/profile.png" class="author-profile-img"> <span itemprop="author" itemscope itemtype="http://schema.org/Person"> <span itemprop="name">Wilson Fok</span> </span> on <time datetime="2020-08-02 00:00:00 +0800" itemprop="datePublished">Aug 2, 2020</time> </p> <span class="disqus-comment-count" data-disqus-identifier="/deep_learning/2020/08/02/Learning_Object_Detection_part3/"></span> <div class="post-categories"> Category : <a href="/blog/categories/Deep_Learning">Deep_Learning</a> </div> </div> <div class="card-body" itemprop="articleBody"> <h3 id="object-detection">Object Detection</h3> <p>In this context, object detection refers to the localization of the circle, rectangle and triangle on an image with the use of bounding boxes. Classification refers to identification of object colors, namely red, green and blue.</p> <p>RetinaNet uses resNet as the model’s backbone. I have selected a small resNet, resNet 18. The details of the implementation can be found in the code below. The outputs of the resNet are three feature maps of different spatial resolutions, denoted by the white rectangles with gray boarders on the left (see the figure from the original publication below, <a href="https://arxiv.org/abs/1708.02002">Focal Loss for Dense Object Detection</a>).</p> <p><img src="/assets/images/toy/retinaNet.png" alt="Picture description" width="1100px" /></p> <center> RetinaNet and how it works </center> <p></p> <p>These feature maps are fed into a feature pyramid net (in green) which extends the hierarchical scales of spatial resolutions from three to five. One way to visualize the feature pyramid is to look at the green pyramid in the figure below. Note that resolution increases as we move down, and vice versa. This concept is particularly important because these scales help ensure the network can detect objects of various sizes as they appear across the scales. For example, a person may appear quite big in a portrait but tiny in a busy traffic scene as one amongst many pedestrians.</p> <p><img src="/assets/images/toy/FCOS.png" alt="Picture description" width="1100px" /></p> <center> FCOS model. This figure of feature pyramid diagram is far clearer than the diagram in the RetinaNet paper </center> <p></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">PyramidFeatures</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">C3_size</span><span class="p">,</span> <span class="n">C4_size</span><span class="p">,</span> <span class="n">C5_size</span><span class="p">,</span> <span class="n">feature_size</span><span class="o">=</span><span class="mi">256</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">PyramidFeatures</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>

        <span class="c1"># upsample C5 to get P5 from the FPN paper
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">P5_1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">C5_size</span><span class="p">,</span> <span class="n">feature_size</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">P5_upsampled</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Upsample</span><span class="p">(</span><span class="n">scale_factor</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s">'nearest'</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">P5_2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">feature_size</span><span class="p">,</span> <span class="n">feature_size</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># add P5 elementwise to C4
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">P4_1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">C4_size</span><span class="p">,</span> <span class="n">feature_size</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">P4_upsampled</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Upsample</span><span class="p">(</span><span class="n">scale_factor</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s">'nearest'</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">P4_2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">feature_size</span><span class="p">,</span> <span class="n">feature_size</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># add P4 elementwise to C3
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">P3_1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">C3_size</span><span class="p">,</span> <span class="n">feature_size</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">P3_2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">feature_size</span><span class="p">,</span> <span class="n">feature_size</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># "P6 is obtained via a 3x3 stride-2 conv on C5"
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">P6</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">C5_size</span><span class="p">,</span> <span class="n">feature_size</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># "P7 is computed by applying ReLU followed by a 3x3 stride-2 conv on P6"
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">P7_1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">ReLU</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">P7_2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">feature_size</span><span class="p">,</span> <span class="n">feature_size</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
        <span class="n">C3</span><span class="p">,</span> <span class="n">C4</span><span class="p">,</span> <span class="n">C5</span> <span class="o">=</span> <span class="n">inputs</span>

        <span class="n">P5_x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">P5_1</span><span class="p">(</span><span class="n">C5</span><span class="p">)</span>
        <span class="n">P5_upsampled_x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">P5_upsampled</span><span class="p">(</span><span class="n">P5_x</span><span class="p">)</span>
        <span class="n">P5_x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">P5_2</span><span class="p">(</span><span class="n">P5_x</span><span class="p">)</span>

        <span class="n">P4_x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">P4_1</span><span class="p">(</span><span class="n">C4</span><span class="p">)</span>
        <span class="n">P4_x</span> <span class="o">=</span> <span class="n">P5_upsampled_x</span> <span class="o">+</span> <span class="n">P4_x</span>
        <span class="n">P4_upsampled_x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">P4_upsampled</span><span class="p">(</span><span class="n">P4_x</span><span class="p">)</span>
        <span class="n">P4_x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">P4_2</span><span class="p">(</span><span class="n">P4_x</span><span class="p">)</span>

        <span class="n">P3_x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">P3_1</span><span class="p">(</span><span class="n">C3</span><span class="p">)</span>
        <span class="n">P3_x</span> <span class="o">=</span> <span class="n">P3_x</span> <span class="o">+</span> <span class="n">P4_upsampled_x</span>
        <span class="n">P3_x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">P3_2</span><span class="p">(</span><span class="n">P3_x</span><span class="p">)</span>

        <span class="n">P6_x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">P6</span><span class="p">(</span><span class="n">C5</span><span class="p">)</span>

        <span class="n">P7_x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">P7_1</span><span class="p">(</span><span class="n">P6_x</span><span class="p">)</span>
        <span class="n">P7_x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">P7_2</span><span class="p">(</span><span class="n">P7_x</span><span class="p">)</span>

        <span class="k">return</span> <span class="p">[</span><span class="n">P3_x</span><span class="p">,</span> <span class="n">P4_x</span><span class="p">,</span> <span class="n">P5_x</span><span class="p">,</span> <span class="n">P6_x</span><span class="p">,</span> <span class="n">P7_x</span><span class="p">]</span>



</code></pre></div></div> <p>One of the most important ideas is that objects can appear anywhere. To detect them successfully, we need to scan the entire image evenly. To do so, we cover an image with pre-defined regions (anchor boxes) that break up the entire image. We can set the ratios and scales of these boxes such that they can capture objects of various sizes and shapes. The left-most diagram shows how anchor boxes systematically cover the entire image.</p> <p><img src="/assets/images/toy/anchorbox_predictionsrefine.png" alt="Picture description" width="1100px" /></p> <center> Anchor boxes and how they work. Source: <a href="https://www.mathworks.com/help/vision/ug/anchor-boxes-for-object-detection.html"> Anchor Boxes for Object Detection - MATLAB &amp; Simulink </a> </center> <p></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="k">class</span> <span class="nc">Anchors</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pyramid_levels</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">strides</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">sizes</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">ratios</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">scales</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Anchors</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>

        <span class="k">if</span> <span class="n">pyramid_levels</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">pyramid_levels</span> <span class="o">=</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">strides</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">strides</span> <span class="o">=</span> <span class="p">[</span><span class="mi">2</span> <span class="o">**</span> <span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">pyramid_levels</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">sizes</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">sizes</span> <span class="o">=</span> <span class="p">[</span><span class="mi">2</span> <span class="o">**</span> <span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="mi">2</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">pyramid_levels</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">ratios</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">ratios</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
        <span class="k">if</span> <span class="n">scales</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">scales</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="mi">2</span><span class="o">**-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mi">2</span><span class="o">**-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">2</span> <span class="o">**</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span> <span class="o">**</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">/</span> <span class="mf">3.0</span><span class="p">),</span> <span class="mi">2</span> <span class="o">**</span> <span class="p">(</span><span class="mf">2.0</span> <span class="o">/</span> <span class="mf">3.0</span><span class="p">)])</span>
            
        <span class="k">print</span> <span class="p">(</span><span class="s">'-----Anchor boxes setting------'</span><span class="p">)</span>
        <span class="k">print</span> <span class="p">(</span><span class="s">'pyramid_levels {}'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">pyramid_levels</span><span class="p">))</span>
        <span class="k">print</span> <span class="p">(</span><span class="s">'strides {}'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">strides</span><span class="p">))</span>
        <span class="k">print</span> <span class="p">(</span><span class="s">'sizes {}'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">sizes</span><span class="p">))</span>
        <span class="k">print</span> <span class="p">(</span><span class="s">'ratios {}'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">ratios</span><span class="p">))</span>
        <span class="k">print</span> <span class="p">(</span><span class="s">'scales {}'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">scales</span><span class="p">))</span>
        
        <span class="bp">self</span><span class="p">.</span><span class="n">num_anchors</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">ratios</span><span class="p">)</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">scales</span><span class="p">)</span>
        <span class="k">print</span> <span class="p">(</span><span class="s">'num_anchors {}'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">num_anchors</span><span class="p">))</span>
        
        

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">image</span><span class="p">):</span>
        
        <span class="n">image_shape</span> <span class="o">=</span> <span class="n">image</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">:]</span>
        <span class="n">image_shape</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">image_shape</span><span class="p">)</span>
        <span class="n">image_shapes</span> <span class="o">=</span> <span class="p">[(</span><span class="n">image_shape</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">**</span> <span class="n">x</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="p">(</span><span class="mi">2</span> <span class="o">**</span> <span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">pyramid_levels</span><span class="p">]</span>

        <span class="c1"># compute anchors over all pyramid levels
</span>        <span class="n">all_anchors</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">4</span><span class="p">)).</span><span class="n">astype</span><span class="p">(</span><span class="n">TORCH_DATATYPE</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">p</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">pyramid_levels</span><span class="p">):</span>
            <span class="n">anchors</span>         <span class="o">=</span> <span class="n">generate_anchors</span><span class="p">(</span><span class="n">base_size</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">sizes</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span> <span class="n">ratios</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">ratios</span><span class="p">,</span> <span class="n">scales</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">scales</span><span class="p">)</span>
            <span class="n">shifted_anchors</span> <span class="o">=</span> <span class="n">shift</span><span class="p">(</span><span class="n">image_shapes</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span> <span class="bp">self</span><span class="p">.</span><span class="n">strides</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span> <span class="n">anchors</span><span class="p">)</span>
            <span class="n">all_anchors</span>     <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">all_anchors</span><span class="p">,</span> <span class="n">shifted_anchors</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

        <span class="n">all_anchors</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">all_anchors</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">all_anchors</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">all_anchors</span><span class="p">.</span><span class="n">astype</span><span class="p">(</span><span class="n">TORCH_DATATYPE</span><span class="p">))</span>
        
        <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">is_available</span><span class="p">():</span>
            <span class="n">all_anchors</span> <span class="o">=</span> <span class="n">all_anchors</span><span class="p">.</span><span class="n">cuda</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">all_anchors</span>
    

<span class="k">def</span> <span class="nf">generate_anchors</span><span class="p">(</span><span class="n">base_size</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">ratios</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">scales</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="s">"""
    Generate anchor (reference) windows by enumerating aspect ratios X
    scales w.r.t. a reference window.
    """</span>

    <span class="k">if</span> <span class="n">ratios</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">ratios</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>

    <span class="k">if</span> <span class="n">scales</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">scales</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="mi">2</span> <span class="o">**</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span> <span class="o">**</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">/</span> <span class="mf">3.0</span><span class="p">),</span> <span class="mi">2</span> <span class="o">**</span> <span class="p">(</span><span class="mf">2.0</span> <span class="o">/</span> <span class="mf">3.0</span><span class="p">)])</span>

    <span class="n">num_anchors</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">ratios</span><span class="p">)</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">scales</span><span class="p">)</span>

    <span class="c1"># initialize output anchors
</span>    <span class="n">anchors</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">num_anchors</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>

    <span class="c1"># scale base_size
</span>    <span class="n">anchors</span><span class="p">[:,</span> <span class="mi">2</span><span class="p">:]</span> <span class="o">=</span> <span class="n">base_size</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">tile</span><span class="p">(</span><span class="n">scales</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">ratios</span><span class="p">))).</span><span class="n">T</span>

    <span class="c1"># compute areas of anchors
</span>    <span class="n">areas</span> <span class="o">=</span> <span class="n">anchors</span><span class="p">[:,</span> <span class="mi">2</span><span class="p">]</span> <span class="o">*</span> <span class="n">anchors</span><span class="p">[:,</span> <span class="mi">3</span><span class="p">]</span>

    <span class="c1"># correct for ratios
</span>    <span class="n">anchors</span><span class="p">[:,</span> <span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">areas</span> <span class="o">/</span> <span class="n">np</span><span class="p">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">ratios</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">scales</span><span class="p">)))</span>
    <span class="n">anchors</span><span class="p">[:,</span> <span class="mi">3</span><span class="p">]</span> <span class="o">=</span> <span class="n">anchors</span><span class="p">[:,</span> <span class="mi">2</span><span class="p">]</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">ratios</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">scales</span><span class="p">))</span>

    <span class="c1"># transform from (x_ctr, y_ctr, w, h) -&gt; (x1, y1, x2, y2)
</span>    <span class="n">anchors</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">-=</span> <span class="n">np</span><span class="p">.</span><span class="n">tile</span><span class="p">(</span><span class="n">anchors</span><span class="p">[:,</span> <span class="mi">2</span><span class="p">]</span> <span class="o">*</span> <span class="mf">0.5</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)).</span><span class="n">T</span>
    <span class="n">anchors</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">-=</span> <span class="n">np</span><span class="p">.</span><span class="n">tile</span><span class="p">(</span><span class="n">anchors</span><span class="p">[:,</span> <span class="mi">3</span><span class="p">]</span> <span class="o">*</span> <span class="mf">0.5</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)).</span><span class="n">T</span>

    <span class="k">return</span> <span class="n">anchors</span>

<span class="k">def</span> <span class="nf">compute_shape</span><span class="p">(</span><span class="n">image_shape</span><span class="p">,</span> <span class="n">pyramid_levels</span><span class="p">):</span>
    <span class="s">"""Compute shapes based on pyramid levels.
    :param image_shape:
    :param pyramid_levels:
    :return:
    """</span>
    <span class="n">image_shape</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">image_shape</span><span class="p">[:</span><span class="mi">2</span><span class="p">])</span>
    <span class="n">image_shapes</span> <span class="o">=</span> <span class="p">[(</span><span class="n">image_shape</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">**</span> <span class="n">x</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="p">(</span><span class="mi">2</span> <span class="o">**</span> <span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">pyramid_levels</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">image_shapes</span>

<span class="k">def</span> <span class="nf">anchors_for_shape</span><span class="p">(</span>
    <span class="n">image_shape</span><span class="p">,</span>
    <span class="n">pyramid_levels</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="n">ratios</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="n">scales</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="n">strides</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="n">sizes</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="n">shapes_callback</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
<span class="p">):</span>

    <span class="n">image_shapes</span> <span class="o">=</span> <span class="n">compute_shape</span><span class="p">(</span><span class="n">image_shape</span><span class="p">,</span> <span class="n">pyramid_levels</span><span class="p">)</span>

    <span class="c1"># compute anchors over all pyramid levels
</span>    <span class="n">all_anchors</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">p</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">pyramid_levels</span><span class="p">):</span>
        <span class="n">anchors</span>         <span class="o">=</span> <span class="n">generate_anchors</span><span class="p">(</span><span class="n">base_size</span><span class="o">=</span><span class="n">sizes</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span> <span class="n">ratios</span><span class="o">=</span><span class="n">ratios</span><span class="p">,</span> <span class="n">scales</span><span class="o">=</span><span class="n">scales</span><span class="p">)</span>
        <span class="n">shifted_anchors</span> <span class="o">=</span> <span class="n">shift</span><span class="p">(</span><span class="n">image_shapes</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span> <span class="n">strides</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span> <span class="n">anchors</span><span class="p">)</span>
        <span class="n">all_anchors</span>     <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">all_anchors</span><span class="p">,</span> <span class="n">shifted_anchors</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">all_anchors</span>

<span class="k">def</span> <span class="nf">shift</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">anchors</span><span class="p">):</span>
    <span class="n">shift_x</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="o">+</span> <span class="mf">0.5</span><span class="p">)</span> <span class="o">*</span> <span class="n">stride</span>
    <span class="n">shift_y</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">+</span> <span class="mf">0.5</span><span class="p">)</span> <span class="o">*</span> <span class="n">stride</span>

    <span class="n">shift_x</span><span class="p">,</span> <span class="n">shift_y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">shift_x</span><span class="p">,</span> <span class="n">shift_y</span><span class="p">)</span>

    <span class="n">shifts</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">vstack</span><span class="p">((</span>
        <span class="n">shift_x</span><span class="p">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">shift_y</span><span class="p">.</span><span class="n">ravel</span><span class="p">(),</span>
        <span class="n">shift_x</span><span class="p">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">shift_y</span><span class="p">.</span><span class="n">ravel</span><span class="p">()</span>
    <span class="p">)).</span><span class="n">transpose</span><span class="p">()</span>

    <span class="c1"># add A anchors (1, A, 4) to
</span>    <span class="c1"># cell K shifts (K, 1, 4) to get
</span>    <span class="c1"># shift anchors (K, A, 4)
</span>    <span class="c1"># reshape to (K*A, 4) shifted anchors
</span>    <span class="n">A</span> <span class="o">=</span> <span class="n">anchors</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">K</span> <span class="o">=</span> <span class="n">shifts</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">all_anchors</span> <span class="o">=</span> <span class="p">(</span><span class="n">anchors</span><span class="p">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span> <span class="o">+</span> <span class="n">shifts</span><span class="p">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="mi">4</span><span class="p">)).</span><span class="n">transpose</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">)))</span>
    <span class="n">all_anchors</span> <span class="o">=</span> <span class="n">all_anchors</span><span class="p">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">K</span> <span class="o">*</span> <span class="n">A</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">all_anchors</span>
</code></pre></div></div> <p>In this toy example, I have tried Ratios = [0.5, 1, 2], Scales = [0.5, 0.7, 1, 1.25, 1,6], totaling 15 combinations. A ratio of 1 is good for circle and square. By imposing a minimum area in the generation of the toy dataset, I make sure the samples are easy for the network because I do not need to look at scales that are below 0.5. But, what if a circle, triangle, or rectangle does not fall in these pre-determine box sizes? The solution to this is a class + box subnet branches.</p> <p>The class + box subnet comprises of two independent branches. The class subnet branch (ClassificationModel) predicts the classes of the object, may it be circle, rectangle or triangle. K is the number of shapes. In this case, K is 3. A represents the number of pre-defined anchor boxes. The box subnet branch (RegressionModel)predicts how the pre-defined anchor boxes ought to be adjusted to fit the object tightly. It regresses the change needed to apply to the corners of the most relevant anchor boxes or those with a circle, triangle or rectangle inside (see diagram in the middle and far-right). Note that the class + box subnet’s weights are shared across all levels and locations.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">RegressionModel</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_features_in</span><span class="p">,</span> <span class="n">num_anchors</span><span class="o">=</span><span class="mi">9</span><span class="p">,</span> <span class="n">feature_size</span><span class="o">=</span><span class="mi">256</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">RegressionModel</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="p">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">num_features_in</span><span class="p">,</span> <span class="n">feature_size</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">act1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">ReLU</span><span class="p">()</span>

        <span class="bp">self</span><span class="p">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">feature_size</span><span class="p">,</span> <span class="n">feature_size</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">act2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">ReLU</span><span class="p">()</span>

        <span class="bp">self</span><span class="p">.</span><span class="n">conv3</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">feature_size</span><span class="p">,</span> <span class="n">feature_size</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">act3</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">ReLU</span><span class="p">()</span>

        <span class="bp">self</span><span class="p">.</span><span class="n">conv4</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">feature_size</span><span class="p">,</span> <span class="n">feature_size</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">act4</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">ReLU</span><span class="p">()</span>

        <span class="bp">self</span><span class="p">.</span><span class="n">output</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">feature_size</span><span class="p">,</span> <span class="n">num_anchors</span> <span class="o">*</span> <span class="mi">4</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">act1</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>

        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">act2</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>

        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">conv3</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">act3</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>

        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">conv4</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">act4</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>

        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">output</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>

        <span class="c1"># out is B x C x W x H, with C = 4*num_anchors
</span>        <span class="n">out</span> <span class="o">=</span> <span class="n">out</span><span class="p">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">out</span><span class="p">.</span><span class="n">contiguous</span><span class="p">().</span><span class="n">view</span><span class="p">(</span><span class="n">out</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">ClassificationModel</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_features_in</span><span class="p">,</span> <span class="n">num_anchors</span><span class="o">=</span><span class="mi">9</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">80</span><span class="p">,</span> <span class="n">prior</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">feature_size</span><span class="o">=</span><span class="mi">256</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">ClassificationModel</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="p">.</span><span class="n">num_classes</span> <span class="o">=</span> <span class="n">num_classes</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">num_anchors</span> <span class="o">=</span> <span class="n">num_anchors</span>

        <span class="bp">self</span><span class="p">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">num_features_in</span><span class="p">,</span> <span class="n">feature_size</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">act1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">ReLU</span><span class="p">()</span>

        <span class="bp">self</span><span class="p">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">feature_size</span><span class="p">,</span> <span class="n">feature_size</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">act2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">ReLU</span><span class="p">()</span>

        <span class="bp">self</span><span class="p">.</span><span class="n">conv3</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">feature_size</span><span class="p">,</span> <span class="n">feature_size</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">act3</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">ReLU</span><span class="p">()</span>

        <span class="bp">self</span><span class="p">.</span><span class="n">conv4</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">feature_size</span><span class="p">,</span> <span class="n">feature_size</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">act4</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">ReLU</span><span class="p">()</span>

        <span class="bp">self</span><span class="p">.</span><span class="n">output</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">feature_size</span><span class="p">,</span> <span class="n">num_anchors</span> <span class="o">*</span> <span class="n">num_classes</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">output_act</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Sigmoid</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">act1</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>

        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">act2</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>

        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">conv3</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">act3</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>

        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">conv4</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">act4</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>

        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">output</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">output_act</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>

        <span class="c1"># out is B x C x W x H, with C = n_classes + n_anchors
</span>        <span class="n">out1</span> <span class="o">=</span> <span class="n">out</span><span class="p">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

        <span class="n">batch_size</span><span class="p">,</span> <span class="n">width</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">channels</span> <span class="o">=</span> <span class="n">out1</span><span class="p">.</span><span class="n">shape</span>

        <span class="n">out2</span> <span class="o">=</span> <span class="n">out1</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">width</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">num_anchors</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">num_classes</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">out2</span><span class="p">.</span><span class="n">contiguous</span><span class="p">().</span><span class="n">view</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">num_classes</span><span class="p">)</span>


</code></pre></div></div> <p>When we interpret the network’s classification and detected bounding boxes, we need to decide which detected boxes are chosen to represent the detected object. In other words, when multiple anchor boxes overlap (measured by intersection-over-union) with each other for a single object to a greater or lesser degree, we choose the box with the highest classification scores and with good overlaps among competing detected boxes. Such situation calls for non-maximum suppression algorithm. The threshold in this model is 0.5 (standard).</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>	    <span class="n">transformed_anchors</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">regressBoxes</span><span class="p">(</span><span class="n">anchors</span><span class="p">,</span> <span class="n">regression</span><span class="p">)</span>
            <span class="n">transformed_anchors</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">clipBoxes</span><span class="p">(</span><span class="n">transformed_anchors</span><span class="p">,</span> <span class="n">img_batch</span><span class="p">)</span>

            <span class="n">finalResult</span> <span class="o">=</span> <span class="p">[[],</span> <span class="p">[],</span> <span class="p">[]]</span>

            <span class="n">finalScores</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">([])</span>
            <span class="n">finalAnchorBoxesIndexes</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">([]).</span><span class="nb">long</span><span class="p">()</span>
            <span class="n">finalAnchorBoxesCoordinates</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">([])</span>

            <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">is_available</span><span class="p">():</span>
                <span class="n">finalScores</span> <span class="o">=</span> <span class="n">finalScores</span><span class="p">.</span><span class="n">cuda</span><span class="p">()</span>
                <span class="n">finalAnchorBoxesIndexes</span> <span class="o">=</span> <span class="n">finalAnchorBoxesIndexes</span><span class="p">.</span><span class="n">cuda</span><span class="p">()</span>
                <span class="n">finalAnchorBoxesCoordinates</span> <span class="o">=</span> <span class="n">finalAnchorBoxesCoordinates</span><span class="p">.</span><span class="n">cuda</span><span class="p">()</span>

            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">classification</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]):</span>
                <span class="n">scores</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">classification</span><span class="p">[:,</span> <span class="p">:,</span> <span class="n">i</span><span class="p">])</span>
                <span class="n">scores_over_thresh</span> <span class="o">=</span> <span class="p">(</span><span class="n">scores</span> <span class="o">&gt;</span> <span class="mf">0.05</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">scores_over_thresh</span><span class="p">.</span><span class="nb">sum</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="c1"># no boxes to NMS, just continue
</span>                    <span class="k">continue</span>

                <span class="n">scores</span> <span class="o">=</span> <span class="n">scores</span><span class="p">[</span><span class="n">scores_over_thresh</span><span class="p">]</span>
                <span class="n">anchorBoxes</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">transformed_anchors</span><span class="p">)</span>
                <span class="n">anchorBoxes</span> <span class="o">=</span> <span class="n">anchorBoxes</span><span class="p">[</span><span class="n">scores_over_thresh</span><span class="p">]</span>
                
                <span class="c1"># post-processing non-maximum suppression
</span>                <span class="n">anchors_nms_idx</span> <span class="o">=</span> <span class="n">nms</span><span class="p">(</span><span class="n">anchorBoxes</span><span class="p">,</span> <span class="n">scores</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">)</span>

                <span class="n">finalResult</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">extend</span><span class="p">(</span><span class="n">scores</span><span class="p">[</span><span class="n">anchors_nms_idx</span><span class="p">])</span>
                <span class="n">finalResult</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="n">extend</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">anchors_nms_idx</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
                <span class="n">finalResult</span><span class="p">[</span><span class="mi">2</span><span class="p">].</span><span class="n">extend</span><span class="p">(</span><span class="n">anchorBoxes</span><span class="p">[</span><span class="n">anchors_nms_idx</span><span class="p">])</span>

                <span class="n">finalScores</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cat</span><span class="p">((</span><span class="n">finalScores</span><span class="p">,</span> <span class="n">scores</span><span class="p">[</span><span class="n">anchors_nms_idx</span><span class="p">]))</span>
                
                <span class="c1"># control the number of predicted classes, one for each box
</span>                <span class="n">finalAnchorBoxesIndexesValue</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">anchors_nms_idx</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
                <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">is_available</span><span class="p">():</span>
                    <span class="n">finalAnchorBoxesIndexesValue</span> <span class="o">=</span> <span class="n">finalAnchorBoxesIndexesValue</span><span class="p">.</span><span class="n">cuda</span><span class="p">()</span>

                <span class="n">finalAnchorBoxesIndexes</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cat</span><span class="p">((</span><span class="n">finalAnchorBoxesIndexes</span><span class="p">,</span> <span class="n">finalAnchorBoxesIndexesValue</span><span class="p">))</span>
                <span class="n">finalAnchorBoxesCoordinates</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cat</span><span class="p">((</span><span class="n">finalAnchorBoxesCoordinates</span><span class="p">,</span> <span class="n">anchorBoxes</span><span class="p">[</span><span class="n">anchors_nms_idx</span><span class="p">]))</span>

</code></pre></div></div> <p>Below, I have draw the computational graph of the RetinaNet. However, it is far to complicated to really draw the steps taken inside the network, but I show you the graph anyway. <img src="/assets/images/toy/models/retinaNet.png" alt="RetinaNet" width="950px" /></p> <center>RetinaNet</center> <p></p> <p>TensorboardX can keep track of the computations being done throughout the network as long as the inputs and outputs are pytorch tensors. That means I have only drawn the parts from input up to right after class + box subnets and focal loss operation. Thus, I draw the computational graph by doing the followings:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">from</span> <span class="nn">tensorboardX</span> <span class="kn">import</span> <span class="n">SummaryWriter</span>
<span class="kn">from</span> <span class="nn">model_retinaNet</span> <span class="kn">import</span> <span class="n">resnet18</span>

<span class="n">writer</span> <span class="o">=</span> <span class="n">SummaryWriter</span><span class="p">(</span><span class="n">logdir</span><span class="o">=</span><span class="n">logdir</span><span class="p">,</span> <span class="n">flush_secs</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.25</span>
<span class="n">gamma</span> <span class="o">=</span> <span class="mf">2.0</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">resnet18</span><span class="p">(</span><span class="n">num_classes</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">gamma</span><span class="p">)</span>
<span class="n">model</span><span class="p">.</span><span class="nb">eval</span><span class="p">()</span> <span class="c1"># not training
</span><span class="n">model</span><span class="p">.</span><span class="n">cuda</span><span class="p">()</span> <span class="c1"># on gpu
</span><span class="n">model</span><span class="p">.</span><span class="n">calculate_focalLoss</span> <span class="o">=</span> <span class="bp">False</span>
<span class="n">dummy_input</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">224</span><span class="p">,</span><span class="mi">224</span><span class="p">).</span><span class="n">cuda</span><span class="p">()</span> <span class="c1"># random input on gpu
</span><span class="s">'''
very important to note that 
1) the input must be tensor
2) write can only keep track of tensor, not native python objects.
this becomes problematic when the model transforms anchor boxes
'''</span>
                    
<span class="n">write</span><span class="p">.</span><span class="n">add_graph</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">input_to_model</span><span class="o">=</span><span class="n">dummy_input</span><span class="p">,</span>
                <span class="n">verbose</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">writer</span><span class="p">.</span><span class="n">close</span><span class="p">()</span>


</code></pre></div></div> <h3 id="focal-loss">Focal Loss</h3> <p>Given the size of the canvas versus the number of objects, we can tell that a vast majority of the anchor boxes do not contain anything and therefore are irrelevant. This imbalance overwhelms network learning with too many easy negative examples. One trick to make training possible is to reduce the weight of the loss on negative samples. Focal loss is one way to achieve this. The basic component of focal loss is binary cross entropy loss, but focal loss uses an extra factor (1-p) ^exponent and alpha to control the contribution of well-classified and less well-classified examples in the loss. As the gamma / exponent rises, the contribution of loss from well-classified examples shrinks.</p> <p><img src="/assets/images/toy/focalLoss_hyperparameters.png" alt="Picture description" width="1100px" /></p> <center> The gamma, a hyperparameter, is very crucial. I figured out that gamma = 2.0 gets the job done (others have found similar values work well too, but the value is really task-dependent. For another task, gamma = 0 or cross entropy loss performs better). Source: <a href="https://arxiv.org/abs/1708.02002"> Focal Loss for Dense Object Detection </a> </center> <p></p> <p>With this knowledge, here is the implementation of a small RetinaNet for detecting circles, rectangles and triangles.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">FocalLoss</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">gamma</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">FocalLoss</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        
        <span class="bp">self</span><span class="p">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="n">alpha</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">gamma</span> <span class="o">=</span> <span class="n">gamma</span>
        

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">classifications</span><span class="p">,</span> <span class="n">regressions</span><span class="p">,</span> <span class="n">anchors</span><span class="p">,</span> <span class="n">annotations</span><span class="p">):</span>
        

        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">classifications</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">classification_losses</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">regression_losses</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="n">anchor</span> <span class="o">=</span> <span class="n">anchors</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span>

        <span class="n">anchor_widths</span>  <span class="o">=</span> <span class="n">anchor</span><span class="p">[:,</span> <span class="mi">2</span><span class="p">]</span> <span class="o">-</span> <span class="n">anchor</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span>
        <span class="n">anchor_heights</span> <span class="o">=</span> <span class="n">anchor</span><span class="p">[:,</span> <span class="mi">3</span><span class="p">]</span> <span class="o">-</span> <span class="n">anchor</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span>
        <span class="n">anchor_ctr_x</span>   <span class="o">=</span> <span class="n">anchor</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">anchor_widths</span>
        <span class="n">anchor_ctr_y</span>   <span class="o">=</span> <span class="n">anchor</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">anchor_heights</span>

        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">):</span>

            <span class="n">classification</span> <span class="o">=</span> <span class="n">classifications</span><span class="p">[</span><span class="n">j</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span>
            <span class="n">regression</span> <span class="o">=</span> <span class="n">regressions</span><span class="p">[</span><span class="n">j</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span>

            <span class="n">bbox_annotation</span> <span class="o">=</span> <span class="n">annotations</span><span class="p">[</span><span class="n">j</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span>
            <span class="n">bbox_annotation</span> <span class="o">=</span> <span class="n">bbox_annotation</span><span class="p">[</span><span class="n">bbox_annotation</span><span class="p">[:,</span> <span class="mi">4</span><span class="p">]</span> <span class="o">!=</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span>
            
            <span class="c1"># prevent issues with log, cannot be too close to 0 or 1
</span>            <span class="n">classification</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">classification</span><span class="p">,</span> <span class="mf">1e-4</span><span class="p">,</span> <span class="mf">1.0</span> <span class="o">-</span> <span class="mf">1e-4</span><span class="p">)</span>

            <span class="s">'''
            calculate focal loss
            '''</span>
            <span class="s">'''
            special case : no object per image
            '''</span>
            <span class="k">if</span> <span class="n">bbox_annotation</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">alpha_factor</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">ones</span><span class="p">(</span><span class="n">classification</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span><span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">alpha</span>
                
                <span class="n">alpha_factor</span> <span class="o">=</span> <span class="n">place_on_cpu_gpu</span><span class="p">(</span><span class="n">alpha_factor</span><span class="p">)</span>


                <span class="n">alpha_factor</span> <span class="o">=</span> <span class="mf">1.</span> <span class="o">-</span> <span class="n">alpha_factor</span>
                <span class="n">focal_weight</span> <span class="o">=</span> <span class="n">classification</span>
                <span class="n">focal_weight</span> <span class="o">=</span> <span class="n">alpha_factor</span> <span class="o">*</span> <span class="n">torch</span><span class="p">.</span><span class="nb">pow</span><span class="p">(</span><span class="n">focal_weight</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">gamma</span><span class="p">)</span>

                <span class="n">bce</span> <span class="o">=</span> <span class="o">-</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">classification</span><span class="p">))</span>

              
            <span class="n">IoU</span> <span class="o">=</span> <span class="n">calc_iou</span><span class="p">(</span><span class="n">anchors</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:],</span> <span class="n">bbox_annotation</span><span class="p">[:,</span> <span class="p">:</span><span class="mi">4</span><span class="p">])</span> <span class="c1"># num_anchors x num_annotations
</span>
            <span class="n">IoU_max</span><span class="p">,</span> <span class="n">IoU_argmax</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nb">max</span><span class="p">(</span><span class="n">IoU</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># num_anchors x 1
</span>
            <span class="c1">#import pdb
</span>            <span class="c1">#pdb.set_trace()
</span>
            <span class="c1"># compute the loss for classification
</span>            <span class="n">targets</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">ones</span><span class="p">(</span><span class="n">classification</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">*</span> <span class="o">-</span><span class="mi">1</span>
            <span class="n">targets</span> <span class="o">=</span> <span class="n">place_on_cpu_gpu</span><span class="p">(</span><span class="n">targets</span><span class="p">)</span>
            
            
            <span class="c1"># create a one-hot-like encoding by simple thresholding
</span>            <span class="n">targets</span><span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">lt</span><span class="p">(</span><span class="n">IoU_max</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">),</span> <span class="p">:]</span> <span class="o">=</span> <span class="mi">0</span>

            <span class="n">positive_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">ge</span><span class="p">(</span><span class="n">IoU_max</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">)</span>

            <span class="n">num_positive_anchors</span> <span class="o">=</span> <span class="n">positive_indices</span><span class="p">.</span><span class="nb">sum</span><span class="p">()</span>

            <span class="n">assigned_annotations</span> <span class="o">=</span> <span class="n">bbox_annotation</span><span class="p">[</span><span class="n">IoU_argmax</span><span class="p">,</span> <span class="p">:]</span>

            <span class="n">targets</span><span class="p">[</span><span class="n">positive_indices</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">targets</span><span class="p">[</span><span class="n">positive_indices</span><span class="p">,</span> <span class="n">assigned_annotations</span><span class="p">[</span><span class="n">positive_indices</span><span class="p">,</span> <span class="mi">4</span><span class="p">].</span><span class="nb">long</span><span class="p">()]</span> <span class="o">=</span> <span class="mi">1</span>

            <span class="n">alpha_factor</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">ones</span><span class="p">(</span><span class="n">targets</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">alpha</span>
            <span class="n">alpha_factor</span> <span class="o">=</span> <span class="n">place_on_cpu_gpu</span><span class="p">(</span><span class="n">alpha_factor</span><span class="p">)</span>

            <span class="n">alpha_factor</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">where</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">eq</span><span class="p">(</span><span class="n">targets</span><span class="p">,</span> <span class="mf">1.</span><span class="p">),</span> <span class="n">alpha_factor</span><span class="p">,</span> <span class="mf">1.</span> <span class="o">-</span> <span class="n">alpha_factor</span><span class="p">)</span>
            <span class="n">focal_weight</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">where</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">eq</span><span class="p">(</span><span class="n">targets</span><span class="p">,</span> <span class="mf">1.</span><span class="p">),</span> <span class="mf">1.</span> <span class="o">-</span> <span class="n">classification</span><span class="p">,</span> <span class="n">classification</span><span class="p">)</span>
            <span class="n">focal_weight</span> <span class="o">=</span> <span class="n">alpha_factor</span> <span class="o">*</span> <span class="n">torch</span><span class="p">.</span><span class="nb">pow</span><span class="p">(</span><span class="n">focal_weight</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">gamma</span><span class="p">)</span>

            <span class="n">bce</span> <span class="o">=</span> <span class="o">-</span><span class="p">(</span><span class="n">targets</span> <span class="o">*</span> <span class="n">torch</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">classification</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">targets</span><span class="p">)</span> <span class="o">*</span> <span class="n">torch</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">classification</span><span class="p">))</span>

            <span class="c1"># cls_loss = focal_weight * torch.pow(bce, gamma)
</span>            <span class="n">cls_loss</span> <span class="o">=</span> <span class="n">focal_weight</span> <span class="o">*</span> <span class="n">bce</span>
            
            <span class="n">zero_holder</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">cls_loss</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
            <span class="n">zero_holder</span> <span class="o">=</span> <span class="n">place_on_cpu_gpu</span><span class="p">(</span><span class="n">zero_holder</span><span class="p">)</span>
            
            <span class="n">cls_loss</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">where</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">ne</span><span class="p">(</span><span class="n">targets</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">),</span> <span class="n">cls_loss</span><span class="p">,</span> <span class="n">zero_holder</span><span class="p">)</span>
            <span class="n">cls_loss</span> <span class="o">=</span> <span class="n">place_on_cpu_gpu</span><span class="p">(</span><span class="n">cls_loss</span><span class="p">)</span>

            <span class="c1"># avoid division by zero error
</span>            <span class="n">classification_losses</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">cls_loss</span><span class="p">.</span><span class="nb">sum</span><span class="p">()</span><span class="o">/</span><span class="n">torch</span><span class="p">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">num_positive_anchors</span><span class="p">.</span><span class="nb">float</span><span class="p">(),</span> <span class="nb">min</span><span class="o">=</span><span class="mf">1.0</span><span class="p">))</span>

            <span class="c1"># compute the loss for regression
</span>
            <span class="k">if</span> <span class="n">positive_indices</span><span class="p">.</span><span class="nb">sum</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">assigned_annotations</span> <span class="o">=</span> <span class="n">assigned_annotations</span><span class="p">[</span><span class="n">positive_indices</span><span class="p">,</span> <span class="p">:]</span>

                <span class="n">anchor_widths_pi</span> <span class="o">=</span> <span class="n">anchor_widths</span><span class="p">[</span><span class="n">positive_indices</span><span class="p">]</span>
                <span class="n">anchor_heights_pi</span> <span class="o">=</span> <span class="n">anchor_heights</span><span class="p">[</span><span class="n">positive_indices</span><span class="p">]</span>
                <span class="n">anchor_ctr_x_pi</span> <span class="o">=</span> <span class="n">anchor_ctr_x</span><span class="p">[</span><span class="n">positive_indices</span><span class="p">]</span>
                <span class="n">anchor_ctr_y_pi</span> <span class="o">=</span> <span class="n">anchor_ctr_y</span><span class="p">[</span><span class="n">positive_indices</span><span class="p">]</span>

                <span class="n">gt_widths</span>  <span class="o">=</span> <span class="n">assigned_annotations</span><span class="p">[:,</span> <span class="mi">2</span><span class="p">]</span> <span class="o">-</span> <span class="n">assigned_annotations</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span>
                <span class="n">gt_heights</span> <span class="o">=</span> <span class="n">assigned_annotations</span><span class="p">[:,</span> <span class="mi">3</span><span class="p">]</span> <span class="o">-</span> <span class="n">assigned_annotations</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span>
                <span class="n">gt_ctr_x</span>   <span class="o">=</span> <span class="n">assigned_annotations</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">gt_widths</span>
                <span class="n">gt_ctr_y</span>   <span class="o">=</span> <span class="n">assigned_annotations</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">gt_heights</span>

                <span class="c1"># clip widths to 1
</span>                <span class="n">gt_widths</span>  <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">gt_widths</span><span class="p">,</span> <span class="nb">min</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
                <span class="n">gt_heights</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">gt_heights</span><span class="p">,</span> <span class="nb">min</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

                <span class="n">targets_dx</span> <span class="o">=</span> <span class="p">(</span><span class="n">gt_ctr_x</span> <span class="o">-</span> <span class="n">anchor_ctr_x_pi</span><span class="p">)</span> <span class="o">/</span> <span class="n">anchor_widths_pi</span>
                <span class="n">targets_dy</span> <span class="o">=</span> <span class="p">(</span><span class="n">gt_ctr_y</span> <span class="o">-</span> <span class="n">anchor_ctr_y_pi</span><span class="p">)</span> <span class="o">/</span> <span class="n">anchor_heights_pi</span>
                <span class="n">targets_dw</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">gt_widths</span> <span class="o">/</span> <span class="n">anchor_widths_pi</span><span class="p">)</span>
                <span class="n">targets_dh</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">gt_heights</span> <span class="o">/</span> <span class="n">anchor_heights_pi</span><span class="p">)</span>

                <span class="n">targets</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">stack</span><span class="p">((</span><span class="n">targets_dx</span><span class="p">,</span> <span class="n">targets_dy</span><span class="p">,</span> <span class="n">targets_dw</span><span class="p">,</span> <span class="n">targets_dh</span><span class="p">))</span>
                <span class="n">targets</span> <span class="o">=</span> <span class="n">targets</span><span class="p">.</span><span class="n">t</span><span class="p">()</span>
                <span class="n">targets</span> <span class="o">=</span> <span class="n">place_on_cpu_gpu</span><span class="p">(</span><span class="n">targets</span><span class="p">)</span>
                
                <span class="n">targets_scale</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">([[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">]])</span>
                <span class="n">targets_scale</span> <span class="o">=</span> <span class="n">place_on_cpu_gpu</span><span class="p">(</span><span class="n">targets_scale</span><span class="p">)</span>
                
<span class="c1">#                negative_indices = 1 + (~positive_indices)
</span>
                <span class="n">regression_diff</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nb">abs</span><span class="p">(</span><span class="n">targets</span> <span class="o">-</span> <span class="n">regression</span><span class="p">[</span><span class="n">positive_indices</span><span class="p">,</span> <span class="p">:])</span>

                <span class="c1"># squared error
</span>                <span class="c1"># torch where is like an if statement, if true then A otherwise B
</span>                <span class="n">regression_loss</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">where</span><span class="p">(</span>
                    <span class="n">torch</span><span class="p">.</span><span class="n">le</span><span class="p">(</span><span class="n">regression_diff</span><span class="p">,</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="mf">9.0</span><span class="p">),</span>
                    <span class="mf">0.5</span> <span class="o">*</span> <span class="mf">9.0</span> <span class="o">*</span> <span class="n">torch</span><span class="p">.</span><span class="nb">pow</span><span class="p">(</span><span class="n">regression_diff</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
                    <span class="n">regression_diff</span> <span class="o">-</span> <span class="mf">0.5</span> <span class="o">/</span> <span class="mf">9.0</span>
                <span class="p">)</span>
                <span class="n">regression_losses</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">regression_loss</span><span class="p">.</span><span class="n">mean</span><span class="p">())</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">regression_loss</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">).</span><span class="nb">float</span><span class="p">()</span>
                <span class="n">regression_loss</span> <span class="o">=</span> <span class="n">place_on_cpu_gpu</span><span class="p">(</span><span class="n">regression_loss</span><span class="p">)</span>
                
                <span class="n">regression_losses</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">regression_loss</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="n">stack</span><span class="p">(</span><span class="n">classification_losses</span><span class="p">),</span> <span class="n">torch</span><span class="p">.</span><span class="n">stack</span><span class="p">(</span><span class="n">regression_losses</span><span class="p">)</span>
</code></pre></div></div> <h3 id="color-classification">Color Classification</h3> <p>Since we are trying to also identify the color of the shapes, I construct a color classifier based on convolutional neural network with a few layers. Technically, RGB channels provide the exact color information and therefore multi-layer network is really not called for. Nonetheless, it is interesting to see that the network generalizes very well and does not show any sign of over fitting (more about this later in the result section). Below, I have draw the computational graph of the color classification network. Because the color classifier is nothing more than a few cascading blocks with a convolutional layer + batch norm + non-linearity layer (ReLU) + max pooling, the graph looks very neat.</p> <p><img src="/assets/images/toy/models/color_classifier.png" alt="Picture description" height="1400px" /></p> <center>color classifier</center> <p></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">ColorClassifier</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">feature_size</span><span class="p">,</span> <span class="n">num_features_in</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">80</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">ColorClassifier</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        
        <span class="bp">self</span><span class="p">.</span><span class="n">num_classes</span> <span class="o">=</span> <span class="n">num_classes</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">act</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        
        <span class="bp">self</span><span class="p">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">num_features_in</span><span class="p">,</span> <span class="n">feature_size</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">bn1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="n">feature_size</span><span class="p">)</span>
        
        <span class="bp">self</span><span class="p">.</span><span class="n">maxpool</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        
        <span class="bp">self</span><span class="p">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">feature_size</span><span class="p">,</span> <span class="n">feature_size</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">bn2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="n">feature_size</span><span class="p">)</span>        
        
        <span class="bp">self</span><span class="p">.</span><span class="n">conv3</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">feature_size</span><span class="p">,</span> <span class="n">feature_size</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">bn3</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="n">feature_size</span><span class="p">)</span>
        
      
        
        <span class="bp">self</span><span class="p">.</span><span class="n">output</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">3</span><span class="o">*</span><span class="mi">3</span><span class="o">*</span><span class="mi">3</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">)</span>
        
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">bn1</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">act</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">maxpool</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">bn2</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">act</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">maxpool</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        
        
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">conv3</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">bn3</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">act</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">maxpool</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        
        <span class="n">out</span> <span class="o">=</span> <span class="n">out</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="n">out</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">output</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">out</span>
</code></pre></div></div> <p>Please feel free to play around with the code in my <a href="https://github.com/WilsonFok2020/object-detection/">repository.</a></p> <h3 id="reference">Reference</h3> <p>Parts of the code are adapted from the following resources:</p> <ul> <li>https://lilianweng.github.io/lil-log/2018/12/27/object-detection-part-4.html</li> <li>https://www.jeremyjordan.me/object-detection-one-stage/</li> <li>https://github.com/signatrix/efficientdet</li> <li>http://pjreddie.com/yolo9000/</li> <li>https://github.com/yhenon/pytorch-retinanet</li> </ul> </div> <div id="disqus_thread"></div> </article> <article class="card" itemscope itemtype="http://schema.org/BlogPosting"> <div class="card-body" itemprop="articleBody"> <link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet"> <div class="card-header"> <span class="title"> <i class="fa fa-share"></i> Share this to: </span> </div> <div id="share-bar"> <div class="share-buttons"> <a href="https://www.facebook.com/sharer/sharer.php?u=/deep_learning/2020/08/02/Learning_Object_Detection_part3/" onclick="window.open(this.href, 'pop-up', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Facebook" > <i class="fa fa-facebook-official share-button"> facebook</i> </a> <a href="https://twitter.com/intent/tweet?text=Learning object detection part 3 - network training and evaluation&url=/deep_learning/2020/08/02/Learning_Object_Detection_part3/" onclick="window.open(this.href, 'pop-up', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Twitter" > <i class="fa fa-twitter share-button"> twitter</i> </a> <a href="https://plus.google.com/share?url=/deep_learning/2020/08/02/Learning_Object_Detection_part3/" onclick="window.open(this.href, 'pop-up', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Google+" > <i class="fa fa-google-plus share-button"> google</i> </a> <a href="https://www.pinterest.com/pin/create/button/?url=/deep_learning/2020/08/02/Learning_Object_Detection_part3/" onclick="window.open(this.href, 'pop-up', 'left=20,top=20,width=900,height=500,toolbar=1,resizable=0'); return false;" title="Share on Pinterest" > <i class="fa fa-pinterest-p share-button"> pinterest</i> </a> <a href="https://www.tumblr.com/share/link?url=/deep_learning/2020/08/02/Learning_Object_Detection_part3/" onclick="window.open(this.href, 'pop-up', 'left=20,top=20,width=900,height=500,toolbar=1,resizable=0'); return false;" title="Share on Tumblr" > <i class="fa fa-tumblr share-button"> tumblr</i> </a> <a href="http://www.reddit.com/submit?url=/deep_learning/2020/08/02/Learning_Object_Detection_part3/" onclick="window.open(this.href, 'pop-up', 'left=20,top=20,width=900,height=500,toolbar=1,resizable=0'); return false;" title="Share on Reddit" > <i class="fa fa-reddit-alien share-button"> reddit</i> </a> <a href="https://www.linkedin.com/shareArticle?mini=true&url=/deep_learning/2020/08/02/Learning_Object_Detection_part3/&title=Learning object detection part 3 - network training and evaluation&summary=&source=Wilson Fok" onclick="window.open(this.href, 'pop-up', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn" > <i class="fa fa-linkedin share-button"> linkedin</i> </a> <a href="mailto:?subject=Learning object detection part 3 - network training and evaluation&amp;body=Check out this site /deep_learning/2020/08/02/Learning_Object_Detection_part3/" title="Share via Email" > <i class="fa fa-envelope share-button"> email</i> </a> </div> </div> </div> </article> <script> var disqus_config = function () { this.page.url = "/deep_learning/2020/08/02/Learning_Object_Detection_part3/"; /* Replace PAGE_URL with your page's canonical URL variable */ this.page.identifier = "/deep_learning/2020/08/02/Learning_Object_Detection_part3"; /* Replace PAGE_IDENTIFIER with your page's unique identifier variable */ }; (function () { /* DON'T EDIT BELOW THIS LINE */ var d = document, s = d.createElement('script'); s.src = 'https://.disqus.com/embed.js'; s.setAttribute('data-timestamp', +new Date()); (d.head || d.body).appendChild(s); })(); </script> <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a> </noscript></div> </div> <!-- End of row--> <div class="row"> <div class="col-md-4"> <div class="card"> <div class="card-header"> About </div> <div class="card-body"> <!-- Your Bio --> <p class="author_bio"> Hello, My name is Wilson Fok. I love to extract useful insights and knowledge from big data. Constructive feedback and insightful comments are very welcome!</p> </div> </div> </div> <div class="col-md-4"> <div class="card"> <div class="card-header">Categories </div> <div class="card-body text-dark"> <div id="#Finance"></div> <li class="tag-head"> <a href="/blog/categories/Finance">Finance</a> </li> <a name="Finance"></a> <div id="#NLP"></div> <li class="tag-head"> <a href="/blog/categories/NLP">NLP</a> </li> <a name="NLP"></a> <div id="#Deep_Learning"></div> <li class="tag-head"> <a href="/blog/categories/Deep_Learning">Deep_Learning</a> </li> <a name="Deep_Learning"></a> <div id="#Others"></div> <li class="tag-head"> <a href="/blog/categories/Others">Others</a> </li> <a name="Others"></a> <div id="#Reading"></div> <li class="tag-head"> <a href="/blog/categories/Reading">Reading</a> </li> <a name="Reading"></a> <div id="#Toastmasters"></div> <li class="tag-head"> <a href="/blog/categories/Toastmasters">Toastmasters</a> </li> <a name="Toastmasters"></a> </div> </div> </div> <div class="col-md-4"> <div class="card"> <div class="card-header">Useful Links </div> <div class="card-body text-dark"> <li > <a href="/about">About Me</a> </li> <li > <a href="/blog">Blog</a> </li> <li > <a href="/blog/categories">Categories</a> </li> <li > <a href="/gallery">Gallery</a> </li> <li > <a href="/contact">Contact Me</a> </li> </div> </div> </div> </div> </div> <footer> <p> Powered by Jekyll. Hosted on <a href="https://pages.github.com">Github</a>. Subscribe via <a href=" /feed.xml ">RSS <i class="fa fa-rss" aria-hidden="true"></i> </a> </p> </footer> </div> <script> var options = { classname: 'my-class', id: 'my-id' }; var nanobar = new Nanobar( options ); nanobar.go( 30 ); nanobar.go( 76 ); nanobar.go(100); </script> <!-- <div hidden id="snipcart" data-api-key="Y2I1NTAyNWYtMTNkMy00ODg0LWE4NDItNTZhYzUxNzJkZTI5NjM3MDI4NTUzNzYyMjQ4NzU0"></div> <script src="https://cdn.snipcart.com/themes/v3.0.0-beta.3/default/snipcart.js" defer></script> --> <script src="/assets/js/mode-switcher.js"></script> <script src="/assets/js/slideshow.js"></script> </body> </html>
