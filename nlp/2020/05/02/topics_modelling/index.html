<!DOCTYPE html> <html lang=" en "><head> <meta charset="utf-8"> <!-- Begin Jekyll SEO tag v2.8.0 --> <title>Topic modeling on COVID-19 publications by NLP | Wilson Fok</title> <meta name="generator" content="Jekyll v3.9.3" /> <meta property="og:title" content="Topic modeling on COVID-19 publications by NLP" /> <meta name="author" content="Wilson Fok" /> <meta property="og:locale" content="en_US" /> <meta name="description" content="The ongoing COVID-19 pandemic has had a great impact on human society and how we treat each other. To equip ourselves with the latest knowledge on COVID-19, I have explored the use of natural language processing techniques to assist us in understanding vast volume of published research results, retrieving relevant articles, and summarizing meaning and insights from scientists around the globe. For those who would like to take on the challenge, I encourage you to visit COVID-19-research-challenge for more detailed information. The study of natural language processing techniques has been an ongoing effort for decades. Like the study of computer vision, it has benefited immensely from many recent advances in deep learning and artificial neural networks. However, this post uses only relatively simple and classic natural language processing techniques as these techniques are already quite good and fast at revealing topics and themes in the COVID-19 literature. Specifically, I am going to deploy standard tools to “clean up” the texts in these articles and use Latent Dirichlet Allocation to identify underlying their common themes. Text preprocessing I take the meta data, which is a csv spreadsheet containing detailed information on each article from Kaggle. I keep the data consistent by removing those without titles or abstracts. In total, I am left with 46K articles. def load_data(): df = pd.read_csv(&#39;../covid/metadata.csv&#39;) mask = df[&#39;abstract&#39;].notnull() mask2 = df[&#39;title&#39;].notnull() mask3 = np.all([mask, mask2], axis=0) df = df[[&#39;title&#39;, &#39;abstract&#39;, &#39;publish_time&#39;, &#39;authors&#39;]] df = df[mask3] data_list = {} for column in df.columns: data_list[column] = df[column].values.tolist() return data_list The English language controls the flow and coherence of texts using stop-words which do not provide much semantic meaning on their own. Additionally, words, predominantly verbs and nouns such as singular and plural, come with different variants depending on the tense or grammatical rules. The way to undo and restore the word to its basic form is called to lemmatize a word. LancasterStemmer seems too aggressive in reducing words to their stems, and after stemming the variation of word meaning may be lost (see examples below). Finally, it is more convenient to work just with lower cases. To this end, I run the following functions to clean up the texts. Example 1 original [&#39;sequence&#39;, &#39;requirements&#39;, &#39;for&#39;, &#39;rna&#39;, &#39;strand&#39;, &#39;transfer&#39;, &#39;during&#39;, &#39;nidovirus&#39;, &#39;discontinuous&#39;, &#39;subgenomic&#39;, &#39;rna&#39;, &#39;synthesis&#39;] no stopword [&#39;sequence&#39;, &#39;requirements&#39;, &#39;rna&#39;, &#39;strand&#39;, &#39;transfer&#39;, &#39;nidovirus&#39;, &#39;discontinuous&#39;, &#39;subgenomic&#39;, &#39;rna&#39;, &#39;synthesis&#39;] WordNetLemmatizer [&#39;sequence&#39;, &#39;requirement&#39;, &#39;rna&#39;, &#39;strand&#39;, &#39;transfer&#39;, &#39;nidovirus&#39;, &#39;discontinuous&#39;, &#39;subgenomic&#39;, &#39;rna&#39;, &#39;synthesis&#39;] LancasterStemmer [&#39;sequ&#39;, &#39;requir&#39;, &#39;rna&#39;, &#39;strand&#39;, &#39;transf&#39;, &#39;nidovir&#39;, &#39;discontinu&#39;, &#39;subgenom&#39;, &#39;rna&#39;, &#39;synthes&#39;] Example 2 original [&#39;healthcare&#39;, &#39;workers&#39;, &#39;willingness&#39;, &#39;to&#39;, &#39;work&#39;, &#39;during&#39;, &#39;an&#39;, &#39;influenza&#39;, &#39;pandemic&#39;, &#39;systematic&#39;, &#39;review&#39;, &#39;and&#39;, &#39;meta&#39;, &#39;analysis&#39;] no stopword [&#39;healthcare&#39;, &#39;workers&#39;, &#39;willingness&#39;, &#39;work&#39;, &#39;influenza&#39;, &#39;pandemic&#39;, &#39;systematic&#39;, &#39;review&#39;, &#39;meta&#39;, &#39;analysis&#39;] WordNetLemmatizer [&#39;healthcare&#39;, &#39;worker&#39;, &#39;willingness&#39;, &#39;work&#39;, &#39;influenza&#39;, &#39;pandemic&#39;, &#39;systematic&#39;, &#39;review&#39;, &#39;meta&#39;, &#39;analysis&#39;] LancasterStemmer [&#39;healthc&#39;, &#39;work&#39;, &#39;wil&#39;, &#39;work&#39;, &#39;influenz&#39;, &#39;pandem&#39;, &#39;system&#39;, &#39;review&#39;, &#39;met&#39;, &#39;analys&#39;] def lower_remove_stop_word_lemmatize(data_list, column, printout_freq=5000): TOKEN = re.compile(r&#39;\b\w{2,}\b&#39;) # &quot;Naive&quot; token similar to that used by sklearn from nltk.corpus import stopwords wnl = nltk.WordNetLemmatizer() no_stop_word_list = [] for i, item in enumerate(data_list[column]): tokens = TOKEN.findall(item) tokens = [token.lower() for token in tokens] if i % printout_freq == 0: print (len(tokens), tokens) tokens = [token for token in tokens if token not in stopwords.words(&#39;english&#39;)] if i % printout_freq == 0: print (len(tokens), tokens) tokens = [wnl.lemmatize(token) for token in tokens] if i % printout_freq == 0: print (len(tokens), tokens) print () no_stop_word_list.append(tokens) return no_stop_word_list A majority of the words in the texts do not occur regularly in these articles. I drop them out based on their occurrence in the entire texts of titles. I like defaultdict because if we do not include certain words, the defaultdict would automatically throws a count value of zero without raising KeyError as in the case of “normal” dict. I have looked at the words distribution between the titles and abstracts. From what I have seen, the words in the titles are far more informative than the words in the abstracts as far as identifying hidden topics in these documents. Being a longer passage of texts, the abstracts contain far more “less meaningful” words. This is in part due to the stop-words list not being comprehensive enough. For instance, it hasn’t filtered out words such as “also”. # Count word frequencies frequency = defaultdict(int) for text in data_list[&#39;title&#39;]: for token in text: frequency[token] += 1 # Only keep words that appear more often processed_corpus = [[token for token in text if frequency[token] &gt; 60] for text in data_list[&#39;abstract&#39;]] # pprint.pprint(processed_corpus) from wordcloud import WordCloud text = &#39; &#39;.join(flatten_list(processed_corpus)) # Generate a word cloud image wordcloud = WordCloud(width=800, height=400, max_font_size=50, max_words=500, background_color=&quot;white&quot;).generate(text) plt.figure() plt.imshow(wordcloud, interpolation=&#39;bilinear&#39;) plt.axis(&quot;off&quot;) plt.savefig(os.path.join(output_dir, &#39;processed_corpus_abstract.png&#39;), transparent=True , dpi=400, bbox_inches=&#39;tight&#39;) plt.close() How to build topic models using Gensim? Building topic models using Gensim is very easy. Here is how I do it. Gensim is especially helpful because it puts memory usage and computational efficiency at heart. Models.LdaModel and its variants trains on the corpus incrementally, e.g. 10k at one time. This makes training feasible on less resourceful computers. ############ Gensim dictionary = corpora.Dictionary(processed_corpus) print(dictionary) bow_corpus = [dictionary.doc2bow(text) for text in processed_corpus] # train the model tfidf = models.TfidfModel(bow_corpus) corpus_tfidf = tfidf[bow_corpus] num_topics = 50 topic_model = models.LdaModel(corpus_tfidf, id2word=dictionary, num_topics=num_topics) # topic_model = models.RpModel(corpus_tfidf, num_topics=num_topics) # topic_model = models.HdpModel(corpus_tfidf, id2word=dictionary) # topic_model = models.LsiModel(corpus_tfidf, # id2word=dictionary, # num_topics=num_topics) # initialize an LSI transformation topic_model.print_topics(num_topics) # both bow-&gt;tfidf and tfidf-&gt;lsi transformations are actually executed here, on the fly corpus_lsi = topic_model[corpus_tfidf] # create a double wrapper over the original corpus: bow-&gt;tfidf-&gt;fold-in-lsi topic_words = {} for i in range(num_topics): topic_words[i] = topic_model.show_topic(i) Choosing the right number of topics is not intuitive and is often a process of trial and error. For instance, 50 topics work fine in this example and I have tried a range of other numbers of topics. Since documents on different topics have different word distributions and word frequency, figuring out how to weight these words in each document is the key to identifying hidden topics amongst the documents. In essence, this is what LDA tries to do by solving for the word weights with linear algebra. The importance of words to a topic is expressed by the word weights. For example, LDA identifies topic 49th and topic 12th with the following words and their weights. I think topic 12th concerns about evolution of the virus whereas topic 49th seems to be about animal infections. 12: [(&#39;bat&#39;, 0.06349142), (&#39;specie&#39;, 0.031821612), (&#39;evolution&#39;, 0.027451487), (&#39;host&#39;, 0.025193721), (&#39;genetics&#39;, 0.019700462), (&#39;genetic&#39;, 0.017949674), (&#39;virus&#39;, 0.016766567), (&#39;evolutionary&#39;, 0.016061012), (&#39;mutation&#39;, 0.015995547), (&#39;rodent&#39;, 0.015365632)], 49: [(&#39;dog&#39;, 0.07506799), (&#39;canine&#39;, 0.05088224), (&#39;2020&#39;, 0.04714561), (&#39;chicken&#39;, 0.036831263), (&#39;strain&#39;, 0.029606815), (&#39;s1&#39;, 0.028369702), (&#39;bronchitis&#39;, 0.023845471), (&#39;peritonitis&#39;, 0.019557731), (&#39;vaccine&#39;, 0.013181845), (&#39;infectious&#39;, 0.012176985)]} How to get a sense of what these topics are about? Let’s take a look at what the documents say when they are likely to have come from several topics. LDA describes each document as a mixture of words taken out from an array of topics. Thus, each document naturally spreads across several topics. To really understanding what a single topic is about, the easiest way is to go through all the documents and find those that exhibit only a single topic or are heavily biased towards one topic. To visualize these topics, I further cluster them into bigger but fewer overarching topics. from sklearn.cluster import MiniBatchKMeans n_clusters = 7 batch_size = 3000 kmeans = MiniBatchKMeans(n_clusters=n_clusters, batch_size=batch_size, verbose=0) counter = 0 x = np.zeros((batch_size, num_topics), dtype=np.float16) # use only dominant documents for each topics dominant_threshold = 0.7 for i, doc in enumerate(corpus_lsi): if counter % batch_size == 0 and counter &gt; 0: print (&#39;fit and reset&#39;) kmeans = kmeans.partial_fit(x) counter = 0 x = np.zeros((batch_size, num_topics), dtype=np.float16) else: col = [item[0] for item in doc] v = [item[1] for item in doc] # dominant document if max(v) &gt; dominant_threshold: x[counter, col] = v counter = counter + 1 # break print (&#39;kmeans.cluster_centers_ {}&#39;.format(kmeans.cluster_centers_)) labels = defaultdict(list) x = np.zeros((1, num_topics), dtype=np.float16) for i, doc in enumerate(corpus_lsi): col = [item[0] for item in doc] v = [item[1] for item in doc] x[0, col] = v label = kmeans.predict(x) labels[label[0]].append(i) for key, value in labels.items(): print (key, len(value)) # look at some documents max_num_samples = 300 # randomly select some samples sample_set = [] for key, values in labels.items(): if len(values) &lt; max_num_samples: s = values else: s = random.sample(values, k=max_num_samples) sample_set += s sample_set = list(set(sample_set)) # no duplicates ns = len(sample_set) print (ns, len(labels)*max_num_samples) X_samples_order = {i:value for i, value in enumerate(sample_set)} samples_X_order = {value:i for i, value in enumerate(sample_set)} vectors = np.zeros((ns, num_topics), dtype=np.float16) selected_titles = {} selected_abstracts = {} for i, (doc, abstract, title) in enumerate(zip(corpus_lsi, data_list[&#39;abstract&#39;], data_list[&#39;title&#39;])): insert_loc = samples_X_order.get(i, None) if insert_loc is not None: # print (insert_loc, i, doc, as_text) col = [item[0] for item in doc] v = [item[1] for item in doc] vectors[insert_loc, col] = v selected_titles[i] = title selected_abstracts[i] = abstract samples_labels = kmeans.predict(vectors) # show the clusters of topics num_dimensions = 2 tsne = TSNE(n_components=num_dimensions) lower_vectors = tsne.fit_transform(vectors) import matplotlib cmap = matplotlib.cm.get_cmap(&#39;tab20b&#39;) vmin = min(samples_labels) vmax = max(samples_labels) plt.figure(figsize=(12, 12)) colors = [&#39;black&#39;, &#39;purple&#39;, &#39;blue&#39;, &#39;green&#39;, &#39;orange&#39;,&#39;red&#39;,&#39;yellow&#39;] colormap_names = [&#39;Greys&#39;, &#39;Purples&#39;, &#39;Blues&#39;, &#39;Greens&#39;, &#39;Oranges&#39;, &#39;Reds&#39;, &#39;spring&#39;] for i in range(n_clusters): where = samples_labels == i plt.scatter(lower_vectors[where,0], lower_vectors[where,1], c=colors[i]) # plt.colorbar() plt.axis(&quot;off&quot;) plt.savefig(os.path.join(output_dir, &#39;tsne_dominant_documents_topics.png&#39;), transparent=True , dpi=400, bbox_inches=&#39;tight&#39;) plt.show() MiniBatchKMeans has been very useful because it allows incremental fitting of the document vectors, a method that drains less computational resources but achieves similar results to KMeans. TSNE is used to visualize the topic space in 2D. For each cluster, I have drawn its wordcloud (sharing the same color theme)." /> <meta property="og:description" content="The ongoing COVID-19 pandemic has had a great impact on human society and how we treat each other. To equip ourselves with the latest knowledge on COVID-19, I have explored the use of natural language processing techniques to assist us in understanding vast volume of published research results, retrieving relevant articles, and summarizing meaning and insights from scientists around the globe. For those who would like to take on the challenge, I encourage you to visit COVID-19-research-challenge for more detailed information. The study of natural language processing techniques has been an ongoing effort for decades. Like the study of computer vision, it has benefited immensely from many recent advances in deep learning and artificial neural networks. However, this post uses only relatively simple and classic natural language processing techniques as these techniques are already quite good and fast at revealing topics and themes in the COVID-19 literature. Specifically, I am going to deploy standard tools to “clean up” the texts in these articles and use Latent Dirichlet Allocation to identify underlying their common themes. Text preprocessing I take the meta data, which is a csv spreadsheet containing detailed information on each article from Kaggle. I keep the data consistent by removing those without titles or abstracts. In total, I am left with 46K articles. def load_data(): df = pd.read_csv(&#39;../covid/metadata.csv&#39;) mask = df[&#39;abstract&#39;].notnull() mask2 = df[&#39;title&#39;].notnull() mask3 = np.all([mask, mask2], axis=0) df = df[[&#39;title&#39;, &#39;abstract&#39;, &#39;publish_time&#39;, &#39;authors&#39;]] df = df[mask3] data_list = {} for column in df.columns: data_list[column] = df[column].values.tolist() return data_list The English language controls the flow and coherence of texts using stop-words which do not provide much semantic meaning on their own. Additionally, words, predominantly verbs and nouns such as singular and plural, come with different variants depending on the tense or grammatical rules. The way to undo and restore the word to its basic form is called to lemmatize a word. LancasterStemmer seems too aggressive in reducing words to their stems, and after stemming the variation of word meaning may be lost (see examples below). Finally, it is more convenient to work just with lower cases. To this end, I run the following functions to clean up the texts. Example 1 original [&#39;sequence&#39;, &#39;requirements&#39;, &#39;for&#39;, &#39;rna&#39;, &#39;strand&#39;, &#39;transfer&#39;, &#39;during&#39;, &#39;nidovirus&#39;, &#39;discontinuous&#39;, &#39;subgenomic&#39;, &#39;rna&#39;, &#39;synthesis&#39;] no stopword [&#39;sequence&#39;, &#39;requirements&#39;, &#39;rna&#39;, &#39;strand&#39;, &#39;transfer&#39;, &#39;nidovirus&#39;, &#39;discontinuous&#39;, &#39;subgenomic&#39;, &#39;rna&#39;, &#39;synthesis&#39;] WordNetLemmatizer [&#39;sequence&#39;, &#39;requirement&#39;, &#39;rna&#39;, &#39;strand&#39;, &#39;transfer&#39;, &#39;nidovirus&#39;, &#39;discontinuous&#39;, &#39;subgenomic&#39;, &#39;rna&#39;, &#39;synthesis&#39;] LancasterStemmer [&#39;sequ&#39;, &#39;requir&#39;, &#39;rna&#39;, &#39;strand&#39;, &#39;transf&#39;, &#39;nidovir&#39;, &#39;discontinu&#39;, &#39;subgenom&#39;, &#39;rna&#39;, &#39;synthes&#39;] Example 2 original [&#39;healthcare&#39;, &#39;workers&#39;, &#39;willingness&#39;, &#39;to&#39;, &#39;work&#39;, &#39;during&#39;, &#39;an&#39;, &#39;influenza&#39;, &#39;pandemic&#39;, &#39;systematic&#39;, &#39;review&#39;, &#39;and&#39;, &#39;meta&#39;, &#39;analysis&#39;] no stopword [&#39;healthcare&#39;, &#39;workers&#39;, &#39;willingness&#39;, &#39;work&#39;, &#39;influenza&#39;, &#39;pandemic&#39;, &#39;systematic&#39;, &#39;review&#39;, &#39;meta&#39;, &#39;analysis&#39;] WordNetLemmatizer [&#39;healthcare&#39;, &#39;worker&#39;, &#39;willingness&#39;, &#39;work&#39;, &#39;influenza&#39;, &#39;pandemic&#39;, &#39;systematic&#39;, &#39;review&#39;, &#39;meta&#39;, &#39;analysis&#39;] LancasterStemmer [&#39;healthc&#39;, &#39;work&#39;, &#39;wil&#39;, &#39;work&#39;, &#39;influenz&#39;, &#39;pandem&#39;, &#39;system&#39;, &#39;review&#39;, &#39;met&#39;, &#39;analys&#39;] def lower_remove_stop_word_lemmatize(data_list, column, printout_freq=5000): TOKEN = re.compile(r&#39;\b\w{2,}\b&#39;) # &quot;Naive&quot; token similar to that used by sklearn from nltk.corpus import stopwords wnl = nltk.WordNetLemmatizer() no_stop_word_list = [] for i, item in enumerate(data_list[column]): tokens = TOKEN.findall(item) tokens = [token.lower() for token in tokens] if i % printout_freq == 0: print (len(tokens), tokens) tokens = [token for token in tokens if token not in stopwords.words(&#39;english&#39;)] if i % printout_freq == 0: print (len(tokens), tokens) tokens = [wnl.lemmatize(token) for token in tokens] if i % printout_freq == 0: print (len(tokens), tokens) print () no_stop_word_list.append(tokens) return no_stop_word_list A majority of the words in the texts do not occur regularly in these articles. I drop them out based on their occurrence in the entire texts of titles. I like defaultdict because if we do not include certain words, the defaultdict would automatically throws a count value of zero without raising KeyError as in the case of “normal” dict. I have looked at the words distribution between the titles and abstracts. From what I have seen, the words in the titles are far more informative than the words in the abstracts as far as identifying hidden topics in these documents. Being a longer passage of texts, the abstracts contain far more “less meaningful” words. This is in part due to the stop-words list not being comprehensive enough. For instance, it hasn’t filtered out words such as “also”. # Count word frequencies frequency = defaultdict(int) for text in data_list[&#39;title&#39;]: for token in text: frequency[token] += 1 # Only keep words that appear more often processed_corpus = [[token for token in text if frequency[token] &gt; 60] for text in data_list[&#39;abstract&#39;]] # pprint.pprint(processed_corpus) from wordcloud import WordCloud text = &#39; &#39;.join(flatten_list(processed_corpus)) # Generate a word cloud image wordcloud = WordCloud(width=800, height=400, max_font_size=50, max_words=500, background_color=&quot;white&quot;).generate(text) plt.figure() plt.imshow(wordcloud, interpolation=&#39;bilinear&#39;) plt.axis(&quot;off&quot;) plt.savefig(os.path.join(output_dir, &#39;processed_corpus_abstract.png&#39;), transparent=True , dpi=400, bbox_inches=&#39;tight&#39;) plt.close() How to build topic models using Gensim? Building topic models using Gensim is very easy. Here is how I do it. Gensim is especially helpful because it puts memory usage and computational efficiency at heart. Models.LdaModel and its variants trains on the corpus incrementally, e.g. 10k at one time. This makes training feasible on less resourceful computers. ############ Gensim dictionary = corpora.Dictionary(processed_corpus) print(dictionary) bow_corpus = [dictionary.doc2bow(text) for text in processed_corpus] # train the model tfidf = models.TfidfModel(bow_corpus) corpus_tfidf = tfidf[bow_corpus] num_topics = 50 topic_model = models.LdaModel(corpus_tfidf, id2word=dictionary, num_topics=num_topics) # topic_model = models.RpModel(corpus_tfidf, num_topics=num_topics) # topic_model = models.HdpModel(corpus_tfidf, id2word=dictionary) # topic_model = models.LsiModel(corpus_tfidf, # id2word=dictionary, # num_topics=num_topics) # initialize an LSI transformation topic_model.print_topics(num_topics) # both bow-&gt;tfidf and tfidf-&gt;lsi transformations are actually executed here, on the fly corpus_lsi = topic_model[corpus_tfidf] # create a double wrapper over the original corpus: bow-&gt;tfidf-&gt;fold-in-lsi topic_words = {} for i in range(num_topics): topic_words[i] = topic_model.show_topic(i) Choosing the right number of topics is not intuitive and is often a process of trial and error. For instance, 50 topics work fine in this example and I have tried a range of other numbers of topics. Since documents on different topics have different word distributions and word frequency, figuring out how to weight these words in each document is the key to identifying hidden topics amongst the documents. In essence, this is what LDA tries to do by solving for the word weights with linear algebra. The importance of words to a topic is expressed by the word weights. For example, LDA identifies topic 49th and topic 12th with the following words and their weights. I think topic 12th concerns about evolution of the virus whereas topic 49th seems to be about animal infections. 12: [(&#39;bat&#39;, 0.06349142), (&#39;specie&#39;, 0.031821612), (&#39;evolution&#39;, 0.027451487), (&#39;host&#39;, 0.025193721), (&#39;genetics&#39;, 0.019700462), (&#39;genetic&#39;, 0.017949674), (&#39;virus&#39;, 0.016766567), (&#39;evolutionary&#39;, 0.016061012), (&#39;mutation&#39;, 0.015995547), (&#39;rodent&#39;, 0.015365632)], 49: [(&#39;dog&#39;, 0.07506799), (&#39;canine&#39;, 0.05088224), (&#39;2020&#39;, 0.04714561), (&#39;chicken&#39;, 0.036831263), (&#39;strain&#39;, 0.029606815), (&#39;s1&#39;, 0.028369702), (&#39;bronchitis&#39;, 0.023845471), (&#39;peritonitis&#39;, 0.019557731), (&#39;vaccine&#39;, 0.013181845), (&#39;infectious&#39;, 0.012176985)]} How to get a sense of what these topics are about? Let’s take a look at what the documents say when they are likely to have come from several topics. LDA describes each document as a mixture of words taken out from an array of topics. Thus, each document naturally spreads across several topics. To really understanding what a single topic is about, the easiest way is to go through all the documents and find those that exhibit only a single topic or are heavily biased towards one topic. To visualize these topics, I further cluster them into bigger but fewer overarching topics. from sklearn.cluster import MiniBatchKMeans n_clusters = 7 batch_size = 3000 kmeans = MiniBatchKMeans(n_clusters=n_clusters, batch_size=batch_size, verbose=0) counter = 0 x = np.zeros((batch_size, num_topics), dtype=np.float16) # use only dominant documents for each topics dominant_threshold = 0.7 for i, doc in enumerate(corpus_lsi): if counter % batch_size == 0 and counter &gt; 0: print (&#39;fit and reset&#39;) kmeans = kmeans.partial_fit(x) counter = 0 x = np.zeros((batch_size, num_topics), dtype=np.float16) else: col = [item[0] for item in doc] v = [item[1] for item in doc] # dominant document if max(v) &gt; dominant_threshold: x[counter, col] = v counter = counter + 1 # break print (&#39;kmeans.cluster_centers_ {}&#39;.format(kmeans.cluster_centers_)) labels = defaultdict(list) x = np.zeros((1, num_topics), dtype=np.float16) for i, doc in enumerate(corpus_lsi): col = [item[0] for item in doc] v = [item[1] for item in doc] x[0, col] = v label = kmeans.predict(x) labels[label[0]].append(i) for key, value in labels.items(): print (key, len(value)) # look at some documents max_num_samples = 300 # randomly select some samples sample_set = [] for key, values in labels.items(): if len(values) &lt; max_num_samples: s = values else: s = random.sample(values, k=max_num_samples) sample_set += s sample_set = list(set(sample_set)) # no duplicates ns = len(sample_set) print (ns, len(labels)*max_num_samples) X_samples_order = {i:value for i, value in enumerate(sample_set)} samples_X_order = {value:i for i, value in enumerate(sample_set)} vectors = np.zeros((ns, num_topics), dtype=np.float16) selected_titles = {} selected_abstracts = {} for i, (doc, abstract, title) in enumerate(zip(corpus_lsi, data_list[&#39;abstract&#39;], data_list[&#39;title&#39;])): insert_loc = samples_X_order.get(i, None) if insert_loc is not None: # print (insert_loc, i, doc, as_text) col = [item[0] for item in doc] v = [item[1] for item in doc] vectors[insert_loc, col] = v selected_titles[i] = title selected_abstracts[i] = abstract samples_labels = kmeans.predict(vectors) # show the clusters of topics num_dimensions = 2 tsne = TSNE(n_components=num_dimensions) lower_vectors = tsne.fit_transform(vectors) import matplotlib cmap = matplotlib.cm.get_cmap(&#39;tab20b&#39;) vmin = min(samples_labels) vmax = max(samples_labels) plt.figure(figsize=(12, 12)) colors = [&#39;black&#39;, &#39;purple&#39;, &#39;blue&#39;, &#39;green&#39;, &#39;orange&#39;,&#39;red&#39;,&#39;yellow&#39;] colormap_names = [&#39;Greys&#39;, &#39;Purples&#39;, &#39;Blues&#39;, &#39;Greens&#39;, &#39;Oranges&#39;, &#39;Reds&#39;, &#39;spring&#39;] for i in range(n_clusters): where = samples_labels == i plt.scatter(lower_vectors[where,0], lower_vectors[where,1], c=colors[i]) # plt.colorbar() plt.axis(&quot;off&quot;) plt.savefig(os.path.join(output_dir, &#39;tsne_dominant_documents_topics.png&#39;), transparent=True , dpi=400, bbox_inches=&#39;tight&#39;) plt.show() MiniBatchKMeans has been very useful because it allows incremental fitting of the document vectors, a method that drains less computational resources but achieves similar results to KMeans. TSNE is used to visualize the topic space in 2D. For each cluster, I have drawn its wordcloud (sharing the same color theme)." /> <link rel="canonical" href="/nlp/2020/05/02/topics_modelling/" /> <meta property="og:url" content="/nlp/2020/05/02/topics_modelling/" /> <meta property="og:site_name" content="Wilson Fok" /> <meta property="og:image" content="/assets/images/covid/processed_corpus_abstract.png" /> <meta property="og:type" content="article" /> <meta property="article:published_time" content="2020-05-02T00:00:00+08:00" /> <meta name="twitter:card" content="summary_large_image" /> <meta property="twitter:image" content="/assets/images/covid/processed_corpus_abstract.png" /> <meta property="twitter:title" content="Topic modeling on COVID-19 publications by NLP" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Wilson Fok"},"dateModified":"2020-05-02T00:00:00+08:00","datePublished":"2020-05-02T00:00:00+08:00","description":"The ongoing COVID-19 pandemic has had a great impact on human society and how we treat each other. To equip ourselves with the latest knowledge on COVID-19, I have explored the use of natural language processing techniques to assist us in understanding vast volume of published research results, retrieving relevant articles, and summarizing meaning and insights from scientists around the globe. For those who would like to take on the challenge, I encourage you to visit COVID-19-research-challenge for more detailed information. The study of natural language processing techniques has been an ongoing effort for decades. Like the study of computer vision, it has benefited immensely from many recent advances in deep learning and artificial neural networks. However, this post uses only relatively simple and classic natural language processing techniques as these techniques are already quite good and fast at revealing topics and themes in the COVID-19 literature. Specifically, I am going to deploy standard tools to “clean up” the texts in these articles and use Latent Dirichlet Allocation to identify underlying their common themes. Text preprocessing I take the meta data, which is a csv spreadsheet containing detailed information on each article from Kaggle. I keep the data consistent by removing those without titles or abstracts. In total, I am left with 46K articles. def load_data(): df = pd.read_csv(&#39;../covid/metadata.csv&#39;) mask = df[&#39;abstract&#39;].notnull() mask2 = df[&#39;title&#39;].notnull() mask3 = np.all([mask, mask2], axis=0) df = df[[&#39;title&#39;, &#39;abstract&#39;, &#39;publish_time&#39;, &#39;authors&#39;]] df = df[mask3] data_list = {} for column in df.columns: data_list[column] = df[column].values.tolist() return data_list The English language controls the flow and coherence of texts using stop-words which do not provide much semantic meaning on their own. Additionally, words, predominantly verbs and nouns such as singular and plural, come with different variants depending on the tense or grammatical rules. The way to undo and restore the word to its basic form is called to lemmatize a word. LancasterStemmer seems too aggressive in reducing words to their stems, and after stemming the variation of word meaning may be lost (see examples below). Finally, it is more convenient to work just with lower cases. To this end, I run the following functions to clean up the texts. Example 1 original [&#39;sequence&#39;, &#39;requirements&#39;, &#39;for&#39;, &#39;rna&#39;, &#39;strand&#39;, &#39;transfer&#39;, &#39;during&#39;, &#39;nidovirus&#39;, &#39;discontinuous&#39;, &#39;subgenomic&#39;, &#39;rna&#39;, &#39;synthesis&#39;] no stopword [&#39;sequence&#39;, &#39;requirements&#39;, &#39;rna&#39;, &#39;strand&#39;, &#39;transfer&#39;, &#39;nidovirus&#39;, &#39;discontinuous&#39;, &#39;subgenomic&#39;, &#39;rna&#39;, &#39;synthesis&#39;] WordNetLemmatizer [&#39;sequence&#39;, &#39;requirement&#39;, &#39;rna&#39;, &#39;strand&#39;, &#39;transfer&#39;, &#39;nidovirus&#39;, &#39;discontinuous&#39;, &#39;subgenomic&#39;, &#39;rna&#39;, &#39;synthesis&#39;] LancasterStemmer [&#39;sequ&#39;, &#39;requir&#39;, &#39;rna&#39;, &#39;strand&#39;, &#39;transf&#39;, &#39;nidovir&#39;, &#39;discontinu&#39;, &#39;subgenom&#39;, &#39;rna&#39;, &#39;synthes&#39;] Example 2 original [&#39;healthcare&#39;, &#39;workers&#39;, &#39;willingness&#39;, &#39;to&#39;, &#39;work&#39;, &#39;during&#39;, &#39;an&#39;, &#39;influenza&#39;, &#39;pandemic&#39;, &#39;systematic&#39;, &#39;review&#39;, &#39;and&#39;, &#39;meta&#39;, &#39;analysis&#39;] no stopword [&#39;healthcare&#39;, &#39;workers&#39;, &#39;willingness&#39;, &#39;work&#39;, &#39;influenza&#39;, &#39;pandemic&#39;, &#39;systematic&#39;, &#39;review&#39;, &#39;meta&#39;, &#39;analysis&#39;] WordNetLemmatizer [&#39;healthcare&#39;, &#39;worker&#39;, &#39;willingness&#39;, &#39;work&#39;, &#39;influenza&#39;, &#39;pandemic&#39;, &#39;systematic&#39;, &#39;review&#39;, &#39;meta&#39;, &#39;analysis&#39;] LancasterStemmer [&#39;healthc&#39;, &#39;work&#39;, &#39;wil&#39;, &#39;work&#39;, &#39;influenz&#39;, &#39;pandem&#39;, &#39;system&#39;, &#39;review&#39;, &#39;met&#39;, &#39;analys&#39;] def lower_remove_stop_word_lemmatize(data_list, column, printout_freq=5000): TOKEN = re.compile(r&#39;\\b\\w{2,}\\b&#39;) # &quot;Naive&quot; token similar to that used by sklearn from nltk.corpus import stopwords wnl = nltk.WordNetLemmatizer() no_stop_word_list = [] for i, item in enumerate(data_list[column]): tokens = TOKEN.findall(item) tokens = [token.lower() for token in tokens] if i % printout_freq == 0: print (len(tokens), tokens) tokens = [token for token in tokens if token not in stopwords.words(&#39;english&#39;)] if i % printout_freq == 0: print (len(tokens), tokens) tokens = [wnl.lemmatize(token) for token in tokens] if i % printout_freq == 0: print (len(tokens), tokens) print () no_stop_word_list.append(tokens) return no_stop_word_list A majority of the words in the texts do not occur regularly in these articles. I drop them out based on their occurrence in the entire texts of titles. I like defaultdict because if we do not include certain words, the defaultdict would automatically throws a count value of zero without raising KeyError as in the case of “normal” dict. I have looked at the words distribution between the titles and abstracts. From what I have seen, the words in the titles are far more informative than the words in the abstracts as far as identifying hidden topics in these documents. Being a longer passage of texts, the abstracts contain far more “less meaningful” words. This is in part due to the stop-words list not being comprehensive enough. For instance, it hasn’t filtered out words such as “also”. # Count word frequencies frequency = defaultdict(int) for text in data_list[&#39;title&#39;]: for token in text: frequency[token] += 1 # Only keep words that appear more often processed_corpus = [[token for token in text if frequency[token] &gt; 60] for text in data_list[&#39;abstract&#39;]] # pprint.pprint(processed_corpus) from wordcloud import WordCloud text = &#39; &#39;.join(flatten_list(processed_corpus)) # Generate a word cloud image wordcloud = WordCloud(width=800, height=400, max_font_size=50, max_words=500, background_color=&quot;white&quot;).generate(text) plt.figure() plt.imshow(wordcloud, interpolation=&#39;bilinear&#39;) plt.axis(&quot;off&quot;) plt.savefig(os.path.join(output_dir, &#39;processed_corpus_abstract.png&#39;), transparent=True , dpi=400, bbox_inches=&#39;tight&#39;) plt.close() How to build topic models using Gensim? Building topic models using Gensim is very easy. Here is how I do it. Gensim is especially helpful because it puts memory usage and computational efficiency at heart. Models.LdaModel and its variants trains on the corpus incrementally, e.g. 10k at one time. This makes training feasible on less resourceful computers. ############ Gensim dictionary = corpora.Dictionary(processed_corpus) print(dictionary) bow_corpus = [dictionary.doc2bow(text) for text in processed_corpus] # train the model tfidf = models.TfidfModel(bow_corpus) corpus_tfidf = tfidf[bow_corpus] num_topics = 50 topic_model = models.LdaModel(corpus_tfidf, id2word=dictionary, num_topics=num_topics) # topic_model = models.RpModel(corpus_tfidf, num_topics=num_topics) # topic_model = models.HdpModel(corpus_tfidf, id2word=dictionary) # topic_model = models.LsiModel(corpus_tfidf, # id2word=dictionary, # num_topics=num_topics) # initialize an LSI transformation topic_model.print_topics(num_topics) # both bow-&gt;tfidf and tfidf-&gt;lsi transformations are actually executed here, on the fly corpus_lsi = topic_model[corpus_tfidf] # create a double wrapper over the original corpus: bow-&gt;tfidf-&gt;fold-in-lsi topic_words = {} for i in range(num_topics): topic_words[i] = topic_model.show_topic(i) Choosing the right number of topics is not intuitive and is often a process of trial and error. For instance, 50 topics work fine in this example and I have tried a range of other numbers of topics. Since documents on different topics have different word distributions and word frequency, figuring out how to weight these words in each document is the key to identifying hidden topics amongst the documents. In essence, this is what LDA tries to do by solving for the word weights with linear algebra. The importance of words to a topic is expressed by the word weights. For example, LDA identifies topic 49th and topic 12th with the following words and their weights. I think topic 12th concerns about evolution of the virus whereas topic 49th seems to be about animal infections. 12: [(&#39;bat&#39;, 0.06349142), (&#39;specie&#39;, 0.031821612), (&#39;evolution&#39;, 0.027451487), (&#39;host&#39;, 0.025193721), (&#39;genetics&#39;, 0.019700462), (&#39;genetic&#39;, 0.017949674), (&#39;virus&#39;, 0.016766567), (&#39;evolutionary&#39;, 0.016061012), (&#39;mutation&#39;, 0.015995547), (&#39;rodent&#39;, 0.015365632)], 49: [(&#39;dog&#39;, 0.07506799), (&#39;canine&#39;, 0.05088224), (&#39;2020&#39;, 0.04714561), (&#39;chicken&#39;, 0.036831263), (&#39;strain&#39;, 0.029606815), (&#39;s1&#39;, 0.028369702), (&#39;bronchitis&#39;, 0.023845471), (&#39;peritonitis&#39;, 0.019557731), (&#39;vaccine&#39;, 0.013181845), (&#39;infectious&#39;, 0.012176985)]} How to get a sense of what these topics are about? Let’s take a look at what the documents say when they are likely to have come from several topics. LDA describes each document as a mixture of words taken out from an array of topics. Thus, each document naturally spreads across several topics. To really understanding what a single topic is about, the easiest way is to go through all the documents and find those that exhibit only a single topic or are heavily biased towards one topic. To visualize these topics, I further cluster them into bigger but fewer overarching topics. from sklearn.cluster import MiniBatchKMeans n_clusters = 7 batch_size = 3000 kmeans = MiniBatchKMeans(n_clusters=n_clusters, batch_size=batch_size, verbose=0) counter = 0 x = np.zeros((batch_size, num_topics), dtype=np.float16) # use only dominant documents for each topics dominant_threshold = 0.7 for i, doc in enumerate(corpus_lsi): if counter % batch_size == 0 and counter &gt; 0: print (&#39;fit and reset&#39;) kmeans = kmeans.partial_fit(x) counter = 0 x = np.zeros((batch_size, num_topics), dtype=np.float16) else: col = [item[0] for item in doc] v = [item[1] for item in doc] # dominant document if max(v) &gt; dominant_threshold: x[counter, col] = v counter = counter + 1 # break print (&#39;kmeans.cluster_centers_ {}&#39;.format(kmeans.cluster_centers_)) labels = defaultdict(list) x = np.zeros((1, num_topics), dtype=np.float16) for i, doc in enumerate(corpus_lsi): col = [item[0] for item in doc] v = [item[1] for item in doc] x[0, col] = v label = kmeans.predict(x) labels[label[0]].append(i) for key, value in labels.items(): print (key, len(value)) # look at some documents max_num_samples = 300 # randomly select some samples sample_set = [] for key, values in labels.items(): if len(values) &lt; max_num_samples: s = values else: s = random.sample(values, k=max_num_samples) sample_set += s sample_set = list(set(sample_set)) # no duplicates ns = len(sample_set) print (ns, len(labels)*max_num_samples) X_samples_order = {i:value for i, value in enumerate(sample_set)} samples_X_order = {value:i for i, value in enumerate(sample_set)} vectors = np.zeros((ns, num_topics), dtype=np.float16) selected_titles = {} selected_abstracts = {} for i, (doc, abstract, title) in enumerate(zip(corpus_lsi, data_list[&#39;abstract&#39;], data_list[&#39;title&#39;])): insert_loc = samples_X_order.get(i, None) if insert_loc is not None: # print (insert_loc, i, doc, as_text) col = [item[0] for item in doc] v = [item[1] for item in doc] vectors[insert_loc, col] = v selected_titles[i] = title selected_abstracts[i] = abstract samples_labels = kmeans.predict(vectors) # show the clusters of topics num_dimensions = 2 tsne = TSNE(n_components=num_dimensions) lower_vectors = tsne.fit_transform(vectors) import matplotlib cmap = matplotlib.cm.get_cmap(&#39;tab20b&#39;) vmin = min(samples_labels) vmax = max(samples_labels) plt.figure(figsize=(12, 12)) colors = [&#39;black&#39;, &#39;purple&#39;, &#39;blue&#39;, &#39;green&#39;, &#39;orange&#39;,&#39;red&#39;,&#39;yellow&#39;] colormap_names = [&#39;Greys&#39;, &#39;Purples&#39;, &#39;Blues&#39;, &#39;Greens&#39;, &#39;Oranges&#39;, &#39;Reds&#39;, &#39;spring&#39;] for i in range(n_clusters): where = samples_labels == i plt.scatter(lower_vectors[where,0], lower_vectors[where,1], c=colors[i]) # plt.colorbar() plt.axis(&quot;off&quot;) plt.savefig(os.path.join(output_dir, &#39;tsne_dominant_documents_topics.png&#39;), transparent=True , dpi=400, bbox_inches=&#39;tight&#39;) plt.show() MiniBatchKMeans has been very useful because it allows incremental fitting of the document vectors, a method that drains less computational resources but achieves similar results to KMeans. TSNE is used to visualize the topic space in 2D. For each cluster, I have drawn its wordcloud (sharing the same color theme).","headline":"Topic modeling on COVID-19 publications by NLP","image":"/assets/images/covid/processed_corpus_abstract.png","mainEntityOfPage":{"@type":"WebPage","@id":"/nlp/2020/05/02/topics_modelling/"},"url":"/nlp/2020/05/02/topics_modelling/"}</script> <!-- End Jekyll SEO tag --> <title>Wilson Fok - A data science enthusiast</title> <meta http-equip="X-UA-Compatible" content="IE=edge"> <meta name="viewport" content="width=device-width, initial-scale=1"> <meta name="description" content="Topic modeling on COVID-19 publications by NLP" /> <meta name="keywords" content="Topic modeling on COVID-19 publications by NLP, Wilson Fok, NLP" /> <link rel="alternate" type="application/rss+xml" title="RSS" href="/feed.xml"> <meta content="" property="fb:app_id"> <meta content="Wilson Fok" property="og:site_name"> <meta content="Topic modeling on COVID-19 publications by NLP" property="og:title"> <meta content="article" property="og:type"> <meta content="Using Python, Gensim,and NLTK to identify research themes and topics on COVID-19" property="og:description"> <meta content="/nlp/2020/05/02/topics_modelling/" property="og:url"> <meta content="2020-05-02T00:00:00+08:00" property="article:published_time"> <meta content="/about/" property="article:author"> <meta content="/assets/img/posts//assets/images/covid/processed_corpus_abstract.png" property="og:image"> <meta content="NLP" property="article:section"> <meta name="twitter:card" content="summary"> <meta name="twitter:site" content="@"> <meta name="twitter:creator" content="@"> <meta name="twitter:title" content="Topic modeling on COVID-19 publications by NLP"> <meta content="Wilson Fok" property="og:site_name"> <meta name="twitter:url" content="/nlp/2020/05/02/topics_modelling/"> <meta name="twitter:description" content="Hello, My name is Wilson Fok. I love to extract useful insights and knowledge from big data. I also like to make new friends and connections. Let's connect! "> <!-- load layout style css --> <link rel="stylesheet" href="/assets/css/main.css" /> <link rel="stylesheet" href="/assets/css/custom-style.css" /> <link rel="stylesheet" href="/assets/bower_components/lightgallery/dist/css/lightgallery.min.css"/> <link rel="stylesheet" href="/assets/bower_components/bootstrap/dist/css/bootstrap.min.css" /> <link rel="stylesheet" href="/assets/bower_components/font-awesome/web-fonts-with-css/css/fontawesome-all.min.css" /> <!-- Favicon --> <link rel="icon" href="/assets/img/favicon.ico" type="image/gif" sizes="16x16"> <!-- Jquery --> <!-- one or more of the below scripts does fancy word animation and dropdown menu --> <script src="/assets/extra_js/jquery-3.4.1.min.js"></script> <script src="/assets/extra_js/picturefill min/picturefill.min.js"></script> <script src="/assets/extra_js/instantsearch min/instantsearch.min.js"></script> <script src="/assets/extra_js/moment min/moment.min.js"></script> <script src="/assets/bower_components/jquery.easing/jquery.easing.min.js"></script> <script src="/assets/bower_components/bootstrap/dist/js/bootstrap.bundle.min.js"></script> <script src="/assets/bower_components/jquery-mousewheel/jquery.mousewheel.min.js"></script> <script src="/assets/bower_components/lightgallery/dist/js/lightgallery-all.min.js"></script> <script src="/assets/bower_components/imagesloaded/imagesloaded.pkgd.min.js"></script> <script src="/assets/bower_components/nanobar/nanobar.min.js"></script> <script src="/assets/bower_components/typewrite/dist/typewrite.min.js"></script> <script type="text/javascript" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script> </head><body> <div class="container-fluid"><header> <script src="https://cdn.knightlab.com/libs/juxtapose/latest/js/juxtapose.min.js"></script> <link rel="stylesheet" href="https://cdn.knightlab.com/libs/juxtapose/latest/css/juxtapose.css"> <div class="col-lg-12"> <div class="row"> <nav class="navbar navbar-expand-lg fixed-top navbar-dark " id="topNav"> <!-- <a class="navbar-brand" href="#">Wilson Fok</a> --> <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="navbar-toggler-icon"></span> </button> <a class="navbar-brand" href="/">Wilson Fok</a> <div class="collapse navbar-collapse" id="navbarNav"> <ul class="navbar-nav"> <li class="nav-item"> <a class="nav-link" href="/about">About Me</a> </li> <li class="nav-item"> <a class="nav-link" href="/blog">Blog</a> </li> <li class="nav-item"> <a class="nav-link" href="/blog/categories">Categories</a> </li> <li class="nav-item"> <a class="nav-link" href="/gallery">Gallery</a> </li> <li class="nav-item"> <a class="nav-link" href="/contact">Contact Me</a> </li> </ul> </div> <ul class="nav justify-content-end"> <!-- <li class="nav-item"> <a class="nav-link" id="search-icon" href="/search/"><i class="fa fa-search" aria-hidden="true"></i></a> </li> --> <li class="nav-item"> <input class="nav-link switch" id="theme-toggle" onclick="modeSwitcher() "type="checkbox" name="checkbox" > </li> </ul> </nav> </div> </div> </header><div class="col-lg-12"> <!-- Blog Post Breadcrumbs --><div class="col-lg-12"> <nav aria-label="breadcrumb" role="navigation"> <ol class="breadcrumb"> <li class="breadcrumb-item"> <a href="/blog"><i class="fa fa-home" aria-hidden="true"></i></a> </li> <li class="breadcrumb-item active" aria-current="page"><a href="/nlp/2020/05/02/topics_modelling/">Topic modeling on COVID-19 publications by NLP</a></li> </ol> </nav> </div><div class="row" id="blog-post-container"> <div class="col-lg-8 offset-md-2"><article class="card" itemscope itemtype="http://schema.org/BlogPosting"> <div class="card-header"> <h1 class="post-title" itemprop="name headline">Topic modeling on COVID-19 publications by NLP</h1> <p></p> <h6 class="post-meta"> <i> Summary : Using Python, Gensim,and NLTK to identify research themes and topics on COVID-19</i> </h6> <p class="post-summary">Posted by : <img src="/assets/img/profile.png" class="author-profile-img"> <span itemprop="author" itemscope itemtype="http://schema.org/Person"> <span itemprop="name">Wilson Fok</span> </span> on <time datetime="2020-05-02 00:00:00 +0800" itemprop="datePublished">May 2, 2020</time> </p> <span class="disqus-comment-count" data-disqus-identifier="/nlp/2020/05/02/topics_modelling/"></span> <div class="post-categories"> Category : <a href="/blog/categories/NLP">NLP</a> </div> </div> <div class="card-body" itemprop="articleBody"> <p>The ongoing COVID-19 pandemic has had a great impact on human society and how we treat each other. To equip ourselves with the latest knowledge on COVID-19, I have explored the use of natural language processing techniques to assist us in understanding vast volume of published research results, retrieving relevant articles, and summarizing meaning and insights from scientists around the globe.</p> <p>For those who would like to take on the challenge, I encourage you to visit <a href="https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge/">COVID-19-research-challenge</a> for more detailed information.</p> <p>The study of natural language processing techniques has been an ongoing effort for decades. Like the study of computer vision, it has benefited immensely from many recent advances in deep learning and artificial neural networks. However, this post uses only relatively simple and classic natural language processing techniques as these techniques are already quite good and fast at revealing topics and themes in the COVID-19 literature.</p> <p>Specifically, I am going to deploy standard tools to “clean up” the texts in these articles and use Latent Dirichlet Allocation to identify underlying their common themes.</p> <h3 id="text-preprocessing">Text preprocessing</h3> <p>I take the meta data, which is a csv spreadsheet containing detailed information on each article from Kaggle. I keep the data consistent by removing those without titles or abstracts. In total, I am left with 46K articles.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="k">def</span> <span class="nf">load_data</span><span class="p">():</span>
    <span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">'../covid/metadata.csv'</span><span class="p">)</span>

    <span class="n">mask</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s">'abstract'</span><span class="p">].</span><span class="n">notnull</span><span class="p">()</span>
    <span class="n">mask2</span> <span class="o">=</span>  <span class="n">df</span><span class="p">[</span><span class="s">'title'</span><span class="p">].</span><span class="n">notnull</span><span class="p">()</span>
    <span class="n">mask3</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">all</span><span class="p">([</span><span class="n">mask</span><span class="p">,</span> <span class="n">mask2</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="p">[[</span><span class="s">'title'</span><span class="p">,</span> <span class="s">'abstract'</span><span class="p">,</span> <span class="s">'publish_time'</span><span class="p">,</span> <span class="s">'authors'</span><span class="p">]]</span>
    <span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">mask3</span><span class="p">]</span>
    
    <span class="n">data_list</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="n">column</span> <span class="ow">in</span> <span class="n">df</span><span class="p">.</span><span class="n">columns</span><span class="p">:</span>
        <span class="n">data_list</span><span class="p">[</span><span class="n">column</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">column</span><span class="p">].</span><span class="n">values</span><span class="p">.</span><span class="n">tolist</span><span class="p">()</span>
        
    <span class="k">return</span> <span class="n">data_list</span>



</code></pre></div></div> <p>The English language controls the flow and coherence of texts using stop-words which do not provide much semantic meaning on their own. Additionally, words, predominantly verbs and nouns such as singular and plural, come with different variants depending on the tense or grammatical rules. The way to undo and restore the word to its basic form is called to lemmatize a word. LancasterStemmer seems too aggressive in reducing words to their stems, and after stemming the variation of word meaning may be lost (see examples below). Finally, it is more convenient to work just with lower cases. To this end, I run the following functions to clean up the texts.</p> <p>Example 1</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="n">original</span> <span class="p">[</span><span class="s">'sequence'</span><span class="p">,</span> <span class="s">'requirements'</span><span class="p">,</span> <span class="s">'for'</span><span class="p">,</span> <span class="s">'rna'</span><span class="p">,</span> <span class="s">'strand'</span><span class="p">,</span> <span class="s">'transfer'</span><span class="p">,</span> <span class="s">'during'</span><span class="p">,</span> <span class="s">'nidovirus'</span><span class="p">,</span> <span class="s">'discontinuous'</span><span class="p">,</span> <span class="s">'subgenomic'</span><span class="p">,</span> <span class="s">'rna'</span><span class="p">,</span> <span class="s">'synthesis'</span><span class="p">]</span>
<span class="n">no</span> <span class="n">stopword</span> <span class="p">[</span><span class="s">'sequence'</span><span class="p">,</span> <span class="s">'requirements'</span><span class="p">,</span> <span class="s">'rna'</span><span class="p">,</span> <span class="s">'strand'</span><span class="p">,</span> <span class="s">'transfer'</span><span class="p">,</span> <span class="s">'nidovirus'</span><span class="p">,</span> <span class="s">'discontinuous'</span><span class="p">,</span> <span class="s">'subgenomic'</span><span class="p">,</span> <span class="s">'rna'</span><span class="p">,</span> <span class="s">'synthesis'</span><span class="p">]</span>
<span class="n">WordNetLemmatizer</span> <span class="p">[</span><span class="s">'sequence'</span><span class="p">,</span> <span class="s">'requirement'</span><span class="p">,</span> <span class="s">'rna'</span><span class="p">,</span> <span class="s">'strand'</span><span class="p">,</span> <span class="s">'transfer'</span><span class="p">,</span> <span class="s">'nidovirus'</span><span class="p">,</span> <span class="s">'discontinuous'</span><span class="p">,</span> <span class="s">'subgenomic'</span><span class="p">,</span> <span class="s">'rna'</span><span class="p">,</span> <span class="s">'synthesis'</span><span class="p">]</span>
<span class="n">LancasterStemmer</span> <span class="p">[</span><span class="s">'sequ'</span><span class="p">,</span> <span class="s">'requir'</span><span class="p">,</span> <span class="s">'rna'</span><span class="p">,</span> <span class="s">'strand'</span><span class="p">,</span> <span class="s">'transf'</span><span class="p">,</span> <span class="s">'nidovir'</span><span class="p">,</span> <span class="s">'discontinu'</span><span class="p">,</span> <span class="s">'subgenom'</span><span class="p">,</span> <span class="s">'rna'</span><span class="p">,</span> <span class="s">'synthes'</span><span class="p">]</span>

</code></pre></div></div> <p>Example 2</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">original</span> <span class="p">[</span><span class="s">'healthcare'</span><span class="p">,</span> <span class="s">'workers'</span><span class="p">,</span> <span class="s">'willingness'</span><span class="p">,</span> <span class="s">'to'</span><span class="p">,</span> <span class="s">'work'</span><span class="p">,</span> <span class="s">'during'</span><span class="p">,</span> <span class="s">'an'</span><span class="p">,</span> <span class="s">'influenza'</span><span class="p">,</span> <span class="s">'pandemic'</span><span class="p">,</span> <span class="s">'systematic'</span><span class="p">,</span> <span class="s">'review'</span><span class="p">,</span> <span class="s">'and'</span><span class="p">,</span> <span class="s">'meta'</span><span class="p">,</span> <span class="s">'analysis'</span><span class="p">]</span>
<span class="n">no</span> <span class="n">stopword</span> <span class="p">[</span><span class="s">'healthcare'</span><span class="p">,</span> <span class="s">'workers'</span><span class="p">,</span> <span class="s">'willingness'</span><span class="p">,</span> <span class="s">'work'</span><span class="p">,</span> <span class="s">'influenza'</span><span class="p">,</span> <span class="s">'pandemic'</span><span class="p">,</span> <span class="s">'systematic'</span><span class="p">,</span> <span class="s">'review'</span><span class="p">,</span> <span class="s">'meta'</span><span class="p">,</span> <span class="s">'analysis'</span><span class="p">]</span>
<span class="n">WordNetLemmatizer</span> <span class="p">[</span><span class="s">'healthcare'</span><span class="p">,</span> <span class="s">'worker'</span><span class="p">,</span> <span class="s">'willingness'</span><span class="p">,</span> <span class="s">'work'</span><span class="p">,</span> <span class="s">'influenza'</span><span class="p">,</span> <span class="s">'pandemic'</span><span class="p">,</span> <span class="s">'systematic'</span><span class="p">,</span> <span class="s">'review'</span><span class="p">,</span> <span class="s">'meta'</span><span class="p">,</span> <span class="s">'analysis'</span><span class="p">]</span>
<span class="n">LancasterStemmer</span> <span class="p">[</span><span class="s">'healthc'</span><span class="p">,</span> <span class="s">'work'</span><span class="p">,</span> <span class="s">'wil'</span><span class="p">,</span> <span class="s">'work'</span><span class="p">,</span> <span class="s">'influenz'</span><span class="p">,</span> <span class="s">'pandem'</span><span class="p">,</span> <span class="s">'system'</span><span class="p">,</span> <span class="s">'review'</span><span class="p">,</span> <span class="s">'met'</span><span class="p">,</span> <span class="s">'analys'</span><span class="p">]</span>

</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="k">def</span> <span class="nf">lower_remove_stop_word_lemmatize</span><span class="p">(</span><span class="n">data_list</span><span class="p">,</span> <span class="n">column</span><span class="p">,</span> <span class="n">printout_freq</span><span class="o">=</span><span class="mi">5000</span><span class="p">):</span>
    <span class="n">TOKEN</span> <span class="o">=</span> <span class="n">re</span><span class="p">.</span><span class="nb">compile</span><span class="p">(</span><span class="sa">r</span><span class="s">'\b\w{2,}\b'</span><span class="p">)</span>
    <span class="c1"># "Naive" token similar to that used by sklearn
</span>    
    <span class="kn">from</span> <span class="nn">nltk.corpus</span> <span class="kn">import</span> <span class="n">stopwords</span>
    <span class="n">wnl</span> <span class="o">=</span> <span class="n">nltk</span><span class="p">.</span><span class="n">WordNetLemmatizer</span><span class="p">()</span>
    
    <span class="n">no_stop_word_list</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">item</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">data_list</span><span class="p">[</span><span class="n">column</span><span class="p">]):</span>
        <span class="n">tokens</span> <span class="o">=</span> <span class="n">TOKEN</span><span class="p">.</span><span class="n">findall</span><span class="p">(</span><span class="n">item</span><span class="p">)</span>
        
        <span class="n">tokens</span> <span class="o">=</span> <span class="p">[</span><span class="n">token</span><span class="p">.</span><span class="n">lower</span><span class="p">()</span> <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">tokens</span><span class="p">]</span>
        
        <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="n">printout_freq</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">print</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">tokens</span><span class="p">),</span> <span class="n">tokens</span><span class="p">)</span>
        <span class="n">tokens</span> <span class="o">=</span> <span class="p">[</span><span class="n">token</span> <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">tokens</span> <span class="k">if</span> <span class="n">token</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">stopwords</span><span class="p">.</span><span class="n">words</span><span class="p">(</span><span class="s">'english'</span><span class="p">)]</span>
        <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="n">printout_freq</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">print</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">tokens</span><span class="p">),</span> <span class="n">tokens</span><span class="p">)</span>
        <span class="n">tokens</span> <span class="o">=</span> <span class="p">[</span><span class="n">wnl</span><span class="p">.</span><span class="n">lemmatize</span><span class="p">(</span><span class="n">token</span><span class="p">)</span> <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">tokens</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="n">printout_freq</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">print</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">tokens</span><span class="p">),</span> <span class="n">tokens</span><span class="p">)</span>
            <span class="k">print</span> <span class="p">()</span>
            
        <span class="n">no_stop_word_list</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">no_stop_word_list</span>



</code></pre></div></div> <p>A majority of the words in the texts do not occur regularly in these articles. I drop them out based on their occurrence in the entire texts of titles.</p> <p>I like defaultdict because if we do not include certain words, the defaultdict would automatically throws a count value of zero without raising KeyError as in the case of “normal” dict.</p> <p>I have looked at the words distribution between the titles and abstracts. From what I have seen, the words in the titles are far more informative than the words in the abstracts as far as identifying hidden topics in these documents. Being a longer passage of texts, the abstracts contain far more “less meaningful” words. This is in part due to the stop-words list not being comprehensive enough. For instance, it hasn’t filtered out words such as “also”.</p> <p><img src="/assets/images/covid/processed_corpus_abstract.png" alt="Picture description" width="1100px" /></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="c1"># Count word frequencies
</span>    <span class="n">frequency</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">text</span> <span class="ow">in</span> <span class="n">data_list</span><span class="p">[</span><span class="s">'title'</span><span class="p">]:</span>
        <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">text</span><span class="p">:</span>
            <span class="n">frequency</span><span class="p">[</span><span class="n">token</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
            
       
    <span class="c1"># Only keep words that appear more often
</span>    <span class="n">processed_corpus</span> <span class="o">=</span> <span class="p">[[</span><span class="n">token</span> <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">text</span> <span class="k">if</span> <span class="n">frequency</span><span class="p">[</span><span class="n">token</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">60</span><span class="p">]</span> <span class="k">for</span> <span class="n">text</span> <span class="ow">in</span> <span class="n">data_list</span><span class="p">[</span><span class="s">'abstract'</span><span class="p">]]</span>
<span class="c1">#    pprint.pprint(processed_corpus)
</span>    
    
    <span class="kn">from</span> <span class="nn">wordcloud</span> <span class="kn">import</span> <span class="n">WordCloud</span>
    <span class="n">text</span> <span class="o">=</span> <span class="s">' '</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">flatten_list</span><span class="p">(</span><span class="n">processed_corpus</span><span class="p">))</span>
    
    <span class="c1"># Generate a word cloud image
</span>    <span class="n">wordcloud</span> <span class="o">=</span> <span class="n">WordCloud</span><span class="p">(</span><span class="n">width</span><span class="o">=</span><span class="mi">800</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="mi">400</span><span class="p">,</span>
                          <span class="n">max_font_size</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">max_words</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">background_color</span><span class="o">=</span><span class="s">"white"</span><span class="p">).</span><span class="n">generate</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
    
    <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">()</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">wordcloud</span><span class="p">,</span> <span class="n">interpolation</span><span class="o">=</span><span class="s">'bilinear'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">axis</span><span class="p">(</span><span class="s">"off"</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">savefig</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">output_dir</span><span class="p">,</span> <span class="s">'processed_corpus_abstract.png'</span><span class="p">),</span> <span class="n">transparent</span><span class="o">=</span><span class="bp">True</span> <span class="p">,</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">400</span><span class="p">,</span> <span class="n">bbox_inches</span><span class="o">=</span><span class="s">'tight'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">close</span><span class="p">()</span>
</code></pre></div></div> <h4 id="how-to-build-topic-models-using-gensim">How to build topic models using Gensim?</h4> <p>Building topic models using Gensim is very easy. Here is how I do it. Gensim is especially helpful because it puts memory usage and computational efficiency at heart. Models.LdaModel and its variants trains on the corpus incrementally, e.g. 10k at one time. This makes training feasible on less resourceful computers.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c1">############ Gensim
</span>
    <span class="n">dictionary</span> <span class="o">=</span> <span class="n">corpora</span><span class="p">.</span><span class="n">Dictionary</span><span class="p">(</span><span class="n">processed_corpus</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="n">dictionary</span><span class="p">)</span>
    
    
    <span class="n">bow_corpus</span> <span class="o">=</span> <span class="p">[</span><span class="n">dictionary</span><span class="p">.</span><span class="n">doc2bow</span><span class="p">(</span><span class="n">text</span><span class="p">)</span> <span class="k">for</span> <span class="n">text</span> <span class="ow">in</span> <span class="n">processed_corpus</span><span class="p">]</span>
    <span class="c1"># train the model
</span>    <span class="n">tfidf</span> <span class="o">=</span> <span class="n">models</span><span class="p">.</span><span class="n">TfidfModel</span><span class="p">(</span><span class="n">bow_corpus</span><span class="p">)</span>
    <span class="n">corpus_tfidf</span> <span class="o">=</span> <span class="n">tfidf</span><span class="p">[</span><span class="n">bow_corpus</span><span class="p">]</span>
	
	<span class="n">num_topics</span> <span class="o">=</span> <span class="mi">50</span>
    <span class="n">topic_model</span> <span class="o">=</span> <span class="n">models</span><span class="p">.</span><span class="n">LdaModel</span><span class="p">(</span><span class="n">corpus_tfidf</span><span class="p">,</span>
                                <span class="n">id2word</span><span class="o">=</span><span class="n">dictionary</span><span class="p">,</span> 
                                <span class="n">num_topics</span><span class="o">=</span><span class="n">num_topics</span><span class="p">)</span>
<span class="c1">#    topic_model  = models.RpModel(corpus_tfidf, num_topics=num_topics)
</span>    <span class="c1"># topic_model = models.HdpModel(corpus_tfidf, id2word=dictionary)
#    topic_model = models.LsiModel(corpus_tfidf,
#                                id2word=dictionary, 
#                                num_topics=num_topics)  # initialize an LSI transformation
</span>    <span class="n">topic_model</span><span class="p">.</span><span class="n">print_topics</span><span class="p">(</span><span class="n">num_topics</span><span class="p">)</span>
    
    <span class="c1"># both bow-&gt;tfidf and tfidf-&gt;lsi transformations are actually executed here, on the fly
</span>    <span class="n">corpus_lsi</span> <span class="o">=</span> <span class="n">topic_model</span><span class="p">[</span><span class="n">corpus_tfidf</span><span class="p">]</span>  <span class="c1"># create a double wrapper over the original corpus: bow-&gt;tfidf-&gt;fold-in-lsi
</span>    
    <span class="n">topic_words</span> <span class="o">=</span> <span class="p">{}</span>
    
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_topics</span><span class="p">):</span>
        <span class="n">topic_words</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">topic_model</span><span class="p">.</span><span class="n">show_topic</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
	
</code></pre></div></div> <p>Choosing the right number of topics is not intuitive and is often a process of trial and error. For instance, 50 topics work fine in this example and I have tried a range of other numbers of topics. Since documents on different topics have different word distributions and word frequency, figuring out how to weight these words in each document is the key to identifying hidden topics amongst the documents. In essence, this is what LDA tries to do by solving for the word weights with linear algebra. The importance of words to a topic is expressed by the word weights. For example, LDA identifies topic 49th and topic 12th with the following words and their weights. I think topic 12th concerns about evolution of the virus whereas topic 49th seems to be about animal infections.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
 <span class="mi">12</span><span class="p">:</span> <span class="p">[(</span><span class="s">'bat'</span><span class="p">,</span> <span class="mf">0.06349142</span><span class="p">),</span>
      <span class="p">(</span><span class="s">'specie'</span><span class="p">,</span> <span class="mf">0.031821612</span><span class="p">),</span>
      <span class="p">(</span><span class="s">'evolution'</span><span class="p">,</span> <span class="mf">0.027451487</span><span class="p">),</span>
      <span class="p">(</span><span class="s">'host'</span><span class="p">,</span> <span class="mf">0.025193721</span><span class="p">),</span>
      <span class="p">(</span><span class="s">'genetics'</span><span class="p">,</span> <span class="mf">0.019700462</span><span class="p">),</span>
      <span class="p">(</span><span class="s">'genetic'</span><span class="p">,</span> <span class="mf">0.017949674</span><span class="p">),</span>
      <span class="p">(</span><span class="s">'virus'</span><span class="p">,</span> <span class="mf">0.016766567</span><span class="p">),</span>
      <span class="p">(</span><span class="s">'evolutionary'</span><span class="p">,</span> <span class="mf">0.016061012</span><span class="p">),</span>
      <span class="p">(</span><span class="s">'mutation'</span><span class="p">,</span> <span class="mf">0.015995547</span><span class="p">),</span>
      <span class="p">(</span><span class="s">'rodent'</span><span class="p">,</span> <span class="mf">0.015365632</span><span class="p">)],</span>
	  
	  
 <span class="mi">49</span><span class="p">:</span> <span class="p">[(</span><span class="s">'dog'</span><span class="p">,</span> <span class="mf">0.07506799</span><span class="p">),</span>
      <span class="p">(</span><span class="s">'canine'</span><span class="p">,</span> <span class="mf">0.05088224</span><span class="p">),</span>
      <span class="p">(</span><span class="s">'2020'</span><span class="p">,</span> <span class="mf">0.04714561</span><span class="p">),</span>
      <span class="p">(</span><span class="s">'chicken'</span><span class="p">,</span> <span class="mf">0.036831263</span><span class="p">),</span>
      <span class="p">(</span><span class="s">'strain'</span><span class="p">,</span> <span class="mf">0.029606815</span><span class="p">),</span>
      <span class="p">(</span><span class="s">'s1'</span><span class="p">,</span> <span class="mf">0.028369702</span><span class="p">),</span>
      <span class="p">(</span><span class="s">'bronchitis'</span><span class="p">,</span> <span class="mf">0.023845471</span><span class="p">),</span>
      <span class="p">(</span><span class="s">'peritonitis'</span><span class="p">,</span> <span class="mf">0.019557731</span><span class="p">),</span>
      <span class="p">(</span><span class="s">'vaccine'</span><span class="p">,</span> <span class="mf">0.013181845</span><span class="p">),</span>
      <span class="p">(</span><span class="s">'infectious'</span><span class="p">,</span> <span class="mf">0.012176985</span><span class="p">)]}</span>
</code></pre></div></div> <h4 id="how-to-get-a-sense-of-what-these-topics-are-about">How to get a sense of what these topics are about?</h4> <p>Let’s take a look at what the documents say when they are likely to have come from several topics. LDA describes each document as a mixture of words taken out from an array of topics. Thus, each document naturally spreads across several topics. To really understanding what a single topic is about, the easiest way is to go through all the documents and find those that exhibit only a single topic or are heavily biased towards one topic.</p> <p>To visualize these topics, I further cluster them into bigger but fewer overarching topics.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
    <span class="kn">from</span> <span class="nn">sklearn.cluster</span> <span class="kn">import</span> <span class="n">MiniBatchKMeans</span>
    
    <span class="n">n_clusters</span> <span class="o">=</span> <span class="mi">7</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">3000</span>
    
    <span class="n">kmeans</span> <span class="o">=</span> <span class="n">MiniBatchKMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="n">n_clusters</span><span class="p">,</span>
                        <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
                        <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">counter</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_topics</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">float16</span><span class="p">)</span>
    

    <span class="c1"># use only dominant documents for each topics
</span>    <span class="n">dominant_threshold</span> <span class="o">=</span> <span class="mf">0.7</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">doc</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">corpus_lsi</span><span class="p">):</span>

        <span class="k">if</span> <span class="n">counter</span> <span class="o">%</span> <span class="n">batch_size</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">counter</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">print</span> <span class="p">(</span><span class="s">'fit and reset'</span><span class="p">)</span>
            <span class="n">kmeans</span> <span class="o">=</span> <span class="n">kmeans</span><span class="p">.</span><span class="n">partial_fit</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            <span class="n">counter</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_topics</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">float16</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">col</span> <span class="o">=</span> <span class="p">[</span><span class="n">item</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">doc</span><span class="p">]</span>
            <span class="n">v</span> <span class="o">=</span>  <span class="p">[</span><span class="n">item</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">doc</span><span class="p">]</span>
            
            <span class="c1"># dominant document
</span>            <span class="k">if</span> <span class="nb">max</span><span class="p">(</span><span class="n">v</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">dominant_threshold</span><span class="p">:</span>
                
                <span class="n">x</span><span class="p">[</span><span class="n">counter</span><span class="p">,</span> <span class="n">col</span><span class="p">]</span> <span class="o">=</span> <span class="n">v</span>
                <span class="n">counter</span> <span class="o">=</span> <span class="n">counter</span> <span class="o">+</span> <span class="mi">1</span>
            <span class="c1"># break
</span>    
    <span class="k">print</span> <span class="p">(</span><span class="s">'kmeans.cluster_centers_ {}'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">kmeans</span><span class="p">.</span><span class="n">cluster_centers_</span><span class="p">))</span>
    
    
    <span class="n">labels</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">list</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_topics</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">float16</span><span class="p">)</span>
    
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">doc</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">corpus_lsi</span><span class="p">):</span>
        
        <span class="n">col</span> <span class="o">=</span> <span class="p">[</span><span class="n">item</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">doc</span><span class="p">]</span>
        <span class="n">v</span> <span class="o">=</span>  <span class="p">[</span><span class="n">item</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">doc</span><span class="p">]</span>
        <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">col</span><span class="p">]</span> <span class="o">=</span> <span class="n">v</span>
        <span class="n">label</span> <span class="o">=</span> <span class="n">kmeans</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">labels</span><span class="p">[</span><span class="n">label</span><span class="p">[</span><span class="mi">0</span><span class="p">]].</span><span class="n">append</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
        
    <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">labels</span><span class="p">.</span><span class="n">items</span><span class="p">():</span>
        <span class="k">print</span> <span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">value</span><span class="p">))</span>
        
    <span class="c1"># look at some documents    
</span>    <span class="n">max_num_samples</span> <span class="o">=</span> <span class="mi">300</span>
    
    <span class="c1"># randomly select some samples
</span>  
    <span class="n">sample_set</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">values</span> <span class="ow">in</span> <span class="n">labels</span><span class="p">.</span><span class="n">items</span><span class="p">():</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">values</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">max_num_samples</span><span class="p">:</span>
            <span class="n">s</span> <span class="o">=</span> <span class="n">values</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">s</span> <span class="o">=</span> <span class="n">random</span><span class="p">.</span><span class="n">sample</span><span class="p">(</span><span class="n">values</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="n">max_num_samples</span><span class="p">)</span>
        <span class="n">sample_set</span> <span class="o">+=</span> <span class="n">s</span>
    <span class="n">sample_set</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">sample_set</span><span class="p">))</span> <span class="c1"># no duplicates
</span>    <span class="n">ns</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">sample_set</span><span class="p">)</span>
    <span class="k">print</span> <span class="p">(</span><span class="n">ns</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">labels</span><span class="p">)</span><span class="o">*</span><span class="n">max_num_samples</span><span class="p">)</span>
    
    <span class="n">X_samples_order</span> <span class="o">=</span> <span class="p">{</span><span class="n">i</span><span class="p">:</span><span class="n">value</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">sample_set</span><span class="p">)}</span> 
    <span class="n">samples_X_order</span> <span class="o">=</span> <span class="p">{</span><span class="n">value</span><span class="p">:</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">sample_set</span><span class="p">)}</span>
    
    
    <span class="n">vectors</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">ns</span><span class="p">,</span> <span class="n">num_topics</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">float16</span><span class="p">)</span>
    <span class="n">selected_titles</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">selected_abstracts</span> <span class="o">=</span> <span class="p">{}</span>
    
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">doc</span><span class="p">,</span> <span class="n">abstract</span><span class="p">,</span> <span class="n">title</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">corpus_lsi</span><span class="p">,</span> <span class="n">data_list</span><span class="p">[</span><span class="s">'abstract'</span><span class="p">],</span>
                                            <span class="n">data_list</span><span class="p">[</span><span class="s">'title'</span><span class="p">])):</span>

        
        <span class="n">insert_loc</span> <span class="o">=</span> <span class="n">samples_X_order</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="bp">None</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">insert_loc</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
<span class="c1">#            print (insert_loc, i, doc, as_text)
</span>            <span class="n">col</span> <span class="o">=</span> <span class="p">[</span><span class="n">item</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">doc</span><span class="p">]</span>
            <span class="n">v</span> <span class="o">=</span>  <span class="p">[</span><span class="n">item</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">doc</span><span class="p">]</span>
            <span class="n">vectors</span><span class="p">[</span><span class="n">insert_loc</span><span class="p">,</span> <span class="n">col</span><span class="p">]</span> <span class="o">=</span> <span class="n">v</span>
            <span class="n">selected_titles</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">title</span>
            <span class="n">selected_abstracts</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">abstract</span>

    <span class="n">samples_labels</span> <span class="o">=</span> <span class="n">kmeans</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">vectors</span><span class="p">)</span>
    
    <span class="c1"># show the clusters of topics
</span>    <span class="n">num_dimensions</span> <span class="o">=</span> <span class="mi">2</span>
    
    <span class="n">tsne</span> <span class="o">=</span> <span class="n">TSNE</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="n">num_dimensions</span><span class="p">)</span>
    <span class="n">lower_vectors</span> <span class="o">=</span> <span class="n">tsne</span><span class="p">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">vectors</span><span class="p">)</span>
    
    <span class="kn">import</span> <span class="nn">matplotlib</span>
    
    <span class="n">cmap</span> <span class="o">=</span> <span class="n">matplotlib</span><span class="p">.</span><span class="n">cm</span><span class="p">.</span><span class="n">get_cmap</span><span class="p">(</span><span class="s">'tab20b'</span><span class="p">)</span>
    <span class="n">vmin</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">samples_labels</span><span class="p">)</span>
    <span class="n">vmax</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">samples_labels</span><span class="p">)</span>
    
    <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">12</span><span class="p">))</span>
    <span class="n">colors</span> <span class="o">=</span> <span class="p">[</span><span class="s">'black'</span><span class="p">,</span> <span class="s">'purple'</span><span class="p">,</span> <span class="s">'blue'</span><span class="p">,</span> <span class="s">'green'</span><span class="p">,</span> <span class="s">'orange'</span><span class="p">,</span><span class="s">'red'</span><span class="p">,</span><span class="s">'yellow'</span><span class="p">]</span>
    <span class="n">colormap_names</span> <span class="o">=</span> <span class="p">[</span><span class="s">'Greys'</span><span class="p">,</span> <span class="s">'Purples'</span><span class="p">,</span> <span class="s">'Blues'</span><span class="p">,</span> <span class="s">'Greens'</span><span class="p">,</span> <span class="s">'Oranges'</span><span class="p">,</span> <span class="s">'Reds'</span><span class="p">,</span> <span class="s">'spring'</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_clusters</span><span class="p">):</span>
        <span class="n">where</span> <span class="o">=</span> <span class="n">samples_labels</span> <span class="o">==</span> <span class="n">i</span>
        
        <span class="n">plt</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">lower_vectors</span><span class="p">[</span><span class="n">where</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span>
                    <span class="n">lower_vectors</span><span class="p">[</span><span class="n">where</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
    <span class="c1"># plt.colorbar()
</span>    <span class="n">plt</span><span class="p">.</span><span class="n">axis</span><span class="p">(</span><span class="s">"off"</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">savefig</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">output_dir</span><span class="p">,</span> <span class="s">'tsne_dominant_documents_topics.png'</span><span class="p">),</span> <span class="n">transparent</span><span class="o">=</span><span class="bp">True</span> <span class="p">,</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">400</span><span class="p">,</span> <span class="n">bbox_inches</span><span class="o">=</span><span class="s">'tight'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
	
</code></pre></div></div> <p>MiniBatchKMeans has been very useful because it allows incremental fitting of the document vectors, a method that drains less computational resources but achieves similar results to KMeans.</p> <p>TSNE is used to visualize the topic space in 2D.</p> <p><img src="/assets/images/covid/tsne_dominant_documents_topics.png" alt="Picture description" width="1100px" /></p> <p>For each cluster, I have drawn its wordcloud (sharing the same color theme).</p> <p><img src="/assets/images/covid/wordcloud_0.png" alt="Picture description" width="1100px" /> <img src="/assets/images/covid/wordcloud_1.png" alt="Picture description" width="1100px" /> <img src="/assets/images/covid/wordcloud_2.png" alt="Picture description" width="1100px" /> <img src="/assets/images/covid/wordcloud_3.png" alt="Picture description" width="1100px" /> <img src="/assets/images/covid/wordcloud_4.png" alt="Picture description" width="1100px" /> <img src="/assets/images/covid/wordcloud_5.png" alt="Picture description" width="1100px" /> <img src="/assets/images/covid/wordcloud_6.png" alt="Picture description" width="1100px" /></p> </div> <div id="disqus_thread"></div> </article> <article class="card" itemscope itemtype="http://schema.org/BlogPosting"> <div class="card-body" itemprop="articleBody"> <link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet"> <div class="card-header"> <span class="title"> <i class="fa fa-share"></i> Share this to: </span> </div> <div id="share-bar"> <div class="share-buttons"> <a href="https://www.facebook.com/sharer/sharer.php?u=/nlp/2020/05/02/topics_modelling/" onclick="window.open(this.href, 'pop-up', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Facebook" > <i class="fa fa-facebook-official share-button"> facebook</i> </a> <a href="https://twitter.com/intent/tweet?text=Topic modeling on COVID-19 publications by NLP&url=/nlp/2020/05/02/topics_modelling/" onclick="window.open(this.href, 'pop-up', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Twitter" > <i class="fa fa-twitter share-button"> twitter</i> </a> <a href="https://plus.google.com/share?url=/nlp/2020/05/02/topics_modelling/" onclick="window.open(this.href, 'pop-up', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Google+" > <i class="fa fa-google-plus share-button"> google</i> </a> <a href="https://www.pinterest.com/pin/create/button/?url=/nlp/2020/05/02/topics_modelling/" onclick="window.open(this.href, 'pop-up', 'left=20,top=20,width=900,height=500,toolbar=1,resizable=0'); return false;" title="Share on Pinterest" > <i class="fa fa-pinterest-p share-button"> pinterest</i> </a> <a href="https://www.tumblr.com/share/link?url=/nlp/2020/05/02/topics_modelling/" onclick="window.open(this.href, 'pop-up', 'left=20,top=20,width=900,height=500,toolbar=1,resizable=0'); return false;" title="Share on Tumblr" > <i class="fa fa-tumblr share-button"> tumblr</i> </a> <a href="http://www.reddit.com/submit?url=/nlp/2020/05/02/topics_modelling/" onclick="window.open(this.href, 'pop-up', 'left=20,top=20,width=900,height=500,toolbar=1,resizable=0'); return false;" title="Share on Reddit" > <i class="fa fa-reddit-alien share-button"> reddit</i> </a> <a href="https://www.linkedin.com/shareArticle?mini=true&url=/nlp/2020/05/02/topics_modelling/&title=Topic modeling on COVID-19 publications by NLP&summary=&source=Wilson Fok" onclick="window.open(this.href, 'pop-up', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn" > <i class="fa fa-linkedin share-button"> linkedin</i> </a> <a href="mailto:?subject=Topic modeling on COVID-19 publications by NLP&amp;body=Check out this site /nlp/2020/05/02/topics_modelling/" title="Share via Email" > <i class="fa fa-envelope share-button"> email</i> </a> </div> </div> </div> </article> <script> var disqus_config = function () { this.page.url = "/nlp/2020/05/02/topics_modelling/"; /* Replace PAGE_URL with your page's canonical URL variable */ this.page.identifier = "/nlp/2020/05/02/topics_modelling"; /* Replace PAGE_IDENTIFIER with your page's unique identifier variable */ }; (function () { /* DON'T EDIT BELOW THIS LINE */ var d = document, s = d.createElement('script'); s.src = 'https://.disqus.com/embed.js'; s.setAttribute('data-timestamp', +new Date()); (d.head || d.body).appendChild(s); })(); </script> <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a> </noscript></div> </div> <!-- End of row--> <div class="row"> <div class="col-md-4"> <div class="card"> <div class="card-header"> About </div> <div class="card-body"> <!-- Your Bio --> <p class="author_bio"> Hello, My name is Wilson Fok. I love to extract useful insights and knowledge from big data. Constructive feedback and insightful comments are very welcome!</p> </div> </div> </div> <div class="col-md-4"> <div class="card"> <div class="card-header">Categories </div> <div class="card-body text-dark"> <div id="#Finance"></div> <li class="tag-head"> <a href="/blog/categories/Finance">Finance</a> </li> <a name="Finance"></a> <div id="#NLP"></div> <li class="tag-head"> <a href="/blog/categories/NLP">NLP</a> </li> <a name="NLP"></a> <div id="#Deep_Learning"></div> <li class="tag-head"> <a href="/blog/categories/Deep_Learning">Deep_Learning</a> </li> <a name="Deep_Learning"></a> <div id="#Others"></div> <li class="tag-head"> <a href="/blog/categories/Others">Others</a> </li> <a name="Others"></a> <div id="#Reading"></div> <li class="tag-head"> <a href="/blog/categories/Reading">Reading</a> </li> <a name="Reading"></a> <div id="#Toastmasters"></div> <li class="tag-head"> <a href="/blog/categories/Toastmasters">Toastmasters</a> </li> <a name="Toastmasters"></a> </div> </div> </div> <div class="col-md-4"> <div class="card"> <div class="card-header">Useful Links </div> <div class="card-body text-dark"> <li > <a href="/about">About Me</a> </li> <li > <a href="/blog">Blog</a> </li> <li > <a href="/blog/categories">Categories</a> </li> <li > <a href="/gallery">Gallery</a> </li> <li > <a href="/contact">Contact Me</a> </li> </div> </div> </div> </div> </div> <footer> <p> Powered by Jekyll. Hosted on <a href="https://pages.github.com">Github</a>. Subscribe via <a href=" /feed.xml ">RSS <i class="fa fa-rss" aria-hidden="true"></i> </a> </p> </footer> </div> <script> var options = { classname: 'my-class', id: 'my-id' }; var nanobar = new Nanobar( options ); nanobar.go( 30 ); nanobar.go( 76 ); nanobar.go(100); </script> <!-- <div hidden id="snipcart" data-api-key="Y2I1NTAyNWYtMTNkMy00ODg0LWE4NDItNTZhYzUxNzJkZTI5NjM3MDI4NTUzNzYyMjQ4NzU0"></div> <script src="https://cdn.snipcart.com/themes/v3.0.0-beta.3/default/snipcart.js" defer></script> --> <script src="/assets/js/mode-switcher.js"></script> <script src="/assets/js/slideshow.js"></script> </body> </html>
